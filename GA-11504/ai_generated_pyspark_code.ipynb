{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nfrom pyspark.sql import functions as F\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Define constants for table names and paths\nFLAT_FILE_PATH = \"/Volumes/genai_demo/ssis/ssis/SampleCurrencyData.txt\"\nDIM_CURRENCY_TABLE = \"sqlserver_catalog.dbo.dimcurrency1\"\nDIM_DATE_TABLE = \"sqlserver_catalog.dbo.dimdate1\"\nTARGET_TABLE = \"sqlserver_catalog.dbo.ssisresult\"\n\n# Function to load data from flat file\ndef load_flat_file_data(file_path):\n    try:\n        logging.info(\"Loading data from flat file: %s\", file_path)\n        flat_file_df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \",\").csv(file_path)\n        return flat_file_df\n    except Exception as e:\n        logging.error(\"Error loading flat file data: %s\", str(e))\n        raise\n\n# Function to perform lookup transformations\ndef perform_lookups(flat_file_df):\n    try:\n        logging.info(\"Performing lookup transformations\")\n\n        # Load DimCurrency table\n        dim_currency_df = spark.table(DIM_CURRENCY_TABLE)\n\n        # Perform lookup on CurrencyID\n        flat_file_df = flat_file_df.join(dim_currency_df, flat_file_df.CurrencyID == dim_currency_df.CurrencyAlternateKey, \"inner\") \\\n                                   .select(flat_file_df[\"*\"], dim_currency_df[\"CurrencyKey\"])\n\n        # Load DimDate table\n        dim_date_df = spark.table(DIM_DATE_TABLE)\n\n        # Perform lookup on CurrencyDate\n        flat_file_df = flat_file_df.join(dim_date_df, flat_file_df.CurrencyDate == dim_date_df.FullDateAlternateKey, \"inner\") \\\n                                   .select(flat_file_df[\"*\"], dim_date_df[\"DateKey\"])\n\n        return flat_file_df\n    except Exception as e:\n        logging.error(\"Error performing lookups: %s\", str(e))\n        raise\n\n# Function to write data to target table\ndef write_to_target_table(transformed_df):\n    try:\n        logging.info(\"Writing data to target table: %s\", TARGET_TABLE)\n        spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE}\")\n        transformed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TARGET_TABLE)\n    except Exception as e:\n        logging.error(\"Error writing to target table: %s\", str(e))\n        raise\n\n# Main ETL process\ndef main():\n    try:\n        # Step 1: Load data from flat file\n        flat_file_df = load_flat_file_data(FLAT_FILE_PATH)\n\n        # Step 2: Perform lookup transformations\n        transformed_df = perform_lookups(flat_file_df)\n\n        # Step 3: Write transformed data to target table\n        write_to_target_table(transformed_df)\n\n        logging.info(\"ETL process completed successfully\")\n    except Exception as e:\n        logging.error(\"ETL process failed: %s\", str(e))\n\n# Execute the main ETL process\nmain()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}