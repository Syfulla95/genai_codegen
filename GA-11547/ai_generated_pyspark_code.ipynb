{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport requests\nfrom pyspark.sql.functions import col, count, avg, max, datediff, current_date, when, lit\nfrom pyspark.sql.types import IntegerType, DecimalType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to fetch data from REST API\ndef fetch_api_data(api_url, customer_ids, api_key):\n    headers = {'Authorization': f'Bearer {api_key}'}\n    results = []\n    for customer_id in customer_ids:\n        try:\n            response = requests.get(api_url.format(Customer_ID=customer_id), headers=headers)\n            if response.status_code == 200:\n                results.append((customer_id, response.json().get('score')))\n            else:\n                logger.error(f\"Failed to fetch data for Customer_ID {customer_id}: {response.status_code}\")\n        except Exception as e:\n            logger.error(f\"Error fetching data for Customer_ID {customer_id}: {str(e)}\")\n    return results\n\n# Load data from Unity Catalog tables\ntry:\n    policy_df = spark.table(\"postgresql_catalog.demo.policydb\")\n    claims_df = spark.table(\"mysql_catalog.vsco.claimsdb\")\n    demographics_df = spark.table(\"sqlserver_catalog.dbo.demographicsdb\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog: {str(e)}\")\n    raise\n\n# Join policy data with customer demographics\ntry:\n    policy_demo_df = policy_df.join(demographics_df, \"Customer_ID\", \"inner\")\nexcept Exception as e:\n    logger.error(f\"Error joining policy and demographics data: {str(e)}\")\n    raise\n\n# Join the result with claims data\ntry:\n    policy_claims_df = policy_demo_df.join(claims_df, \"Policy_ID\", \"inner\")\nexcept Exception as e:\n    logger.error(f\"Error joining policy_demo and claims data: {str(e)}\")\n    raise\n\n# Aggregate data at the customer level\ntry:\n    agg_df = policy_claims_df.groupBy(\"Customer_ID\").agg(\n        count(\"Claim_ID\").alias(\"Total_Claims\"),\n        avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\"),\n        max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        count(\"Policy_ID\").alias(\"Policy_Count\")\n    )\nexcept Exception as e:\n    logger.error(f\"Error aggregating data: {str(e)}\")\n    raise\n\n# Calculate derived fields\ntry:\n    derived_df = agg_df.withColumn(\"Age\", datediff(current_date(), col(\"Date_of_Birth\")) / 365) \\\n        .withColumn(\"Claim_To_Premium_Ratio\", when(col(\"Total_Premium_Paid\") != 0, col(\"Claim_Amount\") / col(\"Total_Premium_Paid\")).otherwise(0)) \\\n        .withColumn(\"Claims_Per_Policy\", when(col(\"Policy_Count\") != 0, col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0)) \\\n        .withColumn(\"Retention_Rate\", lit(0.85)) \\\n        .withColumn(\"Cross_Sell_Opportunities\", lit(\"Multi-Policy Discount, Home Coverage Add-on\")) \\\n        .withColumn(\"Upsell_Potential\", lit(\"Premium Vehicle Coverage\"))\nexcept Exception as e:\n    logger.error(f\"Error calculating derived fields: {str(e)}\")\n    raise\n\n# Fetch fraud scores from API\ntry:\n    fraud_api_url = \"http://18.189.118.116:9010/fraudscore?Customer_ID={Customer_ID}\"\n    fraud_api_key = dbutils.secrets.get(\"api_secrets\", \"fraud_api_key\")\n    fraud_scores = fetch_api_data(fraud_api_url, derived_df.select(\"Customer_ID\").rdd.flatMap(lambda x: x).collect(), fraud_api_key)\n    fraud_df = spark.createDataFrame(fraud_scores, [\"Customer_ID\", \"Fraud_Score\"])\nexcept Exception as e:\n    logger.error(f\"Error fetching fraud scores: {str(e)}\")\n    raise\n\n# Fetch credit scores from API\ntry:\n    credit_api_url = \"http://18.189.118.116:9010/creditscore?Customer_ID={Customer_ID}\"\n    credit_api_key = dbutils.secrets.get(\"api_secrets\", \"credit_api_key\")\n    credit_scores = fetch_api_data(credit_api_url, derived_df.select(\"Customer_ID\").rdd.flatMap(lambda x: x).collect(), credit_api_key)\n    credit_df = spark.createDataFrame(credit_scores, [\"Customer_ID\", \"Credit_Score\"])\nexcept Exception as e:\n    logger.error(f\"Error fetching credit scores: {str(e)}\")\n    raise\n\n# Join derived data with fraud and credit scores\ntry:\n    final_df = derived_df.join(fraud_df, \"Customer_ID\", \"left\").join(credit_df, \"Customer_ID\", \"left\")\nexcept Exception as e:\n    logger.error(f\"Error joining derived data with API scores: {str(e)}\")\n    raise\n\n# Add AI-driven insights\ntry:\n    final_df = final_df.withColumn(\"Churn_Probability\", lit(0.25)) \\\n        .withColumn(\"Next_Best_Offer\", lit(\"Additional Life Coverage\")) \\\n        .withColumn(\"Claims_Fraud_Probability\", lit(0.10)) \\\n        .withColumn(\"Revenue_Potential\", lit(12000.00))\nexcept Exception as e:\n    logger.error(f\"Error adding AI-driven insights: {str(e)}\")\n    raise\n\n# Write the final DataFrame to Unity Catalog table\ntry:\n    spark.sql(\"DROP TABLE IF EXISTS genai_demo.cardinal_health.customer_360\")\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.cardinal_health.customer_360\")\n    logger.info(\"Data successfully written to Unity Catalog table: genai_demo.cardinal_health.customer_360\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog: {str(e)}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}