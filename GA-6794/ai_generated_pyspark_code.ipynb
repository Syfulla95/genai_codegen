{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# %md\n# # Customer 360 View Data Processing\n# This notebook processes customer data to create a comprehensive 360-degree view by integrating various datasets.\n\n# COMMAND ----------\n#\n",
                "import logging\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, avg, max, floor, to_date, when, current_date, datediff, expr\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n# %md\n# ## Load Data\n# Load data from CSV files into DataFrames.\n\n# COMMAND ----------\n#\n",
                "def load_data():\n    try:\n        logger.info(\"Loading data from CSV files...\")\n        claims_df = spark.read.csv(\"dbfs:/FileStore/claims.csv\", header=True, inferSchema=True)\n        demographics_df = spark.read.csv(\"dbfs:/FileStore/demographics.csv\", header=True, inferSchema=True)\n        policy_df = spark.read.csv(\"dbfs:/FileStore/policy.csv\", header=True, inferSchema=True)\n        scores_df = spark.read.csv(\"dbfs:/FileStore/scores.csv\", header=True, inferSchema=True)\n        aiml_insights_df = spark.read.csv(\"dbfs:/FileStore/aiml_insights.csv\", header=True, inferSchema=True)\n        logger.info(\"Data loaded successfully.\")\n        return claims_df, demographics_df, policy_df, scores_df, aiml_insights_df\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\nclaims_df, demographics_df, policy_df, scores_df, aiml_insights_df = load_data()\n\n# COMMAND ----------\n# %md\n# ## Select Relevant Fields\n# Select relevant fields from each dataset.\n\n# COMMAND ----------\n#\n",
                "def select_fields(demographics_df, claims_df, policy_df):\n    try:\n        logger.info(\"Selecting relevant fields from datasets...\")\n        demographics_selected = demographics_df.select(\n            \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\", \"State\", \"Postal_Code\",\n            \"Date_of_Birth\", \"Gender\", \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n        )\n        claims_selected = claims_df.select(\n            \"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\", \"Claim_Amount\", \"Claim_Payout\"\n        )\n        policy_selected = policy_df.select(\n            \"Policy_ID\", \"Customer_ID\", \"Policy_Type\", \"Policy_Status\", \"Policy_Start_Date\", \"Policy_End_Date\",\n            \"Policy_Term\", \"Policy_Premium\", \"Total_Premium_Paid\", \"Renewal_Status\", \"Policy_Addons\"\n        )\n        logger.info(\"Field selection completed.\")\n        return demographics_selected, claims_selected, policy_selected\n    except Exception as e:\n        logger.error(f\"Error selecting fields: {e}\")\n        raise\n\ndemographics_selected, claims_selected, policy_selected = select_fields(demographics_df, claims_df, policy_df)\n\n# COMMAND ----------\n# %md\n# ## Data Integration\n# Merge datasets based on common identifiers.\n\n# COMMAND ----------\n#\n",
                "def join_datasets(demographics_selected, claims_selected, policy_selected):\n    try:\n        logger.info(\"Joining datasets...\")\n        demo_policy_joined = demographics_selected.join(policy_selected, \"Customer_ID\", \"inner\")\n        demo_policy_claims_joined = demo_policy_joined.join(claims_selected, \"Policy_ID\", \"inner\")\n        logger.info(\"Datasets joined successfully.\")\n        return demo_policy_claims_joined\n    except Exception as e:\n        logger.error(f\"Error joining datasets: {e}\")\n        raise\n\ndemo_policy_claims_joined = join_datasets(demographics_selected, claims_selected, policy_selected)\n\n# COMMAND ----------\n# %md\n# ## Data Aggregation and Summarization\n# Aggregate and summarize the data.\n\n# COMMAND ----------\n#\n",
                "def aggregate_data(demo_policy_claims_joined):\n    try:\n        logger.info(\"Aggregating data...\")\n        summarized_data = demo_policy_claims_joined.groupBy(\"Customer_ID\").agg(\n            count(\"Claim_ID\").alias(\"Total_Claims\"),\n            count(\"Policy_ID\").alias(\"Policy_Count\"),\n            max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n            avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n        )\n        logger.info(\"Data aggregation completed.\")\n        return summarized_data\n    except Exception as e:\n        logger.error(f\"Error aggregating data: {e}\")\n        raise\n\nsummarized_data = aggregate_data(demo_policy_claims_joined)\n\n# COMMAND ----------\n# %md\n# ## Custom Calculations\n# Perform custom calculations on the summarized data.\n\n# COMMAND ----------\n#\n",
                "def perform_custom_calculations(summarized_data):\n    try:\n        logger.info(\"Performing custom calculations...\")\n        final_data = summarized_data.withColumn(\n            \"Age\", floor(datediff(current_date(), to_date(col(\"Date_of_Birth\"), 'yyyy-MM-dd')) / 365)\n        ).withColumn(\n            \"Claim_To_Premium_Ratio\",\n            when(col(\"Total_Premium_Paid\") != 0, col(\"Claim_Amount\") / col(\"Total_Premium_Paid\")).otherwise(0)\n        ).withColumn(\n            \"Claims_Per_Policy\",\n            when(col(\"Policy_Count\") != 0, col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0)\n        ).withColumn(\n            \"Retention_Rate\", expr(\"0.85\")\n        ).withColumn(\n            \"Cross_Sell_Opportunities\", expr(\"'Multi-Policy Discount, Home Coverage Add-on'\")\n        ).withColumn(\n            \"Upsell_Potential\", expr(\"'Premium Vehicle Coverage'\")\n        )\n        logger.info(\"Custom calculations completed.\")\n        return final_data\n    except Exception as e:\n        logger.error(f\"Error in custom calculations: {e}\")\n        raise\n\nfinal_data = perform_custom_calculations(summarized_data)\n\n# COMMAND ----------\n# %md\n# ## Comprehensive Data Joining\n# Join with AI/ML insights and scores.\n\n# COMMAND ----------\n#\n",
                "def join_comprehensive_data(final_data, scores_df, aiml_insights_df):\n    try:\n        logger.info(\"Joining with AI/ML insights and scores...\")\n        final_customer_profile = final_data.join(scores_df, \"Customer_ID\", \"inner\") \\\n            .join(aiml_insights_df, \"Customer_ID\", \"inner\")\n        logger.info(\"Comprehensive data joining completed.\")\n        return final_customer_profile\n    except Exception as e:\n        logger.error(f\"Error joining comprehensive data: {e}\")\n        raise\n\nfinal_customer_profile = join_comprehensive_data(final_data, scores_df, aiml_insights_df)\n\n# COMMAND ----------\n# %md\n# ## Output Generation\n# Write the final output to a Unity Catalog table.\n\n# COMMAND ----------\n#\n",
                "def write_output(final_customer_profile):\n    try:\n        logger.info(\"Writing final output to Unity Catalog table...\")\n        spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.customer_360_view\")\n        final_customer_profile.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.customer_360_view\")\n        logger.info(\"Output written successfully.\")\n    except Exception as e:\n        logger.error(f\"Error writing output: {e}\")\n        raise\n\nwrite_output(final_customer_profile)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}