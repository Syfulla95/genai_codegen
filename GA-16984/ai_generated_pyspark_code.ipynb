{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nimport psycopg2\nfrom pyspark.sql.types import StringType\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Step 1: Source Extraction\ntry:\n    # Load data from Unity Catalog tables\n    pes_prep_df = spark.table(\"genai_demo.jnj.pes_prep\")\n    c19_ivl_data_df = spark.table(\"genai_demo.jnj.c19_ivl_data\")\n    c04_ekpo_df = spark.table(\"genai_demo.jnj.c04_ekpo\")\n    c04_bseg_df = spark.table(\"genai_demo.jnj.c04_bseg\")\n    pjotr_df = spark.table(\"genai_demo.jnj.pjotr_\")\n    pjotr_in_pes_df = spark.table(\"genai_demo.jnj.pjotr_in_pes\")\n    \n    # Connect to MySQL database and fetch data\n    mysql_host = dbutils.secrets.get(scope=\"mysql_scope\", key=\"host\")\n    mysql_db = dbutils.secrets.get(scope=\"mysql_scope\", key=\"database\")\n    mysql_user = dbutils.secrets.get(scope=\"mysql_scope\", key=\"username\")\n    mysql_password = dbutils.secrets.get(scope=\"mysql_scope\", key=\"password\")\n    \n    conn = psycopg2.connect(\n        host=mysql_host,\n        database=mysql_db,\n        user=mysql_user,\n        password=mysql_password\n    )\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT * FROM pjotr\")\n    pjotr_data = cursor.fetchall()\n    cursor.execute(\"SELECT * FROM pjotr_prod\")\n    pjotr_prod_data = cursor.fetchall()\n    conn.close()\n    \n    # Convert fetched data to DataFrame\n    pjotr_mysql_df = spark.createDataFrame(pjotr_data, schema=[\"column1\", \"column2\", \"column3\"])\n    pjotr_prod_mysql_df = spark.createDataFrame(pjotr_prod_data, schema=[\"column1\", \"column2\", \"column3\"])\n    \n    logger.info(\"Data extraction completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data extraction: {str(e)}\")\n    raise\n\n# Step 2: Transformation\ntry:\n    # Node 8: Multi-Field Formula\n    def clean_field(value):\n        if value in [\"\", \"NULL\", \"NA\"]:\n            return None\n        return value.strip()\n    \n    clean_udf = F.udf(clean_field, StringType())\n    \n    pes_prep_df = pes_prep_df.withColumn(\"cleaned_field\", clean_udf(F.col(\"field_to_clean\")))\n    \n    # Node 9: Select\n    selected_df = pes_prep_df.select(\"field1\", \"field2\", \"cleaned_field\")\n    \n    # Node 10: Join\n    joined_df = selected_df.join(c19_ivl_data_df, selected_df.field1 == c19_ivl_data_df.field1, \"inner\")\n    \n    # Node 11: Formula\n    transformed_df = joined_df.withColumn(\"new_field\", F.expr(\"field1 + field2\"))\n    \n    # Node 12: Union\n    union_df = transformed_df.union(pjotr_df)\n    \n    # Node 13: Filter\n    filtered_df = union_df.filter(F.col(\"new_field\") > 100)\n    \n    # Node 14: Summarize\n    summarized_df = filtered_df.groupBy(\"field1\").agg(F.sum(\"new_field\").alias(\"sum_new_field\"))\n    \n    # Node 15: Custom Calculations\n    final_df = summarized_df.withColumn(\"custom_field\", F.expr(\"sum_new_field * 2\"))\n    \n    logger.info(\"Data transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data transformation: {str(e)}\")\n    raise\n\n# Step 3: Output\ntry:\n    target_catalog = \"genai_demo\"\n    target_schema = \"jnj\"\n    \n    # Ensure schema exists\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n    \n    # Node 16: Output C03_pjotr.yxdb\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.c03_pjotr\")\n    \n    # Node 17: Output PJOTR_in_PES.yxdb\n    pjotr_in_pes_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.pjotr_in_pes\")\n    \n    # Node 18: Output C03_unmapped_to_PJOTR.yxdb\n    unmapped_df = final_df.filter(F.col(\"custom_field\").isNull())\n    unmapped_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.c03_unmapped_to_pjotr\")\n    \n    # Node 19: Output C03_pjotr_midway.yxdb\n    midway_df = final_df.filter(F.col(\"custom_field\").isNotNull())\n    midway_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.c03_pjotr_midway\")\n    \n    logger.info(\"Data output completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data output: {str(e)}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}