{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to load data from MySQL using JDBC\ndef load_mysql_data_jdbc(query, secret_scope, secret_key):\n    try:\n        # Retrieve credentials securely\n        user = dbutils.secrets.get(secret_scope, f\"{secret_key}_user\")\n        password = dbutils.secrets.get(secret_scope, f\"{secret_key}_password\")\n        host = dbutils.secrets.get(secret_scope, f\"{secret_key}_host\")\n        database = dbutils.secrets.get(secret_scope, f\"{secret_key}_database\")\n\n        # JDBC URL for MySQL\n        jdbc_url = f\"jdbc:mysql://{host}/{database}?user={user}&password={password}\"\n\n        # Load data using JDBC\n        df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"query\", query).load()\n        logger.info(f\"Loaded data from MySQL: {df.count()} records\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading data from MySQL: {str(e)}\")\n        raise\n\n# Load data from Unity Catalog tables\ndef load_unity_catalog_table(table_path):\n    try:\n        df = spark.table(table_path)\n        logger.info(f\"Loaded data from Unity Catalog table {table_path}: {df.count()} records\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading data from Unity Catalog table {table_path}: {str(e)}\")\n        return None\n\n# Load data from MySQL (Node 99)\nmysql_query_99 = \"SELECT pjotr_prod.* FROM pjotr_prod\"\ntry:\n    mysql_df_99 = load_mysql_data_jdbc(mysql_query_99, \"mysql_scope\", \"mysql_key_99\")\nexcept Exception as e:\n    logger.error(f\"Failed to load MySQL data for Node 99: {str(e)}\")\n    mysql_df_99 = None\n\n# Load static data from TextInput (Node 330)\ntext_input_df = load_unity_catalog_table(\"catalog.db.text_input\")\nif text_input_df is None:\n    logger.warning(\"TextInput table not found, proceeding with alternative logic or skipping this step.\")\n\n# Load data from C04_BSEG.yxdb (Node 499)\nbseg_df = load_unity_catalog_table(\"catalog.db.C04_BSEG\")\n\n# Load data from C04_EKPO.yxdb (Node 500)\nekpo_df = load_unity_catalog_table(\"catalog.db.C04_EKPO\")\n\n# Load data from PJOTR_in_PES.yxdb (Node 384)\npjotr_in_pes_df = load_unity_catalog_table(\"catalog.db.PJOTR_in_PES\")\n\n# Load data from C19_ivl_data.yxdb (Node 291)\nivl_data_df = load_unity_catalog_table(\"catalog.db.C19_ivl_data\")\n\n# Load data from mysql_editable (Node 403)\nmysql_query_403 = \"SELECT pjotr_prod.* FROM pjotr_prod\"\ntry:\n    mysql_df_403 = load_mysql_data_jdbc(mysql_query_403, \"mysql_scope\", \"mysql_key_403\")\nexcept Exception as e:\n    logger.error(f\"Failed to load MySQL data for Node 403: {str(e)}\")\n    mysql_df_403 = None\n\n# Load data from mysql_editable (Node 98)\nmysql_query_98 = \"SELECT pjotr.* FROM pjotr\"\ntry:\n    mysql_df_98 = load_mysql_data_jdbc(mysql_query_98, \"mysql_scope\", \"mysql_key_98\")\nexcept Exception as e:\n    logger.error(f\"Failed to load MySQL data for Node 98: {str(e)}\")\n    mysql_df_98 = None\n\n# Load data from PJOTR_.yxdb (Node 401)\npjotr_df = load_unity_catalog_table(\"catalog.db.PJOTR_\")\n\n# Load data from PES_prep.yxdb (Node 249)\npes_prep_df = load_unity_catalog_table(\"catalog.db.PES_prep\")\n\n# Apply Multi-Field Formula Transformation\ndef apply_transformations(df):\n    try:\n        transformed_df = df.withColumn(\"_field1\", F.when(F.col(\"field1\").isin(\"#\", \"UNMAPPED\", \"NULL\"), None).otherwise(F.col(\"field1\"))) \\\n                           .withColumn(\"_field2\", F.when(F.col(\"field2\").startswith(\"00\"), F.expr(\"substring(field2, 3, length(field2))\")).otherwise(F.col(\"field2\")))\n        logger.info(f\"Applied transformations: {transformed_df.count()} records\")\n        return transformed_df\n    except Exception as e:\n        logger.error(f\"Error applying transformations: {str(e)}\")\n        raise\n\nif mysql_df_99 is not None:\n    transformed_df = apply_transformations(mysql_df_99)\n    # Select Relevant Fields\n    selected_df = transformed_df.select(\"_field1\", \"_field2\")\n\n    # Perform Join Operations\n    def perform_joins(df1, df2, join_condition):\n        try:\n            if df2 is not None:\n                joined_df = df1.join(df2, join_condition, \"inner\")\n                logger.info(f\"Performed join: {joined_df.count()} records\")\n                return joined_df\n            else:\n                logger.warning(\"Skipping join operation due to missing DataFrame.\")\n                return df1\n        except Exception as e:\n            logger.error(f\"Error performing join: {str(e)}\")\n            raise\n\n    joined_df = perform_joins(selected_df, text_input_df, \"_field1\")\n\n    # Apply Custom Formula Calculations\n    def apply_custom_calculations(df):\n        try:\n            calculated_df = df.withColumn(\"new_field\", F.expr(\"field1 + field2\"))\n            logger.info(f\"Applied custom calculations: {calculated_df.count()} records\")\n            return calculated_df\n        except Exception as e:\n            logger.error(f\"Error applying custom calculations: {str(e)}\")\n            raise\n\n    calculated_df = apply_custom_calculations(joined_df)\n\n    # Union Data Streams\n    def union_dataframes(df_list):\n        try:\n            union_df = df_list[0]\n            for df in df_list[1:]:\n                union_df = union_df.union(df)\n            logger.info(f\"Unioned dataframes: {union_df.count()} records\")\n            return union_df\n        except Exception as e:\n            logger.error(f\"Error unioning dataframes: {str(e)}\")\n            raise\n\n    union_df = union_dataframes([calculated_df, bseg_df, ekpo_df])\n\n    # Filter Data\n    filtered_df = union_df.filter(F.col(\"new_field\") > 100)\n\n    # Summarize Data\n    summary_df = filtered_df.groupBy(\"_field1\").agg(F.sum(\"new_field\").alias(\"sum_new_field\"))\n\n    # Write Output to Unity Catalog tables\n    def write_to_unity_catalog(df, catalog, schema, table):\n        try:\n            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table}\")\n            logger.info(f\"Written data to {catalog}.{schema}.{table}: {df.count()} records\")\n        except Exception as e:\n            logger.error(f\"Error writing data to {catalog}.{schema}.{table}: {str(e)}\")\n            raise\n\n    write_to_unity_catalog(summary_df, \"catalog_name\", \"schema_name\", \"C03_pjotr\")\n    write_to_unity_catalog(summary_df, \"catalog_name\", \"schema_name\", \"C03_pjotr_midway\")\n    write_to_unity_catalog(summary_df, \"catalog_name\", \"schema_name\", \"C03_unmapped_to_PJOTR\")\n    write_to_unity_catalog(summary_df, \"catalog_name\", \"schema_name\", \"C03_PJOTR\")\n    write_to_unity_catalog(summary_df, \"catalog_name\", \"schema_name\", \"PJOTR_in_PES\")\nelse:\n    logger.error(\"MySQL data for Node 99 is not available, skipping transformations and subsequent steps.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}