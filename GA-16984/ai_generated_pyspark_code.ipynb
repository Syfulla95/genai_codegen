{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StringType\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Securely retrieve credentials for external systems\nmysql_user = dbutils.secrets.get(scope=\"my_scope\", key=\"mysql_user\")\nmysql_password = dbutils.secrets.get(scope=\"my_scope\", key=\"mysql_password\")\nmysql_host = dbutils.secrets.get(scope=\"my_scope\", key=\"mysql_host\")\nmysql_db = dbutils.secrets.get(scope=\"my_scope\", key=\"mysql_db\")\n\n# Function to connect to MySQL and fetch data\ndef fetch_mysql_data(query):\n    try:\n        conn = psycopg2.connect(\n            host=mysql_host,\n            database=mysql_db,\n            user=mysql_user,\n            password=mysql_password\n        )\n        cursor = conn.cursor()\n        cursor.execute(query)\n        data = cursor.fetchall()\n        cursor.close()\n        conn.close()\n        return data\n    except Exception as e:\n        logger.error(f\"Error fetching data from MySQL: {e}\")\n        raise\n\n# Load data from Unity Catalog tables\ntry:\n    pjotr_df = spark.table(\"catalog.source_db.pjotr\")\n    text_input_df = spark.sql(\"SELECT * FROM catalog.source_db.text_input\")\n    bseg_df = spark.table(\"catalog.source_db.c04_bseg\")\n    ekpo_df = spark.table(\"catalog.source_db.c04_ekpo\")\n    pjotr_in_pes_df = spark.table(\"catalog.source_db.pjotr_in_pes\")\n    ivl_data_df = spark.table(\"catalog.source_db.c19_ivl_data\")\n    mysql_editable_df = spark.table(\"catalog.source_db.mysql_editable\")\n    pjotr_yxdb_df = spark.table(\"catalog.source_db.pjotr_yxdb\")\n    pes_prep_df = spark.table(\"catalog.source_db.pes_prep\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog: {e}\")\n    raise\n\n# Apply transformations\ntry:\n    # Node 495: Field Selection\n    selected_fields_df = pes_prep_df.select(\n        \"PO business unit code\", \"PO LE code\", \"PO MRC code\", \"PO site code\",\n        \"PO source system\", \"Posting date\", \"Region\", \"Sector\", \"Site code\", \"Vision sourced data\"\n    )\n\n    # Node 404: Rename fields\n    renamed_df = pjotr_df.withColumnRenamed(\"pjotr_id\", \"PJOTR ID\") \\\n                         .withColumnRenamed(\"pjotr\", \"PJOTR\") \\\n                         .withColumnRenamed(\"pjotr_status\", \"PJOTR status\") \\\n                         .withColumnRenamed(\"pjotr_in_pes\", \"PJOTR in PES\") \\\n                         .withColumnRenamed(\"procurement_sector\", \"Procurement sector\")\n\n    # Node 409: Custom Calculations\n    custom_calculations_df = renamed_df.withColumn(\"procurement_country_code\", F.lit(\"US\")) \\\n                                       .withColumn(\"notes\", F.lit(\"Note\")) \\\n                                       .withColumn(\"porg\", F.lit(\"POrg\")) \\\n                                       .withColumn(\"le_code\", F.lit(\"LE Code\")) \\\n                                       .withColumn(\"le_name\", F.lit(\"LE Name\")) \\\n                                       .withColumn(\"mrc_code\", F.lit(\"MRC Code\")) \\\n                                       .withColumn(\"mrc_name\", F.lit(\"MRC Name\"))\n\n    # Node 248: MultiField Formula\n    transformed_df = selected_fields_df.withColumn(\"Business unit code\", F.concat(F.lit(\"BU_\"), F.col(\"Business unit code\"))) \\\n                                       .withColumn(\"FMRC code\", F.concat(F.lit(\"FMRC_\"), F.col(\"FMRC code\"))) \\\n                                       .withColumn(\"FSID code\", F.concat(F.lit(\"FSID_\"), F.col(\"FSID code\")))\n\n    # Node 100: Summarize\n    summarized_df = transformed_df.groupBy(\"LE code\").agg(\n        F.count(\"LE code\").alias(\"LE_code_count\")\n    )\n\n    # Node 386: Join\n    joined_df = custom_calculations_df.join(summarized_df, custom_calculations_df[\"LE code\"] == summarized_df[\"LE code\"], \"inner\")\n\n    # Node 470: Filter\n    filtered_df = joined_df.filter(joined_df[\"BU code\"] == \"BU_001\")\n\n    # Node 389: Union\n    union_df = filtered_df.unionAll(joined_df)\n\n    # Node 223: Summarize\n    final_summarized_df = union_df.groupBy(\"LE code\", \"MRC code\").agg(\n        F.first(\"LE name\").alias(\"First_LE_name\"),\n        F.first(\"MRC name\").alias(\"First_MRC_name\"),\n        F.sum(\"Spend\").alias(\"Total_Spend\"),\n        F.sum(\"Records\").alias(\"Total_Records\")\n    )\n\n    # Write to Unity Catalog target table\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"final_table\"\n\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    final_summarized_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table}\")\n\nexcept Exception as e:\n    logger.error(f\"Error during transformation or writing: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}