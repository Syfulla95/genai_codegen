{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to load data from Unity Catalog tables\ndef load_data_from_catalog(table_path):\n    try:\n        # Attempt to load the table from Unity Catalog\n        df = spark.table(table_path)\n        logger.info(f\"Loaded data from {table_path} with {df.count()} records\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading data from {table_path}: {str(e)}\")\n        return None  # Return None if loading fails\n\n# Function to load data from MySQL using JDBC\ndef load_data_from_mysql(query, secret_scope, secret_key):\n    try:\n        # Retrieve credentials securely\n        host = dbutils.secrets.get(secret_scope, f\"{secret_key}_host\")\n        port = dbutils.secrets.get(secret_scope, f\"{secret_key}_port\")\n        database = dbutils.secrets.get(secret_scope, f\"{secret_key}_database\")\n        user = dbutils.secrets.get(secret_scope, f\"{secret_key}_user\")\n        password = dbutils.secrets.get(secret_scope, f\"{secret_key}_password\")\n\n        # Check if any secret is missing\n        if not host or not port or not database or not user or not password:\n            raise ValueError(\"One or more secrets are missing in the scope\")\n\n        # Connect to MySQL using JDBC\n        jdbc_url = f\"jdbc:mysql://{host}:{port}/{database}\"\n        connection_properties = {\n            \"user\": user,\n            \"password\": password\n        }\n        df = spark.read.jdbc(url=jdbc_url, table=f\"({query}) as query\", properties=connection_properties)\n        logger.info(f\"Loaded data from MySQL with {df.count()} records\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading data from MySQL: {str(e)}\")\n        return None  # Return None if loading fails\n\n# Load data from Unity Catalog tables\npes_prep_df = load_data_from_catalog(\"catalog.db.PES_prep\")\nc19_ivl_data_df = load_data_from_catalog(\"catalog.db.C19_ivl_data\")\nc04_ekpo_df = load_data_from_catalog(\"catalog.db.C04_EKPO\")\nc04_bseg_df = load_data_from_catalog(\"catalog.db.C04_BSEG\")\npjotr_df = load_data_from_catalog(\"catalog.db.PJOTR_\")\npjotr_in_pes_df = load_data_from_catalog(\"catalog.db.PJOTR_in_PES\")\ntext_input_df = load_data_from_catalog(\"catalog.db.TextInput\")\n\n# Load data from MySQL\nmysql_pjotr_df = load_data_from_mysql(\"SELECT * FROM pjotr\", \"mysql_scope\", \"pjotr\")\nmysql_pjotr_prod_df = load_data_from_mysql(\"SELECT * FROM pjotr_prod\", \"mysql_scope\", \"pjotr_prod\")\n\n# Apply transformations\ndef apply_transformations(df):\n    if df is None:\n        logger.error(\"DataFrame is None, skipping transformations\")\n        return None\n    try:\n        # Example transformation: trimming and cleaning data\n        transformed_df = df.withColumn(\"trimmed_column\", F.trim(F.col(\"column_name\")))\n        logger.info(f\"Applied transformations with {transformed_df.count()} records\")\n        return transformed_df\n    except Exception as e:\n        logger.error(f\"Error applying transformations: {str(e)}\")\n        return None\n\n# Apply transformations to dataframes\npes_prep_transformed_df = apply_transformations(pes_prep_df)\nc19_ivl_data_transformed_df = apply_transformations(c19_ivl_data_df)\nc04_ekpo_transformed_df = apply_transformations(c04_ekpo_df)\nc04_bseg_transformed_df = apply_transformations(c04_bseg_df)\npjotr_transformed_df = apply_transformations(pjotr_df)\npjotr_in_pes_transformed_df = apply_transformations(pjotr_in_pes_df)\ntext_input_transformed_df = apply_transformations(text_input_df)\nmysql_pjotr_transformed_df = apply_transformations(mysql_pjotr_df)\nmysql_pjotr_prod_transformed_df = apply_transformations(mysql_pjotr_prod_df)\n\n# Perform join operations\ndef perform_joins(df1, df2, join_condition):\n    if df1 is None or df2 is None:\n        logger.error(\"One or both DataFrames are None, skipping join operation\")\n        return None\n    try:\n        joined_df = df1.join(df2, join_condition, \"inner\")\n        logger.info(f\"Performed join operation with {joined_df.count()} records\")\n        return joined_df\n    except Exception as e:\n        logger.error(f\"Error performing join operation: {str(e)}\")\n        return None\n\n# Example join operation\njoined_df = perform_joins(pes_prep_transformed_df, c19_ivl_data_transformed_df, \"join_column\")\n\n# Apply custom formula calculations\ndef apply_custom_formulas(df):\n    if df is None:\n        logger.error(\"DataFrame is None, skipping custom formulas\")\n        return None\n    try:\n        # Example custom formula\n        calculated_df = df.withColumn(\"custom_column\", F.when(F.col(\"PJOTR_ID\") == 1144, 2806).otherwise(F.col(\"PJOTR_ID\")))\n        logger.info(f\"Applied custom formulas with {calculated_df.count()} records\")\n        return calculated_df\n    except Exception as e:\n        logger.error(f\"Error applying custom formulas: {str(e)}\")\n        return None\n\n# Apply custom formulas\ncustom_formula_df = apply_custom_formulas(joined_df)\n\n# Union data streams\ndef union_data_streams(df_list):\n    if any(df is None for df in df_list):\n        logger.error(\"One or more DataFrames are None, skipping union operation\")\n        return None\n    try:\n        unioned_df = df_list[0]\n        for df in df_list[1:]:\n            unioned_df = unioned_df.union(df)\n        logger.info(f\"Unioned data streams with {unioned_df.count()} records\")\n        return unioned_df\n    except Exception as e:\n        logger.error(f\"Error unioning data streams: {str(e)}\")\n        return None\n\n# Example union operation\nunioned_df = union_data_streams([custom_formula_df, pjotr_transformed_df])\n\n# Filter data\ndef filter_data(df, filter_condition):\n    if df is None:\n        logger.error(\"DataFrame is None, skipping filter operation\")\n        return None\n    try:\n        filtered_df = df.filter(filter_condition)\n        logger.info(f\"Filtered data with {filtered_df.count()} records\")\n        return filtered_df\n    except Exception as e:\n        logger.error(f\"Error filtering data: {str(e)}\")\n        return None\n\n# Example filter operation\nfiltered_df = filter_data(unioned_df, \"filter_column > 100\")\n\n# Summarize data\ndef summarize_data(df, group_by_columns, agg_columns):\n    if df is None:\n        logger.error(\"DataFrame is None, skipping summarization\")\n        return None\n    try:\n        summarized_df = df.groupBy(group_by_columns).agg(*agg_columns)\n        logger.info(f\"Summarized data with {summarized_df.count()} records\")\n        return summarized_df\n    except Exception as e:\n        logger.error(f\"Error summarizing data: {str(e)}\")\n        return None\n\n# Example summarization\nsummarized_df = summarize_data(filtered_df, [\"PJOTR\"], [F.sum(\"Spend\").alias(\"Total_Spend\"), F.count(\"Records\").alias(\"Record_Count\")])\n\n# Write output to Unity Catalog tables\ndef write_output(df, catalog, schema, table):\n    if df is None:\n        logger.error(f\"DataFrame is None, skipping write operation to {catalog}.{schema}.{table}\")\n        return\n    try:\n        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table}\")\n        logger.info(f\"Written output to {catalog}.{schema}.{table}\")\n    except Exception as e:\n        logger.error(f\"Error writing output to {catalog}.{schema}.{table}: {str(e)}\")\n\n# Write outputs\nwrite_output(summarized_df, \"catalog\", \"db\", \"C03_pjotr\")\nwrite_output(pjotr_in_pes_transformed_df, \"catalog\", \"db\", \"PJOTR_in_PES\")\nwrite_output(filtered_df, \"catalog\", \"db\", \"C03_unmapped_to_PJOTR\")\nwrite_output(unioned_df, \"catalog\", \"db\", \"C03_pjotr_midway\")\n\nlogger.info(\"ETL workflow completed successfully\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}