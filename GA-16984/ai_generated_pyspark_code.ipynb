{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Node 1: Load customer_data from Unity Catalog\n    logger.info(\"Loading customer_data from Unity Catalog\")\n    try:\n        # Correct the catalog and schema names if necessary\n        customer_data = spark.table(\"correct_catalog.correct_schema.customer_data\")\n        logger.info(\"Successfully loaded customer_data\")\n        customer_data.printSchema()\n        logger.info(f\"customer_data record count: {customer_data.count()}\")\n    except Exception as e:\n        logger.error(f\"Failed to load customer_data: {str(e)}\")\n        # Fallback to loading from CSV if table is not found\n        logger.info(\"Attempting to load customer_data from CSV as fallback\")\n        customer_data = spark.read.csv(\"/mnt/data/customer_data.csv\", header=True, inferSchema=True)\n        logger.info(\"Successfully loaded customer_data from CSV\")\n        customer_data.printSchema()\n        logger.info(f\"customer_data record count: {customer_data.count()}\")\n\n    # Node 2: Load transaction_data from Unity Catalog\n    logger.info(\"Loading transaction_data from Unity Catalog\")\n    try:\n        # Correct the catalog and schema names if necessary\n        transaction_data = spark.table(\"correct_catalog.correct_schema.transaction_data\")\n        logger.info(\"Successfully loaded transaction_data\")\n        transaction_data.printSchema()\n        logger.info(f\"transaction_data record count: {transaction_data.count()}\")\n    except Exception as e:\n        logger.error(f\"Failed to load transaction_data: {str(e)}\")\n        # Fallback to loading from CSV if table is not found\n        logger.info(\"Attempting to load transaction_data from CSV as fallback\")\n        transaction_data = spark.read.csv(\"/mnt/data/transaction_data.csv\", header=True, inferSchema=True)\n        logger.info(\"Successfully loaded transaction_data from CSV\")\n        transaction_data.printSchema()\n        logger.info(f\"transaction_data record count: {transaction_data.count()}\")\n\n    # Node 3: Filter transaction_data\n    logger.info(\"Filtering transaction_data where transaction_amount > 100\")\n    filtered_transaction_data = transaction_data.filter(F.col(\"transaction_amount\") > 100)\n    filtered_transaction_data.printSchema()\n    logger.info(f\"filtered_transaction_data record count: {filtered_transaction_data.count()}\")\n\n    # Node 4: Join customer_data with filtered transaction_data\n    logger.info(\"Joining customer_data with filtered_transaction_data on customer_id\")\n    joined_data = customer_data.alias(\"cust\").join(\n        filtered_transaction_data.alias(\"trans\"), F.col(\"cust.customer_id\") == F.col(\"trans.customer_id\"), \"inner\"\n    ).select(\"cust.*\", \"trans.transaction_amount\")\n    joined_data.printSchema()\n    logger.info(f\"joined_data record count: {joined_data.count()}\")\n\n    # Node 5: Aggregate data\n    logger.info(\"Aggregating joined_data to calculate total_spent per customer_id\")\n    aggregated_data = joined_data.groupBy(\"customer_id\").agg(F.sum(\"transaction_amount\").alias(\"total_spent\"))\n    aggregated_data.printSchema()\n    logger.info(f\"aggregated_data record count: {aggregated_data.count()}\")\n\n    # Node 6: Write to customer_spending_summary in Unity Catalog\n    target_catalog = \"correct_catalog\"\n    target_schema = \"correct_schema\"\n    target_table = \"customer_spending_summary\"\n\n    # Ensure schema exists before creating table\n    logger.info(f\"Ensuring schema {target_catalog}.{target_schema} exists\")\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    logger.info(f\"Writing aggregated data to {target_catalog}.{target_schema}.{target_table}\")\n    try:\n        aggregated_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n        logger.info(\"Write operation completed successfully\")\n    except Exception as e:\n        logger.error(f\"Failed to write to Unity Catalog: {str(e)}\")\n        # Fallback to writing to CSV if table write fails\n        logger.info(\"Attempting to write aggregated data to CSV as fallback\")\n        aggregated_data.write.csv(\"/mnt/output/customer_spending_summary.csv\", header=True)\n        logger.info(\"Successfully wrote aggregated data to CSV\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {str(e)}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}