{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import when, trim, col, concat, count\nimport psycopg2\nfrom pyspark.sql.utils import AnalysisException\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume Spark session is pre-initialized as 'spark'\n\n# Node 1: Data Source Loading\ntry:\n    # Load data from Unity Catalog tables\n    pes_prep_df = spark.table(\"genai_demo.jnj.pes_prep\")\n    c19_ivl_data_df = spark.table(\"genai_demo.jnj.c19_ivl_data\")\n    c04_ekpo_df = spark.table(\"genai_demo.jnj.c04_ekpo\")\n    c04_bseg_df = spark.table(\"genai_demo.jnj.c04_bseg\")\n    pjotr_df = spark.table(\"genai_demo.jnj.pjotr_\")\n    pjotr_in_pes_df = spark.table(\"genai_demo.jnj.pjotr_in_pes\")\n    \n    # Load data from external PostgreSQL database\n    pg_host = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_host\")\n    pg_port = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_port\")\n    pg_db = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_db\")\n    pg_user = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_user\")\n    pg_password = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_password\")\n    \n    conn = psycopg2.connect(\n        host=pg_host,\n        port=pg_port,\n        database=pg_db,\n        user=pg_user,\n        password=pg_password\n    )\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT * FROM pjotr\")\n    pg_data = cursor.fetchall()\n    pg_columns = [desc[0] for desc in cursor.description]\n    mysql_df = spark.createDataFrame(pg_data, schema=pg_columns)\n    cursor.close()\n    conn.close()\n    \n    logger.info(\"Data source loading completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error loading data sources: {e}\")\n    raise\n\n# Node 2: Multi-Field Formula Transformation\ntry:\n    transformed_df = pes_prep_df.withColumn(\"_Business_unit_code\", when(trim(col(\"Business unit code\")) == \"#\", None).otherwise(trim(col(\"Business unit code\"))))\n    # Apply similar transformations for other fields as needed\n    logger.info(\"Multi-field formula transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in multi-field formula transformation: {e}\")\n    raise\n\n# Node 3: Select Transformation\ntry:\n    selected_df = transformed_df.select(\"_Business_unit_code\", \"_FMRC_code\", \"_FSID_code\", \"_LE_code\", \"_MRC_code\")\n    logger.info(\"Select transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in select transformation: {e}\")\n    raise\n\n# Node 4: Join Transformation\ntry:\n    joined_df = selected_df.join(c04_ekpo_df, selected_df[\"_LE_code\"] == c04_ekpo_df[\"LE code\"], \"inner\")\n    logger.info(\"Join transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in join transformation: {e}\")\n    raise\n\n# Node 5: Formula Transformation\ntry:\n    formula_df = joined_df.withColumn(\"PJOTR_ID\", when(col(\"PJOTR ID\") == 1144, 2806).otherwise(col(\"PJOTR ID\")))\n    logger.info(\"Formula transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in formula transformation: {e}\")\n    raise\n\n# Node 6: Union Transformation\ntry:\n    union_df = formula_df.union(pjotr_in_pes_df)\n    logger.info(\"Union transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in union transformation: {e}\")\n    raise\n\n# Node 7: Filter Transformation\ntry:\n    filtered_df = union_df.filter(col(\"map\") == \"yes\")\n    logger.info(\"Filter transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in filter transformation: {e}\")\n    raise\n\n# Node 8: Summarize Transformation\ntry:\n    summarized_df = filtered_df.groupBy(\"LE code\").agg(count(\"*\").alias(\"record_count\"))\n    logger.info(\"Summarize transformation completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in summarize transformation: {e}\")\n    raise\n\n# Node 9: Custom Calculations\ntry:\n    custom_df = summarized_df.withColumn(\"temp_LE_MRC\", concat(col(\"_LE code\"), col(\"_MRC code\")))\n    logger.info(\"Custom calculations completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error in custom calculations: {e}\")\n    raise\n\n# Node 10: Output Data Sources\ntry:\n    target_catalog = \"genai_demo\"\n    target_schema = \"jnj\"\n    target_table = \"c03_pjotr\"\n    \n    # Ensure schema exists before creating table\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n    \n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    custom_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(\"Output data sources written successfully.\")\nexcept AnalysisException as ae:\n    logger.error(f\"Error in output data sources: {ae}\")\n    raise\nexcept Exception as e:\n    logger.error(f\"Unexpected error in output data sources: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}