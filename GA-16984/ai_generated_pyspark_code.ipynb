{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.utils import AnalysisException\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Step 1: Source Extraction\ntry:\n    # Load data from Unity Catalog tables\n    pes_prep_df = spark.table(\"genai_demo.jnj.pes_prep\")\n    c19_ivl_data_df = spark.table(\"genai_demo.jnj.c19_ivl_data\")\n    c04_ekpo_df = spark.table(\"genai_demo.jnj.c04_ekpo\")\n    c04_bseg_df = spark.table(\"genai_demo.jnj.c04_bseg\")\n    pjotr_df = spark.table(\"genai_demo.jnj.pjotr_\")\n    pjotr_in_pes_df = spark.table(\"genai_demo.jnj.pjotr_in_pes\")\n    pjotr_prod_df = spark.table(\"genai_demo.jnj.pjotr_prod\")\n\n    # Log record counts and schema details\n    logger.info(f\"pes_prep_df: {pes_prep_df.count()} records, schema: {pes_prep_df.schema}\")\n    logger.info(f\"c19_ivl_data_df: {c19_ivl_data_df.count()} records, schema: {c19_ivl_data_df.schema}\")\n    logger.info(f\"c04_ekpo_df: {c04_ekpo_df.count()} records, schema: {c04_ekpo_df.schema}\")\n    logger.info(f\"c04_bseg_df: {c04_bseg_df.count()} records, schema: {c04_bseg_df.schema}\")\n    logger.info(f\"pjotr_df: {pjotr_df.count()} records, schema: {pjotr_df.schema}\")\n    logger.info(f\"pjotr_in_pes_df: {pjotr_in_pes_df.count()} records, schema: {pjotr_in_pes_df.schema}\")\n    logger.info(f\"pjotr_prod_df: {pjotr_prod_df.count()} records, schema: {pjotr_prod_df.schema}\")\nexcept AnalysisException as e:\n    logger.error(f\"Error loading source data: {e}\")\n    raise\n\n# Step 2: Data Cleansing and Standardization\ntry:\n    # Example transformation: trimming whitespace and removing specific values\n    pes_prep_df = pes_prep_df.withColumn(\"trimmed_field\", F.trim(F.col(\"field_name\")))\n    # Log cleaned schema and record counts\n    logger.info(f\"Cleaned pes_prep_df: {pes_prep_df.count()} records, schema: {pes_prep_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during data cleansing: {e}\")\n    raise\n\n# Step 3: Field Selection\ntry:\n    # Select relevant fields\n    selected_fields_df = pes_prep_df.select(\"field1\", \"field2\", \"trimmed_field\")\n    # Log selected fields and schema\n    logger.info(f\"Selected fields: {selected_fields_df.columns}, schema: {selected_fields_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during field selection: {e}\")\n    raise\n\n# Step 4: Data Integration\ntry:\n    # Perform join operations\n    joined_df = selected_fields_df.join(c19_ivl_data_df, \"common_field\", \"inner\")\n    # Drop duplicate columns\n    joined_df = joined_df.drop(\"duplicate_field\")\n    # Log updated record counts and schema\n    logger.info(f\"Joined data: {joined_df.count()} records, schema: {joined_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during data integration: {e}\")\n    raise\n\n# Step 5: Custom Calculations\ntry:\n    # Implement custom logic\n    custom_calculations_df = joined_df.withColumn(\"new_field\", F.expr(\"field1 + field2\"))\n    # Log new or modified fields and schema\n    logger.info(f\"Custom calculations: {custom_calculations_df.columns}, schema: {custom_calculations_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during custom calculations: {e}\")\n    raise\n\n# Step 6: Data Merging\ntry:\n    # Merge data streams\n    merged_df = custom_calculations_df.union(pjotr_df)\n    # Log merged data stream and schema\n    logger.info(f\"Merged data: {merged_df.count()} records, schema: {merged_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during data merging: {e}\")\n    raise\n\n# Step 7: Data Filtering\ntry:\n    # Apply filter conditions\n    filtered_df = merged_df.filter(F.col(\"new_field\") > 100)\n    # Log filtered data and schema\n    logger.info(f\"Filtered data: {filtered_df.count()} records, schema: {filtered_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during data filtering: {e}\")\n    raise\n\n# Step 8: Data Aggregation\ntry:\n    # Aggregate data\n    aggregated_df = filtered_df.groupBy(\"group_field\").agg(F.sum(\"new_field\").alias(\"sum_new_field\"))\n    # Log aggregated fields and schema\n    logger.info(f\"Aggregated data: {aggregated_df.count()} records, schema: {aggregated_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during data aggregation: {e}\")\n    raise\n\n# Step 9: Field Cleanup\ntry:\n    # Rename columns\n    cleaned_df = aggregated_df.withColumnRenamed(\"sum_new_field\", \"total_new_field\")\n    # Log cleaned schema and record counts\n    logger.info(f\"Cleaned data: {cleaned_df.count()} records, schema: {cleaned_df.schema}\")\nexcept Exception as e:\n    logger.error(f\"Error during field cleanup: {e}\")\n    raise\n\n# Step 10: Load to Target\ntry:\n    # Ensure schema exists before creating table\n    target_catalog = \"genai_demo\"\n    target_schema = \"jnj\"\n    target_table = \"final_transformed_data\"\n    \n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n    \n    # Write to Unity Catalog target table\n    cleaned_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data successfully loaded to {target_catalog}.{target_schema}.{target_table}\")\nexcept Exception as e:\n    logger.error(f\"Error during data loading: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}