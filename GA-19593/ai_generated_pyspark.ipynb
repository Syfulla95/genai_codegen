{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\r\nfrom datetime import datetime\r\nfrom pyspark.sql import functions as F\r\n\r\n# Configure logging\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(\"GA_Data_Processing_Pipeline\")\r\n\r\ntry:\r\n    # Step 1: Load Data Sources\r\n    logger.info(\"Loading data sources...\")\r\n    \r\n    # Load static distribution channel information\r\n    channel_df = spark.table(\"catalog.source_db.channel\")\r\n    \r\n    # Load manually entered date fields\r\n    manual_date_df = spark.table(\"catalog.source_db.manual_date\")\r\n    \r\n    # Load tdmedpod table\r\n    tdmedpod_df = spark.sql(\"\"\"\r\n        SELECT * FROM catalog.source_db.tdmedpod\r\n        WHERE BILL_DTE = '2019-07-01' AND Whs = 'D0CG'\r\n    \"\"\")\r\n    \r\n    # Load dbo.tableupdated_new_1000 table\r\n    tableupdated_df = spark.sql(\"\"\"\r\n        SELECT \r\n            SO_AUDAT, FKDAT, WERKS, VTWEG, ZZFINCLASS, BEZEK, SOLDTO_KUNNR, SHIPTO_KUNNR, VGBEL,\r\n            COUNT(DISTINCT VGBEL) AS Distinct_VBELN_Count,\r\n            SUM(BILL_ITM_COUNT) AS BILL_ITM_COUNT,\r\n            SUM(UNIT_LAND_COST) AS UNIT_LAND_COST,\r\n            SUM(SO_NETWR) AS SO_NETWR,\r\n            SUM(SO_NETPR) AS SO_NETPR\r\n        FROM catalog.source_db.tableupdated_new_1000\r\n        WHERE FKDAT BETWEEN '1999-01-01' AND '1999-01-31'\r\n        GROUP BY SO_AUDAT, FKDAT, WERKS, VTWEG, ZZFINCLASS, BEZEK, SOLDTO_KUNNR, SHIPTO_KUNNR, VGBEL\r\n    \"\"\")\r\n\r\n    # Step 2: Date Transformations\r\n    logger.info(\"Performing date transformations...\")\r\n    \r\n    current_date = datetime.now().strftime('%Y-%m-%d')\r\n    manual_date_df = manual_date_df.withColumn(\"DateTime_Out\", F.lit(current_date))\r\n    \r\n    # Calculate date ranges\r\n    manual_date_df = manual_date_df.withColumn(\"Prior_Week_Start\", F.date_sub(F.current_date(), 7)) \\\r\n                                   .withColumn(\"Prior_Week_End\", F.date_sub(F.current_date(), 1)) \\\r\n                                   .withColumn(\"Yesterday\", F.date_sub(F.current_date(), 1)) \\\r\n                                   .withColumn(\"Today\", F.current_date()) \\\r\n                                   .withColumn(\"P1M_Start\", F.add_months(F.current_date(), -1)) \\\r\n                                   .withColumn(\"P1M_End\", F.last_day(F.add_months(F.current_date(), -1)))\r\n\r\n    # Step 3: Data Cleansing\r\n    logger.info(\"Performing data cleansing...\")\r\n    \r\n    tdmedpod_df = tdmedpod_df.fillna(0, subset=[\"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\"])\r\n    \r\n    # Identify null values in primary key fields\r\n    tdmedpod_df = tdmedpod_df.withColumn(\"null_yn\", F.when(\r\n        F.col(\"FNC_ID\").isNull() | F.col(\"Whs\").isNull() | F.col(\"DIST_CHNL_ID\").isNull() |\r\n        F.col(\"SOLDTO\").isNull() | F.col(\"SHIPTO\").isNull(), \"Y\").otherwise(\"N\"))\r\n\r\n    # Step 4: Data Aggregation\r\n    logger.info(\"Performing data aggregation...\")\r\n    \r\n    aggregated_df = tdmedpod_df.groupBy(\"BILL_DATE\").agg(\r\n        F.sum(\"Invoices\").alias(\"Sum_Invoices\"),\r\n        F.sum(\"Invoice_Lines\").alias(\"Sum_Invoice_Lines\"),\r\n        F.sum(\"LANDED_COST\").alias(\"Sum_LANDED_COST\"),\r\n        F.sum(\"EXT_FINAL_PRICE\").alias(\"Sum_EXT_FINAL_PRICE\"),\r\n        F.sum(\"Trans_Charge_Amt\").alias(\"Sum_Trans_Charge_Amt\"),\r\n        F.sum(\"RESTOCK_Fee\").alias(\"Sum_RESTOCK_Fee\"),\r\n        F.sum(\"Special_Hndl_Amt\").alias(\"Sum_Special_Hndl_Amt\"),\r\n        F.sum(\"Vendor_Hndl_Amt\").alias(\"Sum_Vendor_Hndl_Amt\"),\r\n        F.sum(\"MOC_Amt\").alias(\"Sum_MOC_Amt\"),\r\n        F.sum(\"Fuel_Surcharge\").alias(\"Sum_Fuel_Surcharge\")\r\n    )\r\n\r\n    # Step 5: Custom Calculations\r\n    logger.info(\"Performing custom calculations...\")\r\n    \r\n    aggregated_df = aggregated_df.withColumn(\"BIA_SHIP_HNDL_AMT\", \r\n        F.col(\"Sum_Trans_Charge_Amt\") + F.col(\"Sum_RESTOCK_Fee\") + F.col(\"Sum_Special_Hndl_Amt\") +\r\n        F.col(\"Sum_Vendor_Hndl_Amt\") + F.col(\"Sum_MOC_Amt\") + F.col(\"Sum_Fuel_Surcharge\")\r\n    )\r\n    \r\n    aggregated_df = aggregated_df.withColumn(\"COE_SHIP_HNDL_AMT\", \r\n        F.col(\"BIA_SHIP_HNDL_AMT\") + F.col(\"Rush_Order_Fee\") + F.col(\"VENDR_TRANS_CHRG_FRT_ZTV1\") +\r\n        F.col(\"MARKUP_VENDOR_TRANS_FEE_AMT_ZMT1\")\r\n    )\r\n    \r\n    aggregated_df = aggregated_df.withColumn(\"Invoice_Sales\", F.col(\"Sum_EXT_FINAL_PRICE\"))\r\n\r\n    # Step 6: Data Enrichment\r\n    logger.info(\"Performing data enrichment...\")\r\n    \r\n    tdmedpod_df = tdmedpod_df.withColumn(\"FNC_ID\", F.when(F.col(\"FNC_ID\").isNull(), \"OTH\").otherwise(F.col(\"FNC_ID\"))) \\\r\n                             .withColumn(\"FNC_DESC\", F.when(F.col(\"FNC_DESC\").isNull(), \"OTHER\").otherwise(F.col(\"FNC_DESC\")))\r\n\r\n    # Step 7: Data Integration\r\n    logger.info(\"Performing data integration...\")\r\n    \r\n    final_df = tdmedpod_df.join(channel_df, tdmedpod_df[\"DIST_CHNL_ID\"] == channel_df[\"DIST_CHNL\"], \"left\")\r\n\r\n    # Step 8: Output\r\n    logger.info(\"Writing final output to Unity Catalog...\")\r\n    \r\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.final_output\")\r\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.final_output\")\r\n\r\n    logger.info(\"Pipeline execution completed successfully.\")\r\n\r\nexcept Exception as e:\r\n    logger.error(f\"Pipeline execution failed: {e}\")\r\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}