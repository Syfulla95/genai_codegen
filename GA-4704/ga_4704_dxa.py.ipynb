{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# %md\n# # ETL Process for Superstore Data\n# This notebook performs an ETL process on Superstore data, including loading, transforming, and saving data.\n\n# COMMAND ----------\n#\n",
                "# Import necessary libraries\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, concat_ws, year, datediff, when, sum, avg\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is already available as 'spark'\n\n# COMMAND ----------\n#\n",
                "# Define state mapping for renaming states\nstate_mapping = {\n    \"California\": \"CA\", \"New York\": \"NY\", \"Texas\": \"TX\", \"Florida\": \"FL\",\n    \"Illinois\": \"IL\", \"Pennsylvania\": \"PA\", \"Ohio\": \"OH\", \"Georgia\": \"GA\",\n    \"North Carolina\": \"NC\"\n}\n\n# COMMAND ----------\n#\n",
                "try:\n    # Load Orders Central\n    logger.info(\"Loading Orders Central data\")\n    orders_central = spark.read.csv(\"dbfs:/mnt/data/Orders_Central.csv\", header=True, inferSchema=True)\n    orders_central = orders_central.withColumn(\"Region\", F.col(\"Region\").fillna(\"Central\"))\n\n    # COMMAND ----------\n    #\n",
                "# Fix Dates\n    logger.info(\"Fixing dates for Orders Central\")\n    orders_central = orders_central.withColumn(\"Order Date\", concat_ws(\"-\", F.col(\"Order Year\"), F.col(\"Order Month\"), F.col(\"Order Day\")).cast(\"date\")) \\\n                                   .withColumn(\"Ship Date\", concat_ws(\"-\", F.col(\"Ship Year\"), F.col(\"Ship Month\"), F.col(\"Ship Day\")).cast(\"date\")) \\\n                                   .drop(\"Order Year\", \"Order Month\", \"Order Day\", \"Ship Year\", \"Ship Month\", \"Ship Day\") \\\n                                   .withColumnRenamed(\"Discounts\", \"Discount\") \\\n                                   .withColumnRenamed(\"Product\", \"Product Name\")\n\n    # COMMAND ----------\n    #\n",
                "# Remove Nulls\n    logger.info(\"Removing null Order IDs\")\n    orders_central = orders_central.filter(F.col(\"Order ID\").isNotNull())\n\n    # COMMAND ----------\n    #\n",
                "# Fix Data Type\n    logger.info(\"Fixing data types for Orders Central\")\n    orders_central = orders_central.withColumn(\"Discount\", F.col(\"Discount\").cast(\"string\")) \\\n                                   .withColumn(\"Sales\", F.col(\"Sales\").cast(\"double\"))\n\n    # COMMAND ----------\n    #\n",
                "# Rename States\n    logger.info(\"Renaming states in Orders Central\")\n    orders_central = orders_central.replace(state_mapping, subset=[\"State\"])\n\n    # COMMAND ----------\n    #\n",
                "# Load and Pivot Quotas\n    logger.info(\"Loading and pivoting Quota data\")\n    quota = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(\"dbfs:/mnt/data/Quota.xlsx\")\n    quota = quota.selectExpr(\"Region\", \"stack(4, '2014', `2014`, '2015', `2015`, '2016', `2016`, '2017', `2017`) as (Year, Quota)\")\n\n    # COMMAND ----------\n    #\n",
                "# Load Orders West\n    logger.info(\"Loading Orders West data\")\n    orders_west = spark.read.csv(\"dbfs:/mnt/data/Orders_West.csv\", header=True, inferSchema=True)\n\n    # COMMAND ----------\n    #\n",
                "# Load Orders East\n    logger.info(\"Loading Orders East data\")\n    orders_east = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(\"dbfs:/mnt/data/Orders_East.xlsx\")\n\n    # COMMAND ----------\n    #\n",
                "# Load Orders South\n    logger.info(\"Loading Orders South data\")\n    orders_south = spark.read.csv(\"dbfs:/mnt/data/orders_south_2015.csv\", header=True, inferSchema=True)\n\n    # COMMAND ----------\n    #\n",
                "# Union Orders\n    logger.info(\"Unioning all order datasets\")\n    orders_all = orders_central.union(orders_west).union(orders_east).union(orders_south)\n\n    # COMMAND ----------\n    #\n",
                "# Load Returns\n    logger.info(\"Loading Returns data\")\n    returns = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(\"dbfs:/mnt/data/return reasons_new.xlsx\")\n\n    # COMMAND ----------\n    #\n",
                "# Orders and Returns\n    logger.info(\"Joining Orders and Returns data\")\n    # Refactor complex operations into named variables for clarity\n    returned_col = F.col(\"Return Reason\").isNotNull()\n    days_to_ship_col = datediff(F.col(\"Ship Date\"), F.col(\"Order Date\"))\n    discount_col = when(F.col(\"Discount\").isNull(), F.lit(0)).otherwise(F.col(\"Discount\"))\n    year_of_sale_col = year(F.col(\"Order Date\"))\n\n    # Use left join by reversing the order of the dataframes\n    orders_returns = returns.join(orders_all, [\"Order ID\", \"Product ID\"], \"left_outer\") \\\n                            .withColumn(\"Returned\", returned_col) \\\n                            .withColumn(\"Days to Ship\", days_to_ship_col) \\\n                            .withColumn(\"Discount\", discount_col) \\\n                            .withColumn(\"Year of Sale\", year_of_sale_col)\n\n    # COMMAND ----------\n    #\n",
                "# Roll Up Sales\n    logger.info(\"Aggregating sales data\")\n    annual_performance = orders_returns.groupBy(\"Region\", \"Year of Sale\").agg(\n        sum(\"Profit\").alias(\"Total Profit\"),\n        sum(\"Sales\").alias(\"Total Sales\"),\n        sum(\"Quantity\").alias(\"Total Quantity\"),\n        avg(\"Discount\").alias(\"Average Discount\")\n    )\n\n    # COMMAND ----------\n    #\n",
                "# Save Outputs\n    logger.info(\"Saving Annual Regional Performance data\")\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.Annual_Regional_Performance\")\n    annual_performance.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.Annual_Regional_Performance\")\n\n    logger.info(\"Saving Superstore Sales data\")\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.Superstore_Sales\")\n    orders_returns.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.Superstore_Sales\")\n\nexcept Exception as e:\n    logger.error(\"An error occurred during the ETL process\", exc_info=True)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}