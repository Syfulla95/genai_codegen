{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    logger.info(\"Loading data from Unity Catalog tables...\")\n    customers_df = spark.table(\"genai_demo.sas.customers\")\n    transactions_df = spark.table(\"genai_demo.sas.transactions\")\n\n    # Step 1: Filter valid transactions\n    logger.info(\"Filtering valid transactions...\")\n    valid_txns_df = transactions_df.filter((F.col(\"Sales\") > 0) & (F.col(\"Product\").isNotNull()))\n\n    # Step 2: Calculate Effective Price\n    logger.info(\"Calculating Effective Price...\")\n    trans_step2_df = valid_txns_df.withColumn(\"EffectivePrice\", F.col(\"Sales\") * (1 - F.col(\"Discount\") / 100))\n\n    # Step 3: Calculate Total Value\n    logger.info(\"Calculating Total Value...\")\n    trans_step3_df = trans_step2_df.withColumn(\"TotalValue\", F.col(\"EffectivePrice\") * F.col(\"Quantity\"))\n\n    # Step 4: Join with Customers\n    logger.info(\"Joining transactions with customers...\")\n    full_data_df = trans_step3_df.join(customers_df, \"CustomerID\", \"left\").select(\n        trans_step3_df[\"*\"], customers_df[\"Region\"], customers_df[\"JoinDate\"]\n    )\n\n    # Step 5: Calculate Tenure Days\n    logger.info(\"Calculating Tenure Days...\")\n    trans_step5_df = full_data_df.withColumn(\"TenureDays\", F.datediff(F.col(\"TransDate\"), F.col(\"JoinDate\")))\n\n    # Step 6: Assign Tenure Category\n    logger.info(\"Assigning Tenure Category...\")\n    trans_step6_df = trans_step5_df.withColumn(\n        \"TenureCategory\",\n        F.when(F.col(\"TenureDays\") < 180, \"New\")\n        .when(F.col(\"TenureDays\") < 365, \"Medium\")\n        .otherwise(\"Loyal\")\n    )\n\n    # Step 7: Flag High Value Transactions\n    logger.info(\"Flagging High Value Transactions...\")\n    trans_step7_df = trans_step6_df.withColumn(\"HighValueFlag\", F.col(\"TotalValue\") > 2000)\n\n    # Step 8: Classify Product Group\n    logger.info(\"Classifying Product Group...\")\n    trans_step8_df = trans_step7_df.withColumn(\n        \"ProductGroup\",\n        F.when(F.col(\"Product\").isin([\"A\", \"C\"]), \"Core\").otherwise(\"Non-Core\")\n    )\n\n    # Step 9: Prepare Final Data\n    logger.info(\"Preparing Final Data...\")\n    final_data_df = trans_step8_df\n\n    # Step 10: Sort by Product Group\n    logger.info(\"Sorting by Product Group...\")\n    sorted_final_data_df = final_data_df.orderBy(\"ProductGroup\")\n\n    # Step 11: Standardize Total Value\n    logger.info(\"Standardizing Total Value...\")\n    window_spec = Window.partitionBy(\"ProductGroup\")\n    standardized_df = sorted_final_data_df.withColumn(\n        \"StandardizedTotalValue\",\n        (F.col(\"TotalValue\") - F.avg(\"TotalValue\").over(window_spec)) / F.stddev(\"TotalValue\").over(window_spec)\n    )\n\n    # Step 12: Detect Outliers\n    logger.info(\"Detecting Outliers...\")\n    enhanced_final_data_df = standardized_df.withColumn(\n        \"OutlierFlag\", F.abs(F.col(\"StandardizedTotalValue\")) > 2\n    )\n\n    # Step 13: Generate Summary Statistics by Region and Product Group\n    logger.info(\"Generating Summary Statistics by Region and Product Group...\")\n    summary_stats_df = enhanced_final_data_df.groupBy(\"Region\", \"ProductGroup\").agg(\n        F.mean(\"TotalValue\").alias(\"MeanTotalValue\"),\n        F.sum(\"TotalValue\").alias(\"SumTotalValue\"),\n        F.mean(\"Quantity\").alias(\"MeanQuantity\"),\n        F.sum(\"Quantity\").alias(\"SumQuantity\"),\n        F.mean(\"Sales\").alias(\"MeanSales\"),\n        F.sum(\"Sales\").alias(\"SumSales\")\n    )\n\n    # Step 14: Frequency of Tenure Category by Region\n    logger.info(\"Calculating Frequency of Tenure Category by Region...\")\n    tenure_freq_df = enhanced_final_data_df.groupBy(\"Region\", \"TenureCategory\").count()\n\n    # Step 15: Correlation Analysis\n    logger.info(\"Performing Correlation Analysis...\")\n    correlation_df = enhanced_final_data_df.select(\"Sales\", \"Discount\", \"Quantity\", \"TotalValue\").corr()\n\n    # Step 16: Summary by Outlier Flag\n    logger.info(\"Generating Summary by Outlier Flag...\")\n    outlier_summary_df = enhanced_final_data_df.groupBy(\"OutlierFlag\").agg(\n        F.mean(\"Sales\").alias(\"MeanSales\"),\n        F.stddev(\"Sales\").alias(\"StdDevSales\"),\n        F.mean(\"TotalValue\").alias(\"MeanTotalValue\"),\n        F.stddev(\"TotalValue\").alias(\"StdDevTotalValue\"),\n        F.mean(\"Quantity\").alias(\"MeanQuantity\"),\n        F.stddev(\"Quantity\").alias(\"StdDevQuantity\")\n    )\n\n    # Write the final data to Unity Catalog tables\n    logger.info(\"Writing final data to Unity Catalog tables...\")\n    enhanced_final_data_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.sas.enhanced_final_data\")\n    summary_stats_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.sas.summary_stats\")\n    tenure_freq_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.sas.tenure_freq\")\n    outlier_summary_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.sas.outlier_summary\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}