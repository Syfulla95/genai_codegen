{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    logger.info(\"Loading data from Unity Catalog tables...\")\n    customers_df = spark.table(\"genai_demo.sas.customers\")\n    transactions_df = spark.table(\"genai_demo.sas.transactions\")\n\n    # Step 1: Valid Transactions Filtering\n    logger.info(\"Filtering valid transactions...\")\n    valid_txns_df = transactions_df.filter((F.col(\"Sales\") > 0) & (F.col(\"Product\") != \"\"))\n\n    # Step 2: Effective Price Calculation\n    logger.info(\"Calculating effective price...\")\n    trans_step2_df = valid_txns_df.withColumn(\"EffectivePrice\", F.col(\"Sales\") * (1 - F.col(\"Discount\") / 100))\n\n    # Step 3: Total Value Calculation\n    logger.info(\"Calculating total value...\")\n    trans_step3_df = trans_step2_df.withColumn(\"TotalValue\", F.col(\"EffectivePrice\") * F.col(\"Quantity\"))\n\n    # Step 4: Full Data Join\n    logger.info(\"Joining transaction data with customer information...\")\n    full_data_df = trans_step3_df.join(customers_df, \"CustomerID\", \"left\").select(\n        trans_step3_df[\"*\"], customers_df[\"Region\"], customers_df[\"JoinDate\"]\n    )\n\n    # Step 5: Tenure Days Calculation\n    logger.info(\"Calculating tenure days...\")\n    trans_step5_df = full_data_df.withColumn(\"TenureDays\", F.datediff(F.col(\"TransDate\"), F.col(\"JoinDate\")))\n\n    # Step 6: Tenure Category Assignment\n    logger.info(\"Assigning tenure categories...\")\n    trans_step6_df = trans_step5_df.withColumn(\n        \"TenureCategory\",\n        F.when(F.col(\"TenureDays\") < 180, \"New\")\n        .when(F.col(\"TenureDays\") < 365, \"Medium\")\n        .otherwise(\"Loyal\")\n    )\n\n    # Step 7: High Value Flag\n    logger.info(\"Flagging high-value transactions...\")\n    trans_step7_df = trans_step6_df.withColumn(\"HighValueFlag\", F.col(\"TotalValue\") > 2000)\n\n    # Step 8: Product Group Assignment\n    logger.info(\"Assigning product groups...\")\n    trans_step8_df = trans_step7_df.withColumn(\n        \"ProductGroup\",\n        F.when(F.col(\"Product\").isin(\"A\", \"C\"), \"Core\").otherwise(\"Non-Core\")\n    )\n\n    # Final Data Preparation\n    logger.info(\"Preparing final data...\")\n    final_data_df = trans_step8_df\n\n    # Sorting for Standardization\n    logger.info(\"Sorting data by ProductGroup...\")\n    sorted_final_data_df = final_data_df.orderBy(\"ProductGroup\")\n\n    # Z-score Standardization\n    logger.info(\"Standardizing TotalValue...\")\n    window_spec = Window.partitionBy(\"ProductGroup\")\n    standardized_df = sorted_final_data_df.withColumn(\n        \"StandardizedTotalValue\",\n        (F.col(\"TotalValue\") - F.mean(\"TotalValue\").over(window_spec)) / F.stddev(\"TotalValue\").over(window_spec)\n    )\n\n    # Outlier Detection\n    logger.info(\"Detecting outliers...\")\n    enhanced_final_data_df = standardized_df.withColumn(\n        \"OutlierFlag\",\n        F.when(F.abs(F.col(\"StandardizedTotalValue\")) > 2, 1).otherwise(0)\n    )\n\n    # Write the processed data to Unity Catalog tables\n    logger.info(\"Writing enhanced final data to Unity Catalog table...\")\n    spark.sql(\"DROP TABLE IF EXISTS genai_demo.sas.enhanced_final_data\")\n    enhanced_final_data_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.sas.enhanced_final_data\")\n\n    # Additional Outputs: Summary Statistics and Frequency Analysis\n    logger.info(\"Generating summary statistics and frequency analysis...\")\n    summary_stats_df = enhanced_final_data_df.groupBy(\"Region\", \"ProductGroup\").agg(\n        F.mean(\"TotalValue\").alias(\"MeanTotalValue\"),\n        F.sum(\"TotalValue\").alias(\"SumTotalValue\"),\n        F.mean(\"Quantity\").alias(\"MeanQuantity\"),\n        F.sum(\"Quantity\").alias(\"SumQuantity\"),\n        F.mean(\"Sales\").alias(\"MeanSales\"),\n        F.sum(\"Sales\").alias(\"SumSales\")\n    )\n\n    tenure_category_freq_df = enhanced_final_data_df.groupBy(\"TenureCategory\", \"Region\").count()\n\n    # Write additional outputs to Unity Catalog tables\n    logger.info(\"Writing summary statistics to Unity Catalog table...\")\n    spark.sql(\"DROP TABLE IF EXISTS genai_demo.sas.summary_stats\")\n    summary_stats_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.sas.summary_stats\")\n\n    logger.info(\"Writing tenure category frequency analysis to Unity Catalog table...\")\n    spark.sql(\"DROP TABLE IF EXISTS genai_demo.sas.tenure_category_freq\")\n    tenure_category_freq_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.sas.tenure_category_freq\")\n\n    logger.info(\"ETL workflow completed successfully.\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred during the ETL process: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}