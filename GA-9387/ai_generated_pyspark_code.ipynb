{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# MAGIC %md\n# MAGIC # ETL Process for Customer 360 Data\n# MAGIC This notebook performs an ETL process to integrate and analyze customer data from various sources, including policy, claims, demographics, scores, and AI/ML insights.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Import necessary libraries\nimport logging\nfrom pyspark.sql.functions import col, when, datediff, current_date, count, avg, max, lit\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Load Data\n# MAGIC Load data from CSV files into DataFrames.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Load data from CSV files into DataFrames\ntry:\n    logger.info(\"Loading data from CSV files...\")\n    policy_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/policy.csv\", header=True, inferSchema=True)\n    claims_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/claims.csv\", header=True, inferSchema=True)\n    demographics_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/demographics.csv\", header=True, inferSchema=True)\n    scores_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/scores.csv\", header=True, inferSchema=True)\n    aiml_insights_df = spark.read.csv(\"tfs://dataeconomy-9k42/62457/uploads/62457/29aabe9d-c354-4176-8f98-2dd7a5fd7216/aiml_insights.csv\", header=True, inferSchema=True)\nexcept Exception as e:\n    logger.error(f\"Error loading data: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Data Selection\n# MAGIC Select necessary fields from each DataFrame.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Data Selection\ntry:\n    logger.info(\"Selecting necessary fields from each DataFrame...\")\n    selected_demographics_df = demographics_df.select(\n        \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\", \"State\", \"Postal_Code\", \n        \"Date_of_Birth\", \"Gender\", \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n    )\n    selected_policy_df = policy_df.select(\n        \"policy_id\", \"customer_id\", \"policy_type\", \"policy_status\", \"policy_start_date\", \"policy_end_date\", \n        \"policy_term\", \"policy_premium\", \"total_premium_paid\", \"renewal_status\", \"policy_addons\"\n    )\n    selected_claims_df = claims_df.select(\n        \"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\", \"Claim_Amount\", \"Claim_Payout\"\n    )\n    selected_scores_df = scores_df.select(\n        \"Customer_ID\", \"Credit_Score\", \"Fraud_Score\", \"Customer_Risk_Score\"\n    )\n    selected_aiml_insights_df = aiml_insights_df.select(\n        \"Customer_ID\", \"Churn_Probability\", \"Next_Best_Offer\", \"Claims_Fraud_Probability\", \"Revenue_Potential\"\n    )\nexcept Exception as e:\n    logger.error(f\"Error selecting fields: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Data Integration\n# MAGIC Perform data integration through joins.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Data Integration\ntry:\n    logger.info(\"Performing data integration through joins...\")\n    joined_df = selected_demographics_df.join(selected_policy_df, col(\"Customer_ID\") == col(\"customer_id\"), \"inner\") \\\n                                        .join(selected_claims_df, col(\"policy_id\") == col(\"Policy_ID\"), \"inner\")\nexcept Exception as e:\n    logger.error(f\"Error during data integration: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Aggregation and Summarization\n# MAGIC Aggregate and summarize data.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Aggregation and Summarization\ntry:\n    logger.info(\"Aggregating and summarizing data...\")\n    summarized_df = joined_df.groupBy(\"Customer_ID\").agg(\n        count(\"Claim_ID\").alias(\"Total_Claims\"),\n        count(\"policy_id\").alias(\"Policy_Count\"),\n        max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\nexcept Exception as e:\n    logger.error(f\"Error during aggregation: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Custom Calculations\n# MAGIC Perform custom calculations.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Custom Calculations\ntry:\n    logger.info(\"Performing custom calculations...\")\n    age_expr = datediff(current_date(), col(\"Date_of_Birth\")) / 365\n    claim_to_premium_ratio_expr = when(col(\"total_premium_paid\") != 0, col(\"Claim_Amount\") / col(\"total_premium_paid\")).otherwise(0)\n    claims_per_policy_expr = when(col(\"Policy_Count\") != 0, col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0)\n\n    final_df = summarized_df.withColumn(\"Age\", age_expr) \\\n                            .withColumn(\"Claim_To_Premium_Ratio\", claim_to_premium_ratio_expr) \\\n                            .withColumn(\"Claims_Per_Policy\", claims_per_policy_expr) \\\n                            .withColumn(\"Retention_Rate\", lit(0.85)) \\\n                            .withColumn(\"Cross_Sell_Opportunities\", lit(\"Multi-Policy Discount, Home Coverage Add-on\")) \\\n                            .withColumn(\"Upsell_Potential\", lit(\"Premium Vehicle Coverage\"))\nexcept Exception as e:\n    logger.error(f\"Error during custom calculations: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Advanced Insights Integration\n# MAGIC Integrate AI/ML insights and scores data.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Advanced Insights Integration\ntry:\n    logger.info(\"Integrating AI/ML insights and scores data...\")\n    enriched_df = final_df.join(selected_aiml_insights_df, \"Customer_ID\", \"inner\") \\\n                          .join(selected_scores_df, \"Customer_ID\", \"inner\")\nexcept Exception as e:\n    logger.error(f\"Error during insights integration: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Output Generation\n# MAGIC Write the final DataFrame to a Unity Catalog table.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Output Generation\ntry:\n    logger.info(\"Writing the final DataFrame to a Unity Catalog table...\")\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.customer_360\")\n    enriched_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.customer_360\")\nexcept Exception as e:\n    logger.error(f\"Error writing output: {e}\")\n    raise\n\nlogger.info(\"ETL process completed successfully.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}