{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import DataFrame\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Define helper functions for transformations\ndef multi_field_formula_transformation(df: DataFrame) -> DataFrame:\n    \"\"\"Apply multi-field formula transformations.\"\"\"\n    logger.info(\"Applying multi-field formula transformations\")\n    df = df.withColumn(\"Business_unit_code\", F.trim(F.regexp_replace(F.col(\"Business_unit_code\"), \"#|UNMAPPED|NULL\", \"\")))\n    df = df.withColumn(\"FMRC_code\", F.trim(F.regexp_replace(F.col(\"FMRC_code\"), \"#|UNMAPPED|NULL\", \"\")))\n    return df\n\ndef select_transformation(df: DataFrame, fields: list) -> DataFrame:\n    \"\"\"Select specific fields for further processing.\"\"\"\n    logger.info(f\"Selecting fields: {fields}\")\n    return df.select(*fields)\n\ndef join_transformation(df1: DataFrame, df2: DataFrame, join_keys: list) -> DataFrame:\n    \"\"\"Perform join operation between two DataFrames.\"\"\"\n    logger.info(f\"Joining DataFrames on keys: {join_keys}\")\n    df = df1.join(df2, join_keys, \"inner\")\n    for key in join_keys:\n        df = df.drop(df2[key])\n    return df\n\ndef union_transformation(df1: DataFrame, df2: DataFrame) -> DataFrame:\n    \"\"\"Union two DataFrames.\"\"\"\n    logger.info(\"Unioning DataFrames\")\n    return df1.union(df2)\n\ndef formula_transformation(df: DataFrame) -> DataFrame:\n    \"\"\"Apply custom calculations to fields.\"\"\"\n    logger.info(\"Applying formula transformations\")\n    df = df.withColumn(\"Custom_Calculation\", F.expr(\"field1 + field2 * field3\"))\n    return df\n\ndef filter_transformation(df: DataFrame, condition: str) -> DataFrame:\n    \"\"\"Filter DataFrame based on specified condition.\"\"\"\n    logger.info(f\"Filtering DataFrame with condition: {condition}\")\n    return df.filter(condition)\n\ndef summarize_transformation(df: DataFrame, group_by_fields: list, agg_exprs: dict) -> DataFrame:\n    \"\"\"Aggregate data by grouping and performing calculations.\"\"\"\n    logger.info(f\"Summarizing DataFrame by fields: {group_by_fields}\")\n    return df.groupBy(*group_by_fields).agg(agg_exprs)\n\n# Load data from Unity Catalog tables with error handling\ndef load_table(table_name: str) -> DataFrame:\n    \"\"\"Load table from Unity Catalog with error handling.\"\"\"\n    try:\n        df = spark.table(table_name)\n        logger.info(f\"Successfully loaded table: {table_name}\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading table {table_name}: {e}\")\n        raise\n\n# Corrected table loading with enhanced error handling\ndef load_table_with_fallback(table_name: str) -> DataFrame:\n    \"\"\"Load table from Unity Catalog with fallback for missing tables.\"\"\"\n    try:\n        df = spark.table(table_name)\n        logger.info(f\"Successfully loaded table: {table_name}\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading table {table_name}: {e}\")\n        # Fallback logic or alternative handling can be added here\n        raise\n\n# Load data from Unity Catalog tables\npes_prep_df = load_table(\"catalog.source_db.PES_prep\")\nc19_ivl_data_df = load_table_with_fallback(\"catalog.source_db.C19_IVL_data\")  # Ensure correct case sensitivity\nc04_ekpo_df = load_table(\"catalog.source_db.C04_EKPO\")\nc04_bseg_df = load_table(\"catalog.source_db.C04_BSEG\")\npjotr_df = load_table(\"catalog.source_db.PJOTR_\")\npjotr_in_pes_df = load_table(\"catalog.source_db.PJOTR_in_PES\")\n\n# Apply transformations\ntry:\n    pes_prep_df = multi_field_formula_transformation(pes_prep_df)\n    selected_fields = [\"field1\", \"field2\", \"field3\"]  # Example fields\n    pes_prep_df = select_transformation(pes_prep_df, selected_fields)\n    join_keys = [\"LE_code\"]\n    joined_df = join_transformation(pes_prep_df, c19_ivl_data_df, join_keys)\n    unioned_df = union_transformation(joined_df, c04_ekpo_df)\n    transformed_df = formula_transformation(unioned_df)\n    filter_condition = \"Invoice_source_system = 'AP' OR left(Invoice_source_system, 3) = 'ARB'\"\n    filtered_df = filter_transformation(transformed_df, filter_condition)\n    group_by_fields = [\"PJOTR\", \"Spend\", \"Records\"]\n    agg_exprs = {\"Spend\": \"sum\", \"Records\": \"count\"}\n    summarized_df = summarize_transformation(filtered_df, group_by_fields, agg_exprs)\n    logger.info(\"Transformations applied successfully\")\nexcept Exception as e:\n    logger.error(f\"Error during transformations: {e}\")\n    raise\n\n# Output to Unity Catalog tables\ntry:\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table_c03_pjotr = \"C03_pjotr\"\n    target_table_pjotr_in_pes = \"PJOTR_in_PES\"\n\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    summarized_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table_c03_pjotr}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table_c03_pjotr} successfully\")\n\n    pjotr_in_pes_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table_pjotr_in_pes}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table_pjotr_in_pes} successfully\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog tables: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}