{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    pes_prep_df = spark.table(\"genai_demo.jnj.pes_prep\")\n    logger.info(f\"PES_prep loaded with {pes_prep_df.count()} records\")\n    logger.info(f\"PES_prep schema: {pes_prep_df.schema}\")\n\n    c19_ivl_data_df = spark.table(\"genai_demo.jnj.c19_ivl_data\")\n    logger.info(f\"C19_ivl_data loaded with {c19_ivl_data_df.count()} records\")\n    logger.info(f\"C19_ivl_data schema: {c19_ivl_data_df.schema}\")\n\n    c04_ekpo_df = spark.table(\"genai_demo.jnj.c04_ekpo\")\n    logger.info(f\"C04_EKPO loaded with {c04_ekpo_df.count()} records\")\n    logger.info(f\"C04_EKPO schema: {c04_ekpo_df.schema}\")\n\n    c04_bseg_df = spark.table(\"genai_demo.jnj.c04_bseg\")\n    logger.info(f\"C04_BSEG loaded with {c04_bseg_df.count()} records\")\n    logger.info(f\"C04_BSEG schema: {c04_bseg_df.schema}\")\n\n    pjotr_df = spark.table(\"genai_demo.jnj.pjotr_\")\n    logger.info(f\"PJOTR_ loaded with {pjotr_df.count()} records\")\n    logger.info(f\"PJOTR_ schema: {pjotr_df.schema}\")\n\n    pjotr_in_pes_df = spark.table(\"genai_demo.jnj.pjotr_in_pes\")\n    logger.info(f\"PJOTR_in_PES loaded with {pjotr_in_pes_df.count()} records\")\n    logger.info(f\"PJOTR_in_PES schema: {pjotr_in_pes_df.schema}\")\n\n    # Multi-Field Formula Transformation\n    transformed_df = pes_prep_df.withColumn(\"_Business_unit_code\", F.trim(F.col(\"Business_unit_code\"))) \\\n                                .withColumn(\"_LE_code\", F.trim(F.col(\"LE_code\")))\n    logger.info(f\"Multi-Field Formula Transformation applied\")\n\n    # Select Transformation\n    selected_df = transformed_df.select(\"_Business_unit_code\", \"_LE_code\", \"Invoice_amount_USD\")\n    logger.info(f\"Select Transformation applied\")\n\n    # Verify column existence before join\n    if \"_LE_code\" in selected_df.columns and \"IVL_business_unit_code\" in c19_ivl_data_df.columns:\n        # Join Transformation\n        joined_df = selected_df.join(c19_ivl_data_df, selected_df[\"_LE_code\"] == c19_ivl_data_df[\"IVL_business_unit_code\"], \"inner\")\n        logger.info(f\"Join Transformation applied with {joined_df.count()} records\")\n    else:\n        logger.error(\"Join columns do not exist in the DataFrames\")\n        joined_df = spark.createDataFrame([], selected_df.schema)  # Create empty DataFrame with the same schema\n\n    # Ensure both DataFrames have the same schema before union\n    c04_ekpo_df = c04_ekpo_df.selectExpr(\"PO_and_PO_line as _Business_unit_code\", \"PO_line_SLoc as _LE_code\", \"cast(null as double) as Invoice_amount_USD\")\n\n    # Adjust the schema of joined_df to match c04_ekpo_df\n    joined_df = joined_df.selectExpr(\"_Business_unit_code\", \"_LE_code\", \"Invoice_amount_USD\")\n\n    # Union Transformation\n    union_df = joined_df.union(c04_ekpo_df)\n    logger.info(f\"Union Transformation applied with {union_df.count()} records\")\n\n    # Formula Transformation\n    formula_df = union_df.withColumn(\"Calculated_Field\", F.expr(\"Invoice_amount_USD * 1.1\"))\n    logger.info(f\"Formula Transformation applied\")\n\n    # Filter Transformation\n    filtered_df = formula_df.filter(F.col(\"Calculated_Field\") > 1000)\n    logger.info(f\"Filter Transformation applied with {filtered_df.count()} records\")\n\n    # Summarize Transformation\n    summarized_df = filtered_df.groupBy(\"PJOTR\").agg(F.sum(\"Calculated_Field\").alias(\"Total_Spend\"))\n    logger.info(f\"Summarize Transformation applied with {summarized_df.count()} records\")\n\n    # Custom Calculations\n    custom_calculations_df = summarized_df.withColumn(\"Custom_Calc\", F.expr(\"Total_Spend / 100\"))\n    logger.info(f\"Custom Calculations applied\")\n\n    # Output C03_pjotr.yxdb\n    target_catalog = \"genai_demo\"\n    target_schema = \"jnj\"\n    target_table_c03_pjotr = \"c03_pjotr\"\n\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    custom_calculations_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table_c03_pjotr}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table_c03_pjotr}\")\n\n    # Output PJOTR_in_PES.yxdb\n    target_table_pjotr_in_pes = \"pjotr_in_pes\"\n\n    pjotr_in_pes_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table_pjotr_in_pes}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table_pjotr_in_pes}\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {str(e)}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}