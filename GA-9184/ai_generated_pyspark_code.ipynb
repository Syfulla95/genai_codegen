{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# MAGIC %md\n# MAGIC # ETL Process with PySpark\n# MAGIC This notebook performs an ETL process using PySpark, loading data from a flat file and SQL Server, transforming it, and saving it to a Unity Catalog table.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Import necessary libraries\nimport logging\nfrom pyspark.sql import functions as F\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Securely retrieve credentials using Databricks utilities\njdbc_username = dbutils.secrets.get(scope=\"jdbc_scope\", key=\"username\")\njdbc_password = dbutils.secrets.get(scope=\"jdbc_scope\", key=\"password\")\n\n# JDBC connection properties\njdbc_url = \"jdbc:sqlserver://vsco-sqlserver.ctiklfxgg9js.us-east-2.rds.amazonaws.com;databaseName=vsco\"\nconnection_properties = {\n    \"user\": jdbc_username,\n    \"password\": jdbc_password,\n    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 1: Data Source Configuration\ntry:\n    # Load flat file data into a DataFrame\n    flat_file_path = \"dbfs:/mnt/sample_data/SampleCurrencyData.txt\"\n    flat_file_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(flat_file_path)\n    logger.info(\"Flat file data loaded successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error loading flat file data: {e}\")\n    raise\n\ntry:\n    # Load DimCurrency and DimDate tables from SQL Server into DataFrames\n    dim_currency_df = spark.read.jdbc(url=jdbc_url, table=\"AdventureWorksDW2012.DimCurrency\", properties=connection_properties)\n    dim_date_df = spark.read.jdbc(url=jdbc_url, table=\"AdventureWorksDW2012.DimDate\", properties=connection_properties)\n    logger.info(\"SQL Server data loaded successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error loading SQL Server data: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 2: Data Transformation\ntry:\n    # Define join conditions for clarity and maintainability\n    currency_join_condition = F.col(\"CurrencyID\") == F.col(\"CurrencyAlternateKey\")\n    date_join_condition = F.col(\"CurrencyDate\") == F.col(\"FullDateAlternateKey\")\n\n    # Perform an inner join between the flat file DataFrame and the DimCurrency DataFrame on CurrencyID\n    enriched_currency_df = flat_file_df.join(\n        dim_currency_df,\n        currency_join_condition,\n        \"inner\"\n    ).select(\"*\", F.col(\"CurrencyKey\"))\n    logger.info(\"Currency key lookup completed successfully.\")\n\n    # Perform an inner join between the enriched currency DataFrame and the DimDate DataFrame on CurrencyDate\n    final_df = enriched_currency_df.join(\n        dim_date_df,\n        date_join_condition,\n        \"inner\"\n    ).select(\"*\", F.col(\"DateKey\"))\n    logger.info(\"Date key lookup completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during data transformation: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 3: Data Loading\ntry:\n    # Drop the existing table if it exists\n    spark.sql(\"DROP TABLE IF EXISTS catalog_name.schema_name.Sample_OLE_DB_Destination\")\n\n    # Write the transformed data to a Unity Catalog table using Delta format\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog_name.schema_name.Sample_OLE_DB_Destination\")\n    logger.info(\"Data successfully written to Unity Catalog table.\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog: {e}\")\n    raise\n\n# COMMAND ----------\n# MAGIC\n",
                "# Performance Optimizations\n# Cache intermediate DataFrames if beneficial\n# enriched_currency_df.cache()\n# final_df.cache()\n\n# Use broadcast joins for small dimension tables\n# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n\nlogger.info(\"ETL process completed successfully.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}