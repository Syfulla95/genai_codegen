{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n\n# MAGIC %md\n# MAGIC # Data Processing and Analysis with PySpark\n# MAGIC This notebook performs data processing and analysis using PySpark, including data loading, integration, aggregation, and custom calculations.\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Import necessary libraries\nimport logging\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, max, avg, datediff, current_date, lit, when\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is pre-initialized as 'spark'\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Load data from Unity Catalog tables\ndef load_data():\n    try:\n        demographics_df = spark.table(\"genai_demo.jnj.demographics\")\n        claims_df = spark.table(\"genai_demo.jnj.claims\")\n        policy_df = spark.table(\"genai_demo.jnj.policy\")\n        scores_df = spark.table(\"genai_demo.jnj.scores\")\n        aiml_df = spark.table(\"genai_demo.jnj.aiml_insights\")\n        logger.info(\"Data loaded successfully from Unity Catalog tables.\")\n        return demographics_df, claims_df, policy_df, scores_df, aiml_df\n    except Exception as e:\n        logger.error(f\"Error loading data from Unity Catalog tables: {e}\")\n        raise\n\ndemographics_df, claims_df, policy_df, scores_df, aiml_df = load_data()\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Step 1: Data Selection and Filtering\ndef select_and_filter_data(demographics_df, claims_df, policy_df):\n    try:\n        demographics_df = demographics_df.select(\n            col(\"Customer_ID\"), col(\"Customer_Name\"), col(\"Email\"), col(\"Phone_Number\"), col(\"Address\"),\n            col(\"City\"), col(\"State\"), col(\"Postal_Code\"), col(\"Date_of_Birth\"), col(\"Gender\"),\n            col(\"Marital_Status\"), col(\"Occupation\"), col(\"Income_Level\"), col(\"Customer_Segment\")\n        )\n\n        claims_df = claims_df.select(\n            col(\"Claim_ID\"), col(\"Policy_ID\"), col(\"Claim_Date\"), col(\"Claim_Type\"),\n            col(\"Claim_Status\"), col(\"Claim_Amount\"), col(\"Claim_Payout\")\n        )\n\n        policy_df = policy_df.select(\n            col(\"Policy_ID\"), col(\"Customer_ID\"), col(\"Policy_Type\"), col(\"Policy_Status\"),\n            col(\"Policy_Start_Date\"), col(\"Policy_End_Date\"), col(\"Policy_Term\"), col(\"Policy_Premium\"),\n            col(\"Total_Premium_Paid\"), col(\"Renewal_Status\"), col(\"Policy_Addons\")\n        )\n        logger.info(\"Data selection and filtering completed.\")\n        return demographics_df, claims_df, policy_df\n    except Exception as e:\n        logger.error(f\"Error during data selection and filtering: {e}\")\n        raise\n\ndemographics_df, claims_df, policy_df = select_and_filter_data(demographics_df, claims_df, policy_df)\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Step 2: Data Integration\ndef integrate_data(demographics_df, policy_df, claims_df):\n    try:\n        joined_df = demographics_df.join(policy_df, demographics_df[\"Customer_ID\"] == policy_df[\"Customer_ID\"], \"inner\")\n        joined_df = joined_df.join(claims_df, joined_df[\"Policy_ID\"] == claims_df[\"Policy_ID\"], \"inner\")\n        logger.info(\"Data integration completed.\")\n        return joined_df\n    except Exception as e:\n        logger.error(f\"Error during data integration: {e}\")\n        raise\n\njoined_df = integrate_data(demographics_df, policy_df, claims_df)\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Step 3: Data Aggregation\ndef aggregate_data(joined_df):\n    try:\n        summarized_df = joined_df.groupBy(\"Customer_ID\").agg(\n            count(\"Claim_ID\").alias(\"Total_Claims\"),\n            count(\"Policy_ID\").alias(\"Policy_Count\"),\n            max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n            avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n        )\n        logger.info(\"Data aggregation completed.\")\n        return summarized_df\n    except Exception as e:\n        logger.error(f\"Error during data aggregation: {e}\")\n        raise\n\nsummarized_df = aggregate_data(joined_df)\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Step 4: Custom Calculations\ndef perform_custom_calculations(summarized_df, joined_df):\n    try:\n        age_calculation = datediff(current_date(), col(\"Date_of_Birth\")) / 365\n        claim_to_premium_ratio = when(col(\"Total_Premium_Paid\") != 0, col(\"Claim_Amount\") / col(\"Total_Premium_Paid\")).otherwise(0)\n        claims_per_policy = when(col(\"Policy_Count\") != 0, col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0)\n\n        final_df = summarized_df.join(joined_df, \"Customer_ID\", \"inner\").withColumn(\n            \"Age\", age_calculation\n        ).withColumn(\n            \"Claim_To_Premium_Ratio\", claim_to_premium_ratio\n        ).withColumn(\n            \"Claims_Per_Policy\", claims_per_policy\n        ).withColumn(\n            \"Retention_Rate\", lit(0.85)\n        ).withColumn(\n            \"Cross_Sell_Opportunities\", lit(\"Multi-Policy Discount, Home Coverage Add-on\")\n        ).withColumn(\n            \"Upsell_Potential\", lit(\"Premium Vehicle Coverage\")\n        )\n        logger.info(\"Custom calculations completed.\")\n        return final_df\n    except Exception as e:\n        logger.error(f\"Error during custom calculations: {e}\")\n        raise\n\nfinal_df = perform_custom_calculations(summarized_df, joined_df)\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Step 5: Comprehensive Data Joining\ndef comprehensive_data_joining(final_df, aiml_df, scores_df):\n    try:\n        final_df = final_df.join(aiml_df, final_df[\"Customer_ID\"] == aiml_df[\"Customer_ID\"], \"inner\") \\\n                           .join(scores_df, final_df[\"Customer_ID\"] == scores_df[\"Customer_ID\"], \"inner\")\n        logger.info(\"Comprehensive data joining completed.\")\n        return final_df\n    except Exception as e:\n        logger.error(f\"Error during comprehensive data joining: {e}\")\n        raise\n\nfinal_df = comprehensive_data_joining(final_df, aiml_df, scores_df)\n\n# COMMAND ----------\n\n# MAGIC\n",
                "# Output Handling\ndef write_output(final_df):\n    try:\n        spark.sql(\"DROP TABLE IF EXISTS genai_demo.jnj.customer_360_view\")\n        final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.jnj.customer_360_view\")\n        logger.info(\"Data written successfully to Unity Catalog target table.\")\n    except Exception as e:\n        logger.error(f\"Error writing data to Unity Catalog target table: {e}\")\n        raise\n\nwrite_output(final_df)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}