{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom datetime import datetime\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    customers_df = spark.table(\"catalog.source_db.customers\")\n    transactions_df = spark.table(\"catalog.source_db.transactions\")\n\n    # Step 1: Valid Transactions Filtering\n    valid_txns_df = transactions_df.filter((F.col(\"Sales\") > 0) & (F.col(\"Product\") != \"\"))\n    logger.info(\"Filtered valid transactions.\")\n\n    # Step 2: Effective Price Calculation\n    trans_step2_df = valid_txns_df.withColumn(\"EffectivePrice\", F.col(\"Sales\") * (1 - F.col(\"Discount\") / 100))\n    logger.info(\"Calculated effective price.\")\n\n    # Step 3: Total Value Calculation\n    trans_step3_df = trans_step2_df.withColumn(\"TotalValue\", F.col(\"EffectivePrice\") * F.col(\"Quantity\"))\n    logger.info(\"Calculated total value.\")\n\n    # Step 4: Full Data Join\n    full_data_df = trans_step3_df.join(customers_df, \"CustomerID\", \"left\").select(trans_step3_df[\"*\"], customers_df[\"Region\"], customers_df[\"JoinDate\"])\n    logger.info(\"Performed full data join.\")\n\n    # Step 5: Tenure Days Calculation\n    trans_step5_df = full_data_df.withColumn(\"TenureDays\", F.datediff(F.col(\"TransDate\"), F.col(\"JoinDate\")))\n    logger.info(\"Calculated tenure days.\")\n\n    # Step 6: Tenure Category Assignment\n    trans_step6_df = trans_step5_df.withColumn(\"TenureCategory\", F.when(F.col(\"TenureDays\") < 180, \"New\")\n                                               .when(F.col(\"TenureDays\") < 365, \"Medium\")\n                                               .otherwise(\"Loyal\"))\n    logger.info(\"Assigned tenure category.\")\n\n    # Step 7: High Value Flag\n    trans_step7_df = trans_step6_df.withColumn(\"HighValueFlag\", F.col(\"TotalValue\") > 2000)\n    logger.info(\"Flagged high value transactions.\")\n\n    # Step 8: Product Group Assignment\n    trans_step8_df = trans_step7_df.withColumn(\"ProductGroup\", F.when(F.col(\"Product\").isin(\"A\", \"C\"), \"Core\").otherwise(\"Non-Core\"))\n    logger.info(\"Assigned product group.\")\n\n    # Step 9: Z-score Standardization\n    windowSpec = Window.partitionBy(\"ProductGroup\")\n    standardized_df = trans_step8_df.withColumn(\"ZScoreTotalValue\", (F.col(\"TotalValue\") - F.avg(\"TotalValue\").over(windowSpec)) / F.stddev(\"TotalValue\").over(windowSpec))\n    logger.info(\"Standardized total value using Z-score.\")\n\n    # Step 10: Outlier Detection\n    enhanced_final_data_df = standardized_df.withColumn(\"OutlierFlag\", F.abs(F.col(\"ZScoreTotalValue\")) > 2)\n    logger.info(\"Detected outliers.\")\n\n    # Output Handling: Write to Unity Catalog tables\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.enhanced_final_data\")\n    enhanced_final_data_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.enhanced_final_data\")\n    logger.info(\"Saved enhanced final data to Unity Catalog.\")\n\n    # Additional Outputs: Summary Statistics, Frequency Analysis, Correlation Analysis\n    # Summary Statistics by Region and Product Group\n    summary_stats_df = enhanced_final_data_df.groupBy(\"Region\", \"ProductGroup\").agg(\n        F.mean(\"TotalValue\").alias(\"MeanTotalValue\"),\n        F.sum(\"TotalValue\").alias(\"SumTotalValue\"),\n        F.mean(\"Quantity\").alias(\"MeanQuantity\"),\n        F.sum(\"Quantity\").alias(\"SumQuantity\"),\n        F.mean(\"Sales\").alias(\"MeanSales\"),\n        F.sum(\"Sales\").alias(\"SumSales\")\n    )\n    logger.info(\"Calculated summary statistics by region and product group.\")\n\n    # Tenure Category Frequency by Region\n    tenure_freq_df = enhanced_final_data_df.groupBy(\"TenureCategory\", \"Region\").count()\n    logger.info(\"Calculated tenure category frequency by region.\")\n\n    # Correlation Analysis\n    correlation_df = enhanced_final_data_df.select(\"Sales\", \"Discount\", \"Quantity\", \"TotalValue\").corr()\n    logger.info(\"Performed correlation analysis.\")\n\n    # Summary by Outlier Flag\n    outlier_summary_df = enhanced_final_data_df.groupBy(\"OutlierFlag\").agg(\n        F.mean(\"Sales\").alias(\"MeanSales\"),\n        F.stddev(\"Sales\").alias(\"StdDevSales\"),\n        F.mean(\"TotalValue\").alias(\"MeanTotalValue\"),\n        F.stddev(\"TotalValue\").alias(\"StdDevTotalValue\"),\n        F.mean(\"Quantity\").alias(\"MeanQuantity\"),\n        F.stddev(\"Quantity\").alias(\"StdDevQuantity\")\n    )\n    logger.info(\"Calculated summary by outlier flag.\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}