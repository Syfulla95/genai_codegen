{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nfrom pyspark.sql import functions as F\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to connect to PostgreSQL and fetch data\ndef fetch_data_from_postgres(query):\n    try:\n        # Retrieve credentials securely\n        pg_host = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_host\")\n        pg_port = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_port\")\n        pg_dbname = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_dbname\")\n        pg_user = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_user\")\n        pg_password = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_password\")\n\n        # Connect to PostgreSQL\n        conn = psycopg2.connect(\n            host=pg_host,\n            port=pg_port,\n            dbname=pg_dbname,\n            user=pg_user,\n            password=pg_password\n        )\n        cursor = conn.cursor()\n        cursor.execute(query)\n        data = cursor.fetchall()\n        cursor.close()\n        conn.close()\n        return data\n    except Exception as e:\n        logger.error(f\"Error fetching data from PostgreSQL: {e}\")\n        raise\n\n# Load data from Unity Catalog source tables\ntry:\n    # Corrected table name with proper catalog and schema\n    source_df = spark.table(\"catalog_name.source_db.source_table\")\n    logger.info(\"Source data loaded successfully from Unity Catalog\")\nexcept Exception as e:\n    logger.error(f\"Error loading source data: {e}\")\n    # Attempt to provide more information about the error\n    current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n    logger.error(f\"Current schema is: {current_schema}. Please verify the table name and schema.\")\n    raise\n\n# Perform transformations\ntry:\n    # Example transformation: filter, join, and aggregate\n    filtered_df = source_df.filter(F.col(\"column_name\") > 100)\n    logger.info(\"Data filtered successfully\")\n\n    # Assume we have a small dimension table for a broadcast join\n    dim_df = spark.table(\"catalog_name.source_db.dimension_table\")\n    transformed_df = filtered_df.join(F.broadcast(dim_df), \"key_column\", \"inner\")\n    logger.info(\"Data joined successfully\")\n\n    # Example aggregation\n    aggregated_df = transformed_df.groupBy(\"group_column\").agg(F.sum(\"value_column\").alias(\"total_value\"))\n    logger.info(\"Data aggregated successfully\")\n\n    # Cache the intermediate DataFrame if beneficial\n    aggregated_df.cache()\nexcept Exception as e:\n    logger.error(f\"Error during data transformation: {e}\")\n    raise\n\n# Write transformed data to Unity Catalog target table\ntry:\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"table_name\"\n\n    # Ensure schema exists before creating table\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    aggregated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written successfully to {target_catalog}.{target_schema}.{target_table}\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to target table: {e}\")\n    raise\n\n# Corrected Code Explanation:\n# 1. The error message indicated that the table or view could not be found. This was likely due to incorrect catalog or schema names.\n# 2. The code now includes a try-except block to handle potential errors when loading the source data from Unity Catalog.\n# 3. The schema creation step ensures that the target schema exists before attempting to write the data.\n# 4. Enhanced logging provides more context about the current schema and potential issues with table names.\n# 5. The code assumes that the table names and schema are correct, but if the error persists, further investigation into the catalog and schema names may be necessary.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}