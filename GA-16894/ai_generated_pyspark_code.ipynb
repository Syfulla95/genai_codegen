{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nfrom pyspark.sql import functions as F\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to connect to PostgreSQL and fetch data\ndef fetch_data_from_postgresql(query, host, port, database, user, password):\n    try:\n        connection = psycopg2.connect(\n            host=host,\n            port=int(port),  # Ensure port is an integer\n            database=database,\n            user=user,\n            password=password\n        )\n        cursor = connection.cursor()\n        cursor.execute(query)\n        data = cursor.fetchall()\n        cursor.close()\n        connection.close()\n        return data\n    except Exception as e:\n        logger.error(f\"Error fetching data from PostgreSQL: {e}\")\n        raise\n\n# Retrieve PostgreSQL credentials securely\ntry:\n    pg_host = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_host\")\n    pg_port = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_port\")\n    pg_database = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_database\")\n    pg_user = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_user\")\n    pg_password = dbutils.secrets.get(scope=\"my_scope\", key=\"pg_password\")\nexcept Exception as e:\n    logger.error(f\"Error retrieving secrets: {e}\")\n    # Handle missing secrets by providing default values or alternative methods\n    pg_host = \"valid_default_host\"  # Ensure this is a valid host\n    pg_port = 5432  # Use a valid default integer port\n    pg_database = \"default_database\"\n    pg_user = \"default_user\"\n    pg_password = \"default_password\"\n    logger.warning(\"Using default PostgreSQL credentials due to missing secrets\")\n\n# Validate host name resolution\nimport socket\ntry:\n    socket.gethostbyname(pg_host)\n    logger.info(f\"Host {pg_host} resolved successfully\")\nexcept socket.error as e:\n    logger.error(f\"Host name resolution failed for {pg_host}: {e}\")\n    raise\n\n# Fetch data from PostgreSQL\nquery = \"SELECT * FROM legacy_table\"\ntry:\n    postgresql_data = fetch_data_from_postgresql(query, pg_host, pg_port, pg_database, pg_user, pg_password)\nexcept Exception as e:\n    logger.error(f\"Failed to fetch data from PostgreSQL: {e}\")\n    raise\n\n# Convert PostgreSQL data to Spark DataFrame\npostgresql_df = spark.createDataFrame(postgresql_data, schema=[\"column1\", \"column2\", \"column3\"])\n\n# Load data from Unity Catalog source table\ntry:\n    source_df = spark.table(\"catalog.source_db.source_table\")\n    logger.info(\"Source data loaded successfully from Unity Catalog\")\nexcept Exception as e:\n    logger.error(f\"Error loading source data from Unity Catalog: {e}\")\n    raise\n\n# Perform transformations\ntry:\n    # Example transformation: Join with PostgreSQL data\n    transformed_df = source_df.join(postgresql_df, source_df[\"key\"] == postgresql_df[\"key\"], \"left_outer\")\n\n    # Additional transformations (e.g., filtering, aggregations)\n    transformed_df = transformed_df.filter(F.col(\"column1\") > 100)\n    transformed_df = transformed_df.groupBy(\"column2\").agg(F.sum(\"column3\").alias(\"total_column3\"))\n\n    # Cache intermediate DataFrame if beneficial\n    transformed_df.cache()\n    logger.info(\"Data transformations completed successfully\")\nexcept Exception as e:\n    logger.error(f\"Error during data transformations: {e}\")\n    raise\n\n# Write transformed data to Unity Catalog target table\ntry:\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"table_name\"\n\n    # Ensure schema exists before creating table\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    transformed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written successfully to {target_catalog}.{target_schema}.{target_table}\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog target table: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}