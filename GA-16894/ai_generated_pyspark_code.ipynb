{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.utils import AnalysisException\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to retrieve secrets\ndef get_secret(scope, key):\n    try:\n        secret_value = dbutils.secrets.get(scope, key)\n        logger.info(f\"Successfully retrieved secret {key} from scope {scope}\")\n        return secret_value\n    except Exception as e:\n        logger.error(f\"Failed to retrieve secret {key} from scope {scope}: {str(e)}\")\n        return None\n\n# Step 1: Load data from Unity Catalog tables\ntry:\n    source_df = spark.table(\"catalog.source_db.source_table\")\n    logger.info(\"Loaded source data from Unity Catalog\")\nexcept AnalysisException as e:\n    logger.error(f\"Failed to load source data: {str(e)}\")\n    raise\n\n# Step 2: Connect to external PostgreSQL database and fetch data\nconn = None\ncursor = None\ntry:\n    # Check if secrets exist before attempting to retrieve them\n    secret_keys = [\"pg_host\", \"pg_port\", \"pg_dbname\", \"pg_user\", \"pg_password\"]\n    secrets = {key: get_secret(\"my_scope\", key) for key in secret_keys}\n\n    # Ensure all secrets are retrieved successfully\n    if all(secrets.values()):\n        conn = psycopg2.connect(\n            host=secrets[\"pg_host\"],\n            port=secrets[\"pg_port\"],\n            dbname=secrets[\"pg_dbname\"],\n            user=secrets[\"pg_user\"],\n            password=secrets[\"pg_password\"]\n        )\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT * FROM legacy_table\")\n        pg_data = cursor.fetchall()\n        logger.info(\"Fetched data from PostgreSQL\")\n    else:\n        raise ValueError(\"One or more secrets could not be retrieved.\")\nexcept Exception as e:\n    logger.error(f\"Failed to connect to PostgreSQL: {str(e)}\")\n    raise\nfinally:\n    if cursor is not None:\n        cursor.close()\n    if conn is not None:\n        conn.close()\n\n# Step 3: Transform data\ntry:\n    # Example transformation: Join with PostgreSQL data\n    pg_df = spark.createDataFrame(pg_data, schema=[\"column1\", \"column2\", \"column3\"])\n    transformed_df = source_df.join(pg_df, source_df.id == pg_df.column1, \"inner\")\n\n    # Additional transformations\n    transformed_df = transformed_df.withColumn(\"new_column\", F.expr(\"column2 * 2\"))\n    logger.info(\"Data transformation completed\")\nexcept Exception as e:\n    logger.error(f\"Data transformation failed: {str(e)}\")\n    raise\n\n# Step 4: Write transformed data to Unity Catalog target table\ntry:\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"table_name\"\n\n    # Ensure schema exists\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table\n    transformed_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table}\")\nexcept AnalysisException as e:\n    logger.error(f\"Failed to write data to target table: {str(e)}\")\n    raise\n\n# Step 5: Performance optimizations\ntry:\n    # Cache intermediate DataFrame if beneficial\n    transformed_df.cache()\n    logger.info(\"Cached transformed DataFrame\")\n\n    # Use broadcast join for small dimension tables\n    small_df = spark.table(\"catalog.db.small_table\")\n    transformed_df = transformed_df.join(F.broadcast(small_df), \"key\")\n    logger.info(\"Applied broadcast join\")\nexcept Exception as e:\n    logger.error(f\"Performance optimization failed: {str(e)}\")\n    raise\n\nlogger.info(\"ETL workflow completed successfully\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}