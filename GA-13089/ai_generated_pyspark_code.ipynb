{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, StringType, DateType\nfrom datetime import datetime\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    customers_df = spark.table(\"catalog.source_db.customers\")\n    transactions_df = spark.table(\"catalog.source_db.transactions\")\n\n    # Step 1: Valid Transactions Filtering\n    valid_txns_df = transactions_df.filter((F.col(\"Sales\") > 0) & (F.col(\"Product\") != \"\"))\n\n    # Step 2: Effective Price Calculation\n    trans_step2_df = valid_txns_df.withColumn(\"EffectivePrice\", F.col(\"Sales\") * (1 - F.col(\"Discount\") / 100))\n\n    # Step 3: Total Value Calculation\n    trans_step3_df = trans_step2_df.withColumn(\"TotalValue\", F.col(\"EffectivePrice\") * F.col(\"Quantity\"))\n\n    # Step 4: Joining with Customers Data\n    full_data_df = trans_step3_df.join(customers_df, trans_step3_df.CustomerID == customers_df.CustomerID, \"left\") \\\n                                 .select(trans_step3_df[\"*\"], customers_df[\"Region\"], customers_df[\"JoinDate\"])\n\n    # Step 5: Tenure Days Calculation\n    trans_step5_df = full_data_df.withColumn(\"TenureDays\", F.datediff(F.col(\"TransDate\"), F.col(\"JoinDate\")))\n\n    # Step 6: Tenure Category Assignment\n    trans_step6_df = trans_step5_df.withColumn(\"TenureCategory\", \n                                               F.when(F.col(\"TenureDays\") < 180, \"New\")\n                                                .when(F.col(\"TenureDays\") < 365, \"Medium\")\n                                                .otherwise(\"Loyal\"))\n\n    # Step 7: High Value Flag\n    trans_step7_df = trans_step6_df.withColumn(\"HighValueFlag\", F.col(\"TotalValue\") > 2000)\n\n    # Step 8: Product Group Assignment\n    trans_step8_df = trans_step7_df.withColumn(\"ProductGroup\", \n                                               F.when(F.col(\"Product\").isin([\"A\", \"C\"]), \"Core\")\n                                                .otherwise(\"Non-Core\"))\n\n    # Step 9: Final Data Preparation\n    final_data_df = trans_step8_df\n\n    # Step 10: Z-score Standardization\n    sorted_final_data_df = final_data_df.orderBy(\"ProductGroup\")\n    standardized_df = sorted_final_data_df.groupBy(\"ProductGroup\").agg(\n        F.mean(\"TotalValue\").alias(\"mean_TotalValue\"),\n        F.stddev(\"TotalValue\").alias(\"stddev_TotalValue\")\n    ).join(sorted_final_data_df, \"ProductGroup\").withColumn(\n        \"StandardizedTotalValue\", \n        (F.col(\"TotalValue\") - F.col(\"mean_TotalValue\")) / F.col(\"stddev_TotalValue\")\n    )\n\n    # Step 11: Outlier Detection\n    enhanced_final_data_df = standardized_df.withColumn(\"OutlierFlag\", F.abs(F.col(\"StandardizedTotalValue\")) > 2)\n\n    # Write the final data to Unity Catalog tables\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.final_data\")\n    final_data_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.final_data\")\n\n    spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.enhanced_final_data\")\n    enhanced_final_data_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.enhanced_final_data\")\n\n    # Generate reports\n    # Correlation Analysis Report\n    correlation_df = enhanced_final_data_df.select(\"Sales\", \"Discount\", \"Quantity\", \"TotalValue\")\n    correlation_df.stat.corr(\"Sales\", \"Discount\")\n    correlation_df.stat.corr(\"Sales\", \"Quantity\")\n    correlation_df.stat.corr(\"Sales\", \"TotalValue\")\n    correlation_df.stat.corr(\"Discount\", \"Quantity\")\n    correlation_df.stat.corr(\"Discount\", \"TotalValue\")\n    correlation_df.stat.corr(\"Quantity\", \"TotalValue\")\n\n    # Summary Statistics Report\n    summary_stats_df = enhanced_final_data_df.groupBy(\"Region\", \"ProductGroup\").agg(\n        F.mean(\"TotalValue\").alias(\"mean_TotalValue\"),\n        F.sum(\"TotalValue\").alias(\"sum_TotalValue\"),\n        F.mean(\"Quantity\").alias(\"mean_Quantity\"),\n        F.sum(\"Quantity\").alias(\"sum_Quantity\"),\n        F.mean(\"Sales\").alias(\"mean_Sales\"),\n        F.sum(\"Sales\").alias(\"sum_Sales\")\n    )\n\n    # Tenure Category Frequency Report\n    tenure_freq_df = enhanced_final_data_df.groupBy(\"TenureCategory\", \"Region\").count()\n\n    # Outlier Flag Summary Report\n    outlier_summary_df = enhanced_final_data_df.groupBy(\"OutlierFlag\").agg(\n        F.mean(\"Sales\").alias(\"mean_Sales\"),\n        F.stddev(\"Sales\").alias(\"stddev_Sales\"),\n        F.mean(\"TotalValue\").alias(\"mean_TotalValue\"),\n        F.stddev(\"TotalValue\").alias(\"stddev_TotalValue\"),\n        F.mean(\"Quantity\").alias(\"mean_Quantity\"),\n        F.stddev(\"Quantity\").alias(\"stddev_Quantity\")\n    )\n\n    logger.info(\"ETL process completed successfully.\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred during the ETL process: {e}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}