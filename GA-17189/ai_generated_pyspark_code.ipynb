{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StringType\nimport psycopg2\nfrom datetime import datetime\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to connect to PostgreSQL and fetch data\ndef fetch_data_from_postgres(query):\n    try:\n        # Retrieve credentials securely\n        host = dbutils.secrets.get(scope=\"my_scope\", key=\"postgres_host\")\n        dbname = dbutils.secrets.get(scope=\"my_scope\", key=\"postgres_dbname\")\n        user = dbutils.secrets.get(scope=\"my_scope\", key=\"postgres_user\")\n        password = dbutils.secrets.get(scope=\"my_scope\", key=\"postgres_password\")\n        \n        # Connect to PostgreSQL\n        conn = psycopg2.connect(host=host, dbname=dbname, user=user, password=password)\n        cursor = conn.cursor()\n        cursor.execute(query)\n        data = cursor.fetchall()\n        cursor.close()\n        conn.close()\n        return data\n    except Exception as e:\n        logger.error(f\"Error fetching data from PostgreSQL: {e}\")\n        raise\n\n# Load static data from TextInput Channel (Node 69)\ntry:\n    text_input_df = spark.createDataFrame(\n        [(\"D1\", \"Distribution Channel 1\"), (\"D2\", \"Distribution Channel 2\")],\n        [\"DIST_CHNL\", \"DIST_CHNL_DESC\"]\n    )\n    logger.info(f\"TextInput Channel loaded with {text_input_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error loading TextInput Channel: {e}\")\n\n# Load manual date-related fields (Node 24)\ntry:\n    manual_date_df = spark.createDataFrame(\n        [(datetime(2023, 1, 1), datetime(2023, 1, 31), datetime.now(), \"20230101\", \"20230131\")],\n        [\"Start Date\", \"End Date\", \"DateTime_Out\", \"StartTXT\", \"EndTXT\"]\n    )\n    logger.info(f\"Manual Date fields loaded with {manual_date_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error loading Manual Date fields: {e}\")\n\n# Connect to SQL Server and fetch data (Node 48)\ntry:\n    sql_server_data = fetch_data_from_postgres(\"SELECT * FROM tdmedpod WHERE BILL_DTE = '2019-07-01' AND WHS = 'D0CG';\")\n    sql_server_df = spark.createDataFrame(sql_server_data, schema=[\"DIST_CHNL_DESC\", \"RunDte\", \"SO_Date\", \"BILL_DTE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Invoices\", \"Invoice_Lines\", \"LANDED_COST\", \"REV_COST\", \"DIRECT_STD_COST\", \"NET_REV_AMT\", \"Invoice_Sales\", \"EXT_SALES\", \"EXT_FINAL_PRICE\", \"SERVICE_FEE\", \"EXT_SHIP_HNDL\", \"EXT_SALES_TAX\", \"EXT_LOCAL_TAX\", \"BASE_QTY\", \"SELL_QTY\", \"WGT\", \"VOL\", \"Vendor_Trans_Absorb\", \"Vendor_Drop_Ship_Absorb\", \"Ext_Hndl_Drop_Absorb\", \"Vendor_MOC_Absorb\", \"Trans_Absorb_Amt\", \"Trans_Charge_Amt\", \"RESTOCK_Fee\", \"Special_Hndl_Amt\", \"Vendor_Hndl_Amt\", \"MOC_Amt\", \"Fuel_Surcharge\", \"BIA_Ship_Hndl_Amt\", \"Rush_Order_Fee\", \"Vendor_Trans_Charge\", \"Vendor_Drop_Ship_Fee\", \"Markup_Vendor_Trans\", \"Markup_Hndl_Fee\", \"COE_Ship_Hndl_Amt\"])\n    logger.info(f\"SQL Server data loaded with {sql_server_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error fetching data from SQL Server: {e}\")\n\n# Load data from Unity Catalog tables for Dynamic Inputs (Nodes 18, 77, 89, 86, 84)\ntry:\n    week1_df = spark.table(\"catalog.db.dynamic_input_week1\")\n    week2_df = spark.table(\"catalog.db.dynamic_input_week2\")\n    week3_df = spark.table(\"catalog.db.dynamic_input_week3\")\n    week4_df = spark.table(\"catalog.db.dynamic_input_week4\")\n    week5_df = spark.table(\"catalog.db.dynamic_input_week5\")\n    logger.info(\"Dynamic Input data loaded from Unity Catalog tables\")\nexcept Exception as e:\n    logger.error(f\"Error loading Dynamic Input data: {e}\")\n\n# Union operation (Node 78)\ntry:\n    union_df = week1_df.union(week2_df).union(week3_df).union(week4_df).union(week5_df)\n    logger.info(f\"Union operation completed with {union_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error during Union operation: {e}\")\n\n# Rename fields (Node 40)\ntry:\n    renamed_df = union_df.withColumnRenamed(\"SO_AUDAT\", \"SO_Date\") \\\n                         .withColumnRenamed(\"FKDAT\", \"BILL_DATE\") \\\n                         .withColumnRenamed(\"WERKS\", \"Whs\") \\\n                         .withColumnRenamed(\"VTWEG\", \"DIST_CHNL_ID\") \\\n                         .withColumnRenamed(\"ZZFINCLASS\", \"FNC_ID\") \\\n                         .withColumnRenamed(\"BEZEK\", \"FNC_DESC\") \\\n                         .withColumnRenamed(\"SOLDTO_KUNNR\", \"SOLDTO\") \\\n                         .withColumnRenamed(\"SHIPTO_KUNNR\", \"SHIPTO\") \\\n                         .withColumnRenamed(\"VGBEL\", \"RFRNC_DOC_NUM\") \\\n                         .withColumnRenamed(\"lines\", \"Invoice_lines\")\n    logger.info(\"Fields renamed successfully\")\nexcept Exception as e:\n    logger.error(f\"Error renaming fields: {e}\")\n\n# MultiFieldFormula transformation (Node 67)\ntry:\n    transformed_df = renamed_df.select(\n        *[F.when(F.col(c).isNull() | (F.col(c) == \"\"), 0).otherwise(F.col(c)).alias(c) for c in renamed_df.columns]\n    )\n    logger.info(\"MultiFieldFormula transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error applying MultiFieldFormula transformation: {e}\")\n\n# Cleanse transformation (Node 44)\ntry:\n    cleansed_df = transformed_df.select(\n        *[F.upper(F.col(c)).alias(c) for c in transformed_df.columns]\n    )\n    logger.info(\"Cleanse transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error applying Cleanse transformation: {e}\")\n\n# Custom Calculation for Rush_Order_Fee (Node 60)\ntry:\n    rush_fee_df = cleansed_df.withColumn(\"Rush_Order_Fee\", F.col(\"ADDTN_TRANS_FEE_OVRRIDE_ZSRO\"))\n    logger.info(\"Rush_Order_Fee calculated\")\nexcept Exception as e:\n    logger.error(f\"Error calculating Rush_Order_Fee: {e}\")\n\n# Summarize Rush_Order_Fee (Node 42)\ntry:\n    summarize_df = rush_fee_df.groupBy(\"BILL_DATE\").agg(F.sum(\"Rush_Order_Fee\").alias(\"Sum_Rush_Order_Fee\"))\n    logger.info(\"Rush_Order_Fee summarized\")\nexcept Exception as e:\n    logger.error(f\"Error summarizing Rush_Order_Fee: {e}\")\n\n# Dynamic Rename (Node 46)\ntry:\n    dynamic_rename_df = summarize_df.select(\n        *[F.col(c).alias(c.replace(\"Sum_\", \"\")) for c in summarize_df.columns]\n    )\n    logger.info(\"Dynamic Rename transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error applying Dynamic Rename transformation: {e}\")\n\n# Append Fields (Node 56)\ntry:\n    append_fields_df = dynamic_rename_df.withColumn(\"BIA_SHIP_HNDL_AMT\", F.expr(\"[Sum_Trans_Charge_Amt]+[Sum_RESTOCK_Fee]+[Sum_Special_Hndl_Amt]+[Sum_Vendor_Hndl_Amt]+[Sum_MOC_Amt]+[Sum_Fuel_Surcharge]\")) \\\n                                        .withColumn(\"COE_SHIP_HNDL_AMT\", F.expr(\"[Sum_Trans_Charge_Amt]+[Sum_RESTOCK_Fee]+[Sum_Special_Hndl_Amt]+[Sum_Vendor_Hndl_Amt]+[Sum_MOC_Amt]+[Sum_Fuel_Surcharge]+[Rush_Order_Fee]+[VENDR_TRANS_CHRG_FRT_ZTV1]+[MARKUP_VENDOR_TRANS_FEE_AMT_ZMT1]\"))\n    logger.info(\"Fields appended successfully\")\nexcept Exception as e:\n    logger.error(f\"Error appending fields: {e}\")\n\n# Formula: Invoice_Sales (Node 68)\ntry:\n    invoice_sales_df = append_fields_df.withColumn(\"Invoice_Sales\", F.col(\"Sum_EXT_FINAL_PRICE\"))\n    logger.info(\"Invoice_Sales calculated\")\nexcept Exception as e:\n    logger.error(f\"Error calculating Invoice_Sales: {e}\")\n\n# Join operation (Node 70)\ntry:\n    joined_df = invoice_sales_df.join(text_input_df, invoice_sales_df.DIST_CHNL_ID == text_input_df.DIST_CHNL, \"left\")\n    logger.info(\"Join operation completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Join operation: {e}\")\n\n# Union operation (Node 71)\ntry:\n    final_union_df = joined_df.union(invoice_sales_df)\n    logger.info(f\"Final Union operation completed with {final_union_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error during final Union operation: {e}\")\n\n# Custom Calculation for null_yn (Node 79)\ntry:\n    null_yn_df = final_union_df.withColumn(\"null_yn\", F.when(F.col(\"FNC_ID\").isNull() | (F.col(\"FNC_ID\") == \"\"), \"Y\")\n                                           .when(F.col(\"Whs\").isNull() | (F.col(\"Whs\") == \"\"), \"Y\")\n                                           .when(F.col(\"DIST_CHNL_ID\").isNull() | (F.col(\"DIST_CHNL_ID\") == \"\"), \"Y\")\n                                           .when(F.col(\"SOLDTO\").isNull() | (F.col(\"SOLDTO\") == \"\"), \"Y\")\n                                           .when(F.col(\"SHIPTO\").isNull() | (F.col(\"SHIPTO\") == \"\"), \"Y\")\n                                           .otherwise(\"N\"))\n    logger.info(\"null_yn field calculated\")\nexcept Exception as e:\n    logger.error(f\"Error calculating null_yn field: {e}\")\n\n# Filter Tool (Node 80)\ntry:\n    filtered_df = null_yn_df.filter(F.col(\"null_yn\") == \"Y\")\n    logger.info(f\"Filter operation completed with {filtered_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error during Filter operation: {e}\")\n\n# UPDATE NULL for FNC_ID and FNC_DESC (Node 81)\ntry:\n    updated_null_df = filtered_df.withColumn(\"FNC_ID\", F.when(F.col(\"FNC_ID\").isNull() | (F.col(\"FNC_ID\") == \"\"), \"OTH\").otherwise(F.col(\"FNC_ID\"))) \\\n                                 .withColumn(\"FNC_DESC\", F.when(F.col(\"FNC_DESC\").isNull() | (F.col(\"FNC_DESC\") == \"\"), \"OTHER\").otherwise(F.col(\"FNC_DESC\")))\n    logger.info(\"UPDATE NULL transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error applying UPDATE NULL transformation: {e}\")\n\n# Union operation (Node 82)\ntry:\n    union_final_df = updated_null_df.union(final_union_df)\n    logger.info(f\"Union operation completed with {union_final_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error during Union operation: {e}\")\n\n# Select Tool (Node 83)\ntry:\n    select_df = union_final_df.select(\"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Rush_Order_Fee\", \"Sum_Invoice_Lines\", \"Run Date\", \"StartTXT\", \"EndTXT\", \"Start Date\", \"START_2WK\", \"END_2WK\", \"End Date\", \"BIA_SHIP_HNDL_AMT\", \"COE_SHIP_HNDL_AMT\", \"Invoice_Sales\", \"DIST_CHNL\", \"DIST_CHNL_DESC\")\n    logger.info(\"Select Tool transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error applying Select Tool transformation: {e}\")\n\n# Alteryx Select (Node 57)\ntry:\n    alteryx_select_df = select_df.withColumnRenamed(\"Run Date\", \"RunDTE\")\n    logger.info(\"Alteryx Select transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error applying Alteryx Select transformation: {e}\")\n\n# Write to Unity Catalog target table\ntry:\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"table_name\"\n    \n    # Create schema if it doesn't exist\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n    \n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    alteryx_select_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table}\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog: {e}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}