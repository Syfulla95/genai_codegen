{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom datetime import datetime\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to load data from Unity Catalog tables\ndef load_data_from_catalog(table_name: str) -> DataFrame:\n    try:\n        df = spark.table(table_name)\n        logger.info(f\"Loaded data from {table_name} with {df.count()} records.\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading data from {table_name}: {str(e)}\")\n        raise\n\n# Function to perform transformations\ndef perform_transformations():\n    try:\n        # Node 69: TextInput Channel\n        text_input_df = load_data_from_catalog(\"catalog.source_db.text_input_channel\")\n        \n        # Node 24: Text Input Manual Date\n        manual_date_df = load_data_from_catalog(\"catalog.source_db.manual_date_input\")\n        \n        # Node 48: DbFileInput OCT_TC3\n        oct_tc3_df = load_data_from_catalog(\"catalog.source_db.oct_tc3\")\n        \n        # Node 18: Dynamic Input Week 1\n        week1_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week1\")\n        \n        # Node 77: Dynamic Input Week 2\n        week2_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week2\")\n        \n        # Node 89: Dynamic Input Week 5\n        week5_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week5\")\n        \n        # Node 86: Dynamic Input Week 4\n        week4_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week4\")\n        \n        # Node 84: Dynamic Input Week 3\n        week3_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week3\")\n        \n        # Node 19: DateTimeNow\n        current_date = datetime.now()\n        logger.info(f\"Current date generated: {current_date}\")\n        \n        # Node 20: Format\n        formatted_date = current_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n        logger.info(f\"Formatted date: {formatted_date}\")\n        \n        # Node 101: Summarize\n        summarize_df = oct_tc3_df.groupBy(\"BILL_DATE\").agg(\n            F.sum(\"Invoices\").alias(\"Sum_Invoices\"),\n            F.sum(\"Invoice_Lines\").alias(\"Sum_Invoice_Lines\"),\n            F.sum(\"LANDED_COST\").alias(\"Sum_LANDED_COST\"),\n            F.sum(\"EXT_FINAL_PRICE\").alias(\"Sum_EXT_FINAL_PRICE\")\n        )\n        \n        # Node 72: Start / End\n        date_calculation_df = manual_date_df.withColumn(\"Prior_Week_Start\", F.date_sub(F.current_date(), 7))\n        \n        # Node 73: Alteryx Select\n        select_df = date_calculation_df.select(\"DateTime_Out\", \"Start Date\", \"End Date\")\n        \n        # Node 23: DateTime Conversion\n        start_txt_df = select_df.withColumn(\"StartTXT\", F.date_format(\"Start Date\", \"yyyy-MM-dd\"))\n        \n        # Node 63: DateTime Conversion\n        end_txt_df = start_txt_df.withColumn(\"EndTXT\", F.date_format(\"End Date\", \"yyyy-MM-dd\"))\n        \n        # Node 66: ReName\n        rename_df = end_txt_df.selectExpr(\"Run Date\", \"EndTXT\", \"StartTXT\", \"Start Date\", \"End Date\")\n        \n        # Node 88: Formula: 2WK START AND END\n        formula_df = rename_df.withColumn(\"START_2WK\", F.date_sub(F.current_date(), 14))\n        \n        # Node 22: Select Tool\n        select_tool_df = formula_df.select(\"Run Date\", \"StartDTE\", \"EndDTE\", \"Prior Week Start\", \"Prior Week End\", \"Yesterday\", \"Today\", \"StartTXT\", \"EndTXT\", \"RunDTE\", \"P1M_Start\", \"P2M_Start\", \"P2M_End\", \"P3M_End\", \"Start Date\", \"END_1WK\", \"START_2WK\", \"END_2WK\", \"START_3WK\", \"END_3WK\", \"START_4WK\", \"End Date\", \"End_4Wk\", \"START_5WK\")\n        \n        # Node 78: Union\n        union_df = week1_df.union(week2_df).union(week3_df).union(week4_df).union(week5_df)\n        \n        # Node 27: Alteryx Select\n        alteryx_select_df = union_df.select(\"*\")\n        \n        # Node 40: FIELD NAMES\n        field_names_df = alteryx_select_df.selectExpr(\"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Invoice_lines\")\n        \n        # Node 67: MultiFieldFormula\n        multifield_formula_df = field_names_df.withColumn(\"EXTND_LAND_CST\", F.col(\"LANDED_COST\") * 1.1)\n        \n        # Node 44: Cleanse\n        cleanse_df = multifield_formula_df.dropDuplicates()\n        \n        # Node 60: Rush_Fee\n        rush_fee_df = cleanse_df.withColumn(\"Rush_Order_Fee\", F.lit(10))\n        \n        # Node 42: Summarize\n        summarize_rush_fee_df = rush_fee_df.groupBy(\"BILL_DATE\").agg(F.sum(\"Rush_Order_Fee\").alias(\"Sum_Rush_Order_Fee\"))\n        \n        # Node 46: Dynamic Rename\n        dynamic_rename_df = summarize_rush_fee_df.selectExpr(\"Invoice_Lines\", \"Rush_Order_Fee\", \"AVG_INVOICE_PRICE\", \"EXTND_FNL_PRICE1\", \"VBRP_BRGEW\", \"VBRP_VOLUM\")\n        \n        # Node 56: Append Fields\n        append_fields_df = dynamic_rename_df.withColumn(\"Additional_Field\", F.lit(\"Additional_Value\"))\n        \n        # Node 61: Total Shipping and Handling\n        shipping_handling_df = append_fields_df.withColumn(\"BIA_SHIP_HNDL_AMT\", F.col(\"Rush_Order_Fee\") + F.col(\"EXTND_FNL_PRICE1\"))\n        \n        # Node 68: Formula: Invoice_Sales\n        invoice_sales_df = shipping_handling_df.withColumn(\"Invoice_Sales\", F.col(\"Sum_Rush_Order_Fee\") + F.col(\"Sum_EXT_FINAL_PRICE\"))\n        \n        # Node 70: Join\n        join_df = invoice_sales_df.join(text_input_df, \"DIST_CHNL_ID\", \"inner\")\n        \n        # Node 71: Union\n        final_union_df = join_df.union(invoice_sales_df)\n        \n        # Node 79: Formula\n        formula_null_yn_df = final_union_df.withColumn(\"null_yn\", F.when(F.col(\"Invoice_Sales\").isNull(), 1).otherwise(0))\n        \n        # Node 80: Filter Tool\n        filter_tool_df = formula_null_yn_df.filter(F.col(\"null_yn\") == 0)\n        \n        # Node 81: UPDATE NULL\n        update_null_df = filter_tool_df.withColumn(\"FNC_ID\", F.when(F.col(\"FNC_ID\").isNull(), \"Unknown\").otherwise(F.col(\"FNC_ID\")))\n        \n        # Node 82: Union\n        final_df = update_null_df.union(filter_tool_df)\n        \n        # Node 83: Select Tool\n        select_tool_final_df = final_df.select(\"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Rush_Order_Fee\", \"Sum_Invoice_Lines\", \"Run Date\", \"StartTXT\", \"EndTXT\", \"Start Date\", \"START_2WK\", \"END_2WK\", \"End Date\", \"BIA_SHIP_HNDL_AMT\", \"COE_SHIP_HNDL_AMT\", \"Invoice_Sales\", \"DIST_CHNL\", \"DIST_CHNL_DESC\")\n        \n        # Node 57: Alteryx Select\n        alteryx_select_final_df = select_tool_final_df.selectExpr(\"RunDTE\", \"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Rush_Order_Fee\", \"COE_SHIP_HNDL_AMT\")\n        \n        # Write to Unity Catalog target table\n        target_catalog = \"catalog_name\"\n        target_schema = \"schema_name\"\n        target_table = \"table_name\"\n        \n        # Create schema if it doesn't exist\n        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n        logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n        \n        # Write to Unity Catalog target table (overwrite mode handles table replacement)\n        alteryx_select_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n        logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table}\")\n        \n    except Exception as e:\n        logger.error(f\"Error during transformations: {str(e)}\")\n        raise\n\n# Execute transformations\nperform_transformations()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}