{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom datetime import datetime, timedelta\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Helper function to safely execute SQL and log errors\ndef execute_sql(query: str) -> DataFrame:\n    try:\n        logger.info(f\"Executing SQL: {query}\")\n        return spark.sql(query)\n    except Exception as e:\n        logger.error(f\"Error executing SQL: {query}, Error: {str(e)}\")\n        raise\n\n# Load data from Unity Catalog tables\ntry:\n    oct_tc3_df = execute_sql(\"SELECT * FROM catalog.source_db.oct_tc3\")\n    week1_df = execute_sql(\"SELECT * FROM catalog.source_db.week1\")\n    week2_df = execute_sql(\"SELECT * FROM catalog.source_db.week2\")\n    week3_df = execute_sql(\"SELECT * FROM catalog.source_db.week3\")\n    week4_df = execute_sql(\"SELECT * FROM catalog.source_db.week4\")\n    week5_df = execute_sql(\"SELECT * FROM catalog.source_db.week5\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog: {str(e)}\")\n    raise\n\n# Transformation: DateTimeNow\ncurrent_date = datetime.now()\nlogger.info(f\"Current Date: {current_date}\")\n\n# Transformation: Format DateTime\ndate_time_out = current_date.strftime('%Y-%m-%d')\nlogger.info(f\"Formatted DateTime: {date_time_out}\")\n\n# Transformation: Summarize\nsummarized_df = oct_tc3_df.groupBy(\"BILL_DTE\").agg(\n    F.sum(\"Invoices\").alias(\"Sum_Invoices\"),\n    F.sum(\"Invoice_Lines\").alias(\"Sum_Invoice_Lines\"),\n    F.sum(\"LANDED_COST\").alias(\"Sum_LANDED_COST\"),\n    F.sum(\"EXT_FINAL_PRICE\").alias(\"Sum_EXT_FINAL_PRICE\"),\n    F.sum(\"Trans_Charge_Amt\").alias(\"Sum_Trans_Charge_Amt\"),\n    F.sum(\"RESTOCK_Fee\").alias(\"Sum_RESTOCK_Fee\"),\n    F.sum(\"Special_Hndl_Amt\").alias(\"Sum_Special_Hndl_Amt\"),\n    F.sum(\"Vendor_Hndl_Amt\").alias(\"Sum_Vendor_Hndl_Amt\"),\n    F.sum(\"MOC_Amt\").alias(\"Sum_MOC_Amt\"),\n    F.sum(\"Fuel_Surcharge\").alias(\"Sum_Fuel_Surcharge\")\n)\nlogger.info(\"Summarization complete\")\n\n# Transformation: Date Calculation\nprior_week_start = current_date - timedelta(days=current_date.weekday() + 7)\nprior_week_end = prior_week_start + timedelta(days=6)\nyesterday = current_date - timedelta(days=1)\ntoday = current_date\np1m_start = (current_date.replace(day=1) - timedelta(days=1)).replace(day=1)\np1m_end = p1m_start + timedelta(days=31)\np2m_start = (p1m_start.replace(day=1) - timedelta(days=1)).replace(day=1)\np2m_end = p2m_start + timedelta(days=31)\np3m_start = (p2m_start.replace(day=1) - timedelta(days=1)).replace(day=1)\np3m_end = p3m_start + timedelta(days=31)\n\nlogger.info(f\"Date Calculations: Prior Week Start: {prior_week_start}, Prior Week End: {prior_week_end}, Yesterday: {yesterday}, Today: {today}\")\n\n# Transformation: Union\nunion_df = week1_df.union(week2_df).union(week3_df).union(week4_df).union(week5_df)\nlogger.info(\"Union of weekly data complete\")\n\n# Transformation: Custom Calculation\nrush_order_fee_df = union_df.withColumn(\"Rush_Order_Fee\", F.col(\"ADDTN_TRANS_FEE_OVRRIDE_ZSRO\"))\nsum_rush_order_fee_df = rush_order_fee_df.groupBy().agg(F.sum(\"Rush_Order_Fee\").alias(\"Sum_Rush_Order_Fee\"))\nlogger.info(\"Rush Order Fee calculation complete\")\n\n# Transformation: Total Shipping and Handling\ntotal_shipping_handling_df = summarized_df.withColumn(\"BIA_SHIP_HNDL_AMT\", F.col(\"Sum_Trans_Charge_Amt\") * 0.1) \\\n                                          .withColumn(\"COE_SHIP_HNDL_AMT\", F.col(\"Sum_Trans_Charge_Amt\") * 0.2)\nlogger.info(\"Total Shipping and Handling calculation complete\")\n\n# Transformation: Invoice Sales\ninvoice_sales_df = summarized_df.withColumn(\"Invoice_Sales\", F.col(\"Sum_EXT_FINAL_PRICE\") * 1.05)\nlogger.info(\"Invoice Sales calculation complete\")\n\n# Transformation: Join\njoined_df = invoice_sales_df.join(oct_tc3_df, \"DIST_CHNL_ID\", \"inner\")\nlogger.info(\"Join operation complete\")\n\n# Transformation: Filter\nfiltered_df = joined_df.filter(F.col(\"DIST_CHNL_ID\").isNotNull())\nlogger.info(\"Filter operation complete\")\n\n# Transformation: Update Null\nupdated_df = filtered_df.withColumn(\"FNC_ID\", F.when(F.col(\"FNC_ID\").isNull(), F.lit(\"Unknown\")).otherwise(F.col(\"FNC_ID\"))) \\\n                        .withColumn(\"FNC_DESC\", F.when(F.col(\"FNC_DESC\").isNull(), F.lit(\"Unknown\")).otherwise(F.col(\"FNC_DESC\")))\nlogger.info(\"Update Null operation complete\")\n\n# Write to Unity Catalog target table\ntarget_catalog = \"catalog_name\"\ntarget_schema = \"schema_name\"\ntarget_table = \"table_name\"\n\n# Ensure schema exists before creating table\nspark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\nlogger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n# Write to Unity Catalog target table (overwrite mode handles table replacement)\nupdated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\nlogger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table} successfully\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}