{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom datetime import datetime\nimport psycopg2\nfrom pyspark.sql import functions as F\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Step 1: Load Data Sources\ntry:\n    # Load static text input for Node 69\n    dist_chnl_df = spark.table(\"catalog.source_db.dist_chnl\")\n\n    # Load manual date input for Node 24\n    manual_date_df = spark.table(\"catalog.source_db.manual_date\")\n\n    # Load data from SQL Server for Node 48\n    sql_server_conn_info = {\n        \"host\": dbutils.secrets.get(\"sql_server_scope\", \"host\"),\n        \"database\": dbutils.secrets.get(\"sql_server_scope\", \"database\"),\n        \"user\": dbutils.secrets.get(\"sql_server_scope\", \"user\"),\n        \"password\": dbutils.secrets.get(\"sql_server_scope\", \"password\")\n    }\n    conn = psycopg2.connect(\n        host=sql_server_conn_info[\"host\"],\n        database=sql_server_conn_info[\"database\"],\n        user=sql_server_conn_info[\"user\"],\n        password=sql_server_conn_info[\"password\"]\n    )\n    sql_query = \"SELECT * FROM OCT_TC3\"\n    oct_tc3_df = spark.read.format(\"jdbc\").option(\"url\", f\"jdbc:postgresql://{sql_server_conn_info['host']}/{sql_server_conn_info['database']}\").option(\"query\", sql_query).option(\"user\", sql_server_conn_info[\"user\"]).option(\"password\", sql_server_conn_info[\"password\"]).load()\n\n    # Load dynamic inputs for Nodes 18, 77, 89, 86, 84\n    dynamic_week_dfs = []\n    for week_node in [\"week1\", \"week2\", \"week3\", \"week4\", \"week5\"]:\n        dynamic_week_df = spark.table(f\"catalog.source_db.dynamic_input_{week_node}\")\n        dynamic_week_dfs.append(dynamic_week_df)\n\n    logger.info(\"Data sources loaded successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error loading data sources: {e}\")\n    raise\n\n# Step 2: Implement Transformations\ntry:\n    # Node 19: DateTimeNow\n    current_date_df = spark.createDataFrame([(datetime.now(),)], [\"CurrentDate\"])\n\n    # Node 20: Format\n    formatted_date_df = current_date_df.withColumn(\"DateTime_Out\", F.date_format(\"CurrentDate\", \"yyyy-MM-dd HH:mm:ss\"))\n\n    # Node 101: Summarize\n    summarize_df = oct_tc3_df.groupBy(\"BILL_DATE\").agg(F.sum(\"INVOICES\").alias(\"Sum_Invoices\"))\n\n    # Node 72: Start / End\n    start_end_df = manual_date_df.withColumn(\"Prior_Week_Start\", F.date_sub(\"Start_Date\", 7)).withColumn(\"Prior_Week_End\", F.date_sub(\"End_Date\", 7))\n\n    # Node 73: Alteryx Select\n    select_df = start_end_df.select(\"Start_Date\", \"End_Date\")\n\n    # Node 23 & 63: DateTime Conversion\n    datetime_conversion_df = select_df.withColumn(\"StartTXT\", F.date_format(\"Start_Date\", \"yyyy-MM-dd\")).withColumn(\"EndTXT\", F.date_format(\"End_Date\", \"yyyy-MM-dd\"))\n\n    # Node 66: ReName\n    rename_df = datetime_conversion_df.withColumnRenamed(\"StartTXT\", \"Run_Date\")\n\n    # Node 88: Formula: 2WK START AND END\n    formula_df = rename_df.withColumn(\"END_1WK\", F.date_sub(\"Run_Date\", 7)).withColumn(\"START_2WK\", F.date_sub(\"Run_Date\", 14))\n\n    # Node 22: Select Tool\n    select_tool_df = formula_df.select(\"Run_Date\")\n\n    # Node 78: Union\n    union_df = dynamic_week_dfs[0]\n    for df in dynamic_week_dfs[1:]:\n        union_df = union_df.union(df)\n\n    # Node 27: Alteryx Select\n    alteryx_select_df = union_df.select(\"*\")\n\n    # Node 40: FIELD NAMES\n    field_names_df = alteryx_select_df.withColumnRenamed(\"Unknown\", \"Renamed_Unknown\")\n\n    # Node 67: MultiFieldFormula\n    multifield_formula_df = field_names_df.fillna(\"N/A\")\n\n    # Node 44: Cleanse\n    cleanse_df = multifield_formula_df.select([F.upper(F.col(c)).alias(c) for c in multifield_formula_df.columns])\n\n    # Node 60: Output Preparation\n    output_preparation_df = cleanse_df\n\n    logger.info(\"Transformations implemented successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during transformations: {e}\")\n    raise\n\n# Step 3: Write Output\ntry:\n    # Ensure schema exists before creating table\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"output_table\"\n\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    output_preparation_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(\"Output written successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error writing output: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}