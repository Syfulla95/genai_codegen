{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nfrom pyspark.sql import functions as F\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Securely retrieve credentials for external systems\ndb_user = dbutils.secrets.get(scope=\"my_scope\", key=\"db_user\")\ndb_password = dbutils.secrets.get(scope=\"my_scope\", key=\"db_password\")\ndb_host = dbutils.secrets.get(scope=\"my_scope\", key=\"db_host\")\ndb_port = dbutils.secrets.get(scope=\"my_scope\", key=\"db_port\")\ndb_name = dbutils.secrets.get(scope=\"my_scope\", key=\"db_name\")\n\n# Connect to external PostgreSQL database\ntry:\n    conn = psycopg2.connect(\n        dbname=db_name,\n        user=db_user,\n        password=db_password,\n        host=db_host,\n        port=db_port\n    )\n    logger.info(\"Connected to PostgreSQL database successfully\")\nexcept Exception as e:\n    logger.error(f\"Error connecting to PostgreSQL database: {e}\")\n    raise\n\n# Load data from Unity Catalog tables\ntry:\n    text_input_channel_df = spark.table(\"catalog.source_db.text_input_channel\")\n    logger.info(f\"Loaded text_input_channel_df with {text_input_channel_df.count()} records\")\n    \n    manual_date_df = spark.table(\"catalog.source_db.manual_date\")\n    logger.info(f\"Loaded manual_date_df with {manual_date_df.count()} records\")\n    \n    oct_tc3_df = spark.table(\"catalog.source_db.oct_tc3\")\n    logger.info(f\"Loaded oct_tc3_df with {oct_tc3_df.count()} records\")\n    \n    week1_df = spark.table(\"catalog.source_db.week1\")\n    logger.info(f\"Loaded week1_df with {week1_df.count()} records\")\n    \n    week2_df = spark.table(\"catalog.source_db.week2\")\n    logger.info(f\"Loaded week2_df with {week2_df.count()} records\")\n    \n    week3_df = spark.table(\"catalog.source_db.week3\")\n    logger.info(f\"Loaded week3_df with {week3_df.count()} records\")\n    \n    week4_df = spark.table(\"catalog.source_db.week4\")\n    logger.info(f\"Loaded week4_df with {week4_df.count()} records\")\n    \n    week5_df = spark.table(\"catalog.source_db.week5\")\n    logger.info(f\"Loaded week5_df with {week5_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog tables: {e}\")\n    raise\n\n# Union transformation\ntry:\n    union_df = week1_df.union(week2_df).union(week3_df).union(week4_df).union(week5_df)\n    logger.info(f\"Union transformation completed with {union_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error during union transformation: {e}\")\n    raise\n\n# Rename fields\ntry:\n    renamed_df = union_df.withColumnRenamed(\"SO_AUDAT\", \"SO_Date\") \\\n                         .withColumnRenamed(\"FKDAT\", \"BILL_DATE\") \\\n                         .withColumnRenamed(\"WERKS\", \"Whs\") \\\n                         .withColumnRenamed(\"VTWEG\", \"DIST_CHNL_ID\") \\\n                         .withColumnRenamed(\"ZZFINCLASS\", \"FNC_ID\") \\\n                         .withColumnRenamed(\"BEZEK\", \"FNC_DESC\") \\\n                         .withColumnRenamed(\"SOLDTO_KUNNR\", \"SOLDTO\") \\\n                         .withColumnRenamed(\"SHIPTO_KUNNR\", \"SHIPTO\") \\\n                         .withColumnRenamed(\"VGBEL\", \"RFRNC_DOC_NUM\") \\\n                         .withColumnRenamed(\"lines\", \"Invoice_lines\")\n    logger.info(\"Field renaming completed\")\nexcept Exception as e:\n    logger.error(f\"Error during field renaming: {e}\")\n    raise\n\n# MultiFieldFormula transformation\ntry:\n    formula_df = renamed_df.select(\n        *[F.when(F.col(c).isNull() | (F.col(c) == \"\"), 0).otherwise(F.col(c)).alias(c) for c in renamed_df.columns]\n    )\n    logger.info(\"MultiFieldFormula transformation completed\")\nexcept Exception as e:\n    logger.error(f\"Error during MultiFieldFormula transformation: {e}\")\n    raise\n\n# Cleansing transformation\ntry:\n    cleanse_df = formula_df.select(\n        *[F.upper(F.col(c)).alias(c) for c in formula_df.columns]\n    )\n    logger.info(\"Cleansing transformation completed\")\nexcept Exception as e:\n    logger.error(f\"Error during cleansing transformation: {e}\")\n    raise\n\n# Rush_Fee calculation\ntry:\n    rush_fee_df = cleanse_df.withColumn(\"Rush_Order_Fee\", F.col(\"ADDTN_TRANS_FEE_OVRRIDE_ZSRO\"))\n    logger.info(\"Rush_Fee calculation completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Rush_Fee calculation: {e}\")\n    raise\n\n# Summarize Rush_Order_Fee\ntry:\n    summarize_df = rush_fee_df.groupBy().agg(F.sum(\"Rush_Order_Fee\").alias(\"Sum_Rush_Order_Fee\"))\n    logger.info(\"Summarize Rush_Order_Fee completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Summarize Rush_Order_Fee: {e}\")\n    raise\n\n# Dynamic Rename\ntry:\n    dynamic_rename_df = summarize_df.select(\n        *[F.col(c).alias(c.replace(\"Sum_\", \"\")) for c in summarize_df.columns]\n    )\n    logger.info(\"Dynamic Rename completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Dynamic Rename: {e}\")\n    raise\n\n# Append Fields\ntry:\n    append_df = dynamic_rename_df.join(text_input_channel_df, \"DIST_CHNL_ID\", \"left\")\n    logger.info(\"Append Fields completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Append Fields: {e}\")\n    raise\n\n# Total Shipping and Handling calculations\ntry:\n    total_shipping_df = append_df.withColumn(\"BIA_SHIP_HNDL_AMT\", \n                                             F.col(\"Sum_Trans_Charge_Amt\") + F.col(\"Sum_RESTOCK_Fee\") + \n                                             F.col(\"Sum_Special_Hndl_Amt\") + F.col(\"Sum_Vendor_Hndl_Amt\") + \n                                             F.col(\"Sum_MOC_Amt\") + F.col(\"Sum_Fuel_Surcharge\")) \\\n                                 .withColumn(\"COE_SHIP_HNDL_AMT\", \n                                             F.col(\"Sum_Trans_Charge_Amt\") + F.col(\"Sum_RESTOCK_Fee\") + \n                                             F.col(\"Sum_Special_Hndl_Amt\") + F.col(\"Sum_Vendor_Hndl_Amt\") + \n                                             F.col(\"Sum_MOC_Amt\") + F.col(\"Sum_Fuel_Surcharge\") + \n                                             F.col(\"Rush_Order_Fee\") + F.col(\"VENDR_TRANS_CHRG_FRT_ZTV1\") + \n                                             F.col(\"MARKUP_VENDOR_TRANS_FEE_AMT_ZMT1\"))\n    logger.info(\"Total Shipping and Handling calculations completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Total Shipping and Handling calculations: {e}\")\n    raise\n\n# Formula: Invoice_Sales\ntry:\n    invoice_sales_df = total_shipping_df.withColumn(\"Invoice_Sales\", F.col(\"Sum_EXT_FINAL_PRICE\"))\n    logger.info(\"Formula: Invoice_Sales completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Formula: Invoice_Sales: {e}\")\n    raise\n\n# Join transformation\ntry:\n    join_df = invoice_sales_df.join(text_input_channel_df, \"DIST_CHNL_ID\", \"left\")\n    logger.info(\"Join transformation completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Join transformation: {e}\")\n    raise\n\n# Union transformation\ntry:\n    final_union_df = join_df.union(invoice_sales_df)\n    logger.info(\"Final Union transformation completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Final Union transformation: {e}\")\n    raise\n\n# Formula: Identify primary key null\ntry:\n    null_identification_df = final_union_df.withColumn(\"null_yn\", \n                                                       F.when(F.col(\"FNC_ID\").isNull() | (F.col(\"FNC_ID\") == \"\"), 'Y')\n                                                       .when(F.col(\"Whs\").isNull() | (F.col(\"Whs\") == \"\"), 'Y')\n                                                       .when(F.col(\"DIST_CHNL_ID\").isNull() | (F.col(\"DIST_CHNL_ID\") == \"\"), 'Y')\n                                                       .when(F.col(\"SOLDTO\").isNull() | (F.col(\"SOLDTO\") == \"\"), 'Y')\n                                                       .when(F.col(\"SHIPTO\").isNull() | (F.col(\"SHIPTO\") == \"\"), 'Y')\n                                                       .otherwise('N'))\n    logger.info(\"Formula: Identify primary key null completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Formula: Identify primary key null: {e}\")\n    raise\n\n# Filter Tool\ntry:\n    filtered_df = null_identification_df.filter(F.col(\"null_yn\") == 'Y')\n    logger.info(f\"Filter Tool completed with {filtered_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error during Filter Tool: {e}\")\n    raise\n\n# UPDATE NULL\ntry:\n    update_null_df = filtered_df.withColumn(\"FNC_ID\", \n                                            F.when(F.col(\"FNC_ID\").isNull() | (F.col(\"FNC_ID\") == \"\"), 'OTH')\n                                            .otherwise(F.col(\"FNC_ID\"))) \\\n                                .withColumn(\"FNC_DESC\", \n                                            F.when(F.col(\"FNC_DESC\").isNull() | (F.col(\"FNC_DESC\") == \"\"), 'OTHER')\n                                            .otherwise(F.col(\"FNC_DESC\")))\n    logger.info(\"UPDATE NULL completed\")\nexcept Exception as e:\n    logger.error(f\"Error during UPDATE NULL: {e}\")\n    raise\n\n# Union transformation\ntry:\n    final_df = update_null_df.union(filtered_df)\n    logger.info(f\"Final Union transformation completed with {final_df.count()} records\")\nexcept Exception as e:\n    logger.error(f\"Error during Final Union transformation: {e}\")\n    raise\n\n# Select Tool\ntry:\n    selected_df = final_df.select(\"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \n                                  \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Rush_Order_Fee\", \"COE_SHIP_HNDL_AMT\")\n    logger.info(\"Select Tool completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Select Tool: {e}\")\n    raise\n\n# Alteryx Select: Rename\ntry:\n    renamed_final_df = selected_df.withColumnRenamed(\"Run Date\", \"RunDTE\")\n    logger.info(\"Alteryx Select: Rename completed\")\nexcept Exception as e:\n    logger.error(f\"Error during Alteryx Select: Rename: {e}\")\n    raise\n\n# Write output to Unity Catalog target table\ntry:\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"table_name\"\n    \n    # Ensure schema exists before creating table\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n    \n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    renamed_final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table} successfully\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog target table: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}