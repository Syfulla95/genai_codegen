{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nfrom pyspark.sql import functions as F\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Securely retrieve credentials for external systems\ndb_host = dbutils.secrets.get(scope=\"my_scope\", key=\"db_host\")\ndb_port = dbutils.secrets.get(scope=\"my_scope\", key=\"db_port\")\ndb_name = dbutils.secrets.get(scope=\"my_scope\", key=\"db_name\")\ndb_user = dbutils.secrets.get(scope=\"my_scope\", key=\"db_user\")\ndb_password = dbutils.secrets.get(scope=\"my_scope\", key=\"db_password\")\n\n# Function to connect to PostgreSQL and fetch data\ndef fetch_data_from_postgresql(query):\n    try:\n        conn = psycopg2.connect(\n            host=db_host,\n            port=db_port,\n            database=db_name,\n            user=db_user,\n            password=db_password\n        )\n        cursor = conn.cursor()\n        cursor.execute(query)\n        data = cursor.fetchall()\n        cursor.close()\n        conn.close()\n        return data\n    except Exception as e:\n        logger.error(f\"Error fetching data from PostgreSQL: {e}\")\n        raise\n\n# Load data from Unity Catalog tables\ntry:\n    text_input_df = spark.table(\"catalog.source_db.text_input_channel\")\n    manual_date_df = spark.table(\"catalog.source_db.manual_date_input\")\n    oct_tc3_df = spark.table(\"catalog.source_db.oct_tc3\")\n    week1_df = spark.table(\"catalog.source_db.week1\")\n    week2_df = spark.table(\"catalog.source_db.week2\")\n    week3_df = spark.table(\"catalog.source_db.week3\")\n    week4_df = spark.table(\"catalog.source_db.week4\")\n    week5_df = spark.table(\"catalog.source_db.week5\")\n    logger.info(\"Data loaded from Unity Catalog tables\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog tables: {e}\")\n    raise\n\n# Transformation: Cleanse\ndef cleanse(df):\n    for col in df.columns:\n        df = df.withColumn(col, F.upper(F.col(col)))\n    return df\n\n# Apply cleansing transformation\ntry:\n    cleansed_df = cleanse(oct_tc3_df)\n    logger.info(\"Cleansing transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during cleansing transformation: {e}\")\n    raise\n\n# Transformation: Rush_Fee\ntry:\n    rush_fee_df = cleansed_df.withColumn(\"Rush_Order_Fee\", F.col(\"ADDTN_TRANS_FEE_OVRRIDE_ZSRO\"))\n    logger.info(\"Rush_Fee transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Rush_Fee transformation: {e}\")\n    raise\n\n# Transformation: Summarize\ntry:\n    summarize_df = rush_fee_df.groupBy().agg(F.sum(\"Rush_Order_Fee\").alias(\"Sum_Rush_Order_Fee\"))\n    logger.info(\"Summarize transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Summarize transformation: {e}\")\n    raise\n\n# Transformation: Dynamic Rename\ndef dynamic_rename(df):\n    for col in df.columns:\n        if col.startswith(\"Sum_\"):\n            df = df.withColumnRenamed(col, col.replace(\"Sum_\", \"\"))\n    return df\n\n# Apply dynamic rename transformation\ntry:\n    renamed_df = dynamic_rename(summarize_df)\n    logger.info(\"Dynamic Rename transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Dynamic Rename transformation: {e}\")\n    raise\n\n# Transformation: Append Fields\ntry:\n    appended_df = renamed_df.unionByName(week1_df).unionByName(week2_df).unionByName(week3_df).unionByName(week4_df).unionByName(week5_df)\n    logger.info(\"Append Fields transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Append Fields transformation: {e}\")\n    raise\n\n# Transformation: Total Shipping and Handling\ntry:\n    total_shipping_df = appended_df.withColumn(\"BIA_SHIP_HNDL_AMT\", F.col(\"EXTND_SHPNG_HNDLNG\") + F.col(\"COE_SHIP_HNDL_AMT\"))\n    logger.info(\"Total Shipping and Handling transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Total Shipping and Handling transformation: {e}\")\n    raise\n\n# Transformation: Formula: Invoice_Sales\ntry:\n    invoice_sales_df = total_shipping_df.withColumn(\"Invoice_Sales\", F.col(\"Sum_EXT_FINAL_PRICE\"))\n    logger.info(\"Formula: Invoice_Sales transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Formula: Invoice_Sales transformation: {e}\")\n    raise\n\n# Transformation: Join\ntry:\n    joined_df = invoice_sales_df.join(text_input_df, [\"DIST_CHNL_ID\", \"DIST_CHNL\"], \"inner\")\n    logger.info(\"Join transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Join transformation: {e}\")\n    raise\n\n# Transformation: Union\ntry:\n    union_df = joined_df.unionByName(manual_date_df)\n    logger.info(\"Union transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Union transformation: {e}\")\n    raise\n\n# Transformation: Formula\ntry:\n    formula_df = union_df.withColumn(\"null_yn\", F.when(F.col(\"DIST_CHNL_ID\").isNull(), \"Y\").otherwise(\"N\"))\n    logger.info(\"Formula transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Formula transformation: {e}\")\n    raise\n\n# Transformation: Filter Tool\ntry:\n    filtered_df = formula_df.filter(F.col(\"null_yn\") == \"Y\")\n    logger.info(\"Filter Tool transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Filter Tool transformation: {e}\")\n    raise\n\n# Transformation: UPDATE NULL\ntry:\n    update_null_df = filtered_df.withColumn(\"FNC_ID\", F.when(F.col(\"FNC_ID\").isNull(), \"OTH\").otherwise(F.col(\"FNC_ID\"))) \\\n                                .withColumn(\"FNC_DESC\", F.when(F.col(\"FNC_DESC\").isNull(), \"OTHER\").otherwise(F.col(\"FNC_DESC\")))\n    logger.info(\"UPDATE NULL transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during UPDATE NULL transformation: {e}\")\n    raise\n\n# Transformation: Union\ntry:\n    final_union_df = update_null_df.unionByName(joined_df)\n    logger.info(\"Final Union transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Final Union transformation: {e}\")\n    raise\n\n# Transformation: Select Tool\ntry:\n    select_tool_df = final_union_df.select(\"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Rush_Order_Fee\", \"Sum_Invoice_Lines\", \"Run Date\", \"StartTXT\", \"EndTXT\", \"Start Date\", \"START_2WK\", \"END_2WK\", \"End Date\", \"BIA_SHIP_HNDL_AMT\", \"COE_SHIP_HNDL_AMT\", \"Invoice_Sales\", \"DIST_CHNL\", \"DIST_CHNL_DESC\")\n    logger.info(\"Select Tool transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Select Tool transformation: {e}\")\n    raise\n\n# Transformation: Alteryx Select\ntry:\n    alteryx_select_df = select_tool_df.select(\"RunDTE\", \"SO_Date\", \"BILL_DATE\", \"Whs\", \"DIST_CHNL_ID\", \"FNC_ID\", \"FNC_DESC\", \"SOLDTO\", \"SHIPTO\", \"RFRNC_DOC_NUM\", \"Rush_Order_Fee\", \"COE_SHIP_HNDL_AMT\")\n    logger.info(\"Alteryx Select transformation applied\")\nexcept Exception as e:\n    logger.error(f\"Error during Alteryx Select transformation: {e}\")\n    raise\n\n# Output to Unity Catalog\ntry:\n    target_catalog = \"catalog_name\"\n    target_schema = \"schema_name\"\n    target_table = \"output_table\"\n\n    # Ensure schema exists before creating table\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    alteryx_select_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table}\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog: {e}\")\n    raise\n\n# Output to CSV\ntry:\n    csv_path = \"/mnt/output/CHresult.csv\"\n    alteryx_select_df.coalesce(1).write.csv(csv_path, header=True, mode=\"overwrite\")\n    logger.info(f\"Data written to CSV at {csv_path}\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to CSV: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}