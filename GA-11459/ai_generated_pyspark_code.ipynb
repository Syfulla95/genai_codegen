{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport psycopg2\nimport requests\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, DecimalType, StringType, DateType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Securely retrieve credentials using Databricks utilities\npostgres_user = dbutils.secrets.get(scope=\"my_scope\", key=\"postgres_user\")\npostgres_password = dbutils.secrets.get(scope=\"my_scope\", key=\"postgres_password\")\nmysql_user = dbutils.secrets.get(scope=\"my_scope\", key=\"mysql_user\")\nmysql_password = dbutils.secrets.get(scope=\"my_scope\", key=\"mysql_password\")\nsqlserver_user = dbutils.secrets.get(scope=\"my_scope\", key=\"sqlserver_user\")\nsqlserver_password = dbutils.secrets.get(scope=\"my_scope\", key=\"sqlserver_password\")\nfraud_api_key = dbutils.secrets.get(scope=\"my_scope\", key=\"fraud_api_key\")\ncredit_api_key = dbutils.secrets.get(scope=\"my_scope\", key=\"credit_api_key\")\n\n# Load data from Unity Catalog tables\ntry:\n    customer_demo_df = spark.table(\"catalog.demographics_db.customer_demographics_table\")\n    policy_data_df = spark.table(\"catalog.policy_db.policy_table\")\n    claims_data_df = spark.table(\"catalog.claims_db.claims_table\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog: {e}\")\n    raise\n\n# Fetch Fraud Score from REST API\ndef fetch_fraud_score(customer_id):\n    try:\n        response = requests.get(\n            \"https://fraudscore.api/endpoint\",\n            headers={\"Authorization\": f\"Bearer {fraud_api_key}\"},\n            params={\"customer_id\": customer_id}\n        )\n        response.raise_for_status()\n        return response.json().get(\"fraud_score\", None)\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Error fetching fraud score for customer {customer_id}: {e}\")\n        return None\n\n# Fetch Credit Score from REST API\ndef fetch_credit_score(customer_id):\n    try:\n        response = requests.get(\n            \"https://creditscore.api/endpoint\",\n            headers={\"Authorization\": f\"Bearer {credit_api_key}\"},\n            params={\"customer_id\": customer_id}\n        )\n        response.raise_for_status()\n        return response.json().get(\"credit_score\", None)\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Error fetching credit score for customer {customer_id}: {e}\")\n        return None\n\n# Join policy data with customer demographics\npolicy_demo_df = policy_data_df.join(customer_demo_df, \"Customer_ID\", \"inner\")\n\n# Join the result with claims data\npolicy_claims_df = policy_demo_df.join(claims_data_df, \"Policy_ID\", \"inner\")\n\n# Aggregate data at customer level\nagg_customer_df = policy_claims_df.groupBy(\"Customer_ID\").agg(\n    F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n    F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\"),\n    F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n    F.countDistinct(\"Policy_ID\").alias(\"Policy_Count\")\n)\n\n# Calculate derived fields\nderived_df = agg_customer_df.withColumn(\"Age\", F.datediff(F.current_date(), F.col(\"Date_of_Birth\")) / 365) \\\n    .withColumn(\"Claim_To_Premium_Ratio\", F.when(F.col(\"Total_Premium_Paid\") != 0, F.col(\"Claim_Amount\") / F.col(\"Total_Premium_Paid\")).otherwise(0)) \\\n    .withColumn(\"Claims_Per_Policy\", F.when(F.col(\"Policy_Count\") != 0, F.col(\"Total_Claims\") / F.col(\"Policy_Count\")).otherwise(0)) \\\n    .withColumn(\"Retention_Rate\", F.lit(0.85)) \\\n    .withColumn(\"Cross_Sell_Opportunities\", F.lit(\"Multi-Policy Discount, Home Coverage Add-on\")) \\\n    .withColumn(\"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\"))\n\n# Fetch fraud and credit scores\nfraud_scores = derived_df.select(\"Customer_ID\").rdd.map(lambda row: (row[\"Customer_ID\"], fetch_fraud_score(row[\"Customer_ID\"])))\ncredit_scores = derived_df.select(\"Customer_ID\").rdd.map(lambda row: (row[\"Customer_ID\"], fetch_credit_score(row[\"Customer_ID\"])))\n\nfraud_scores_df = spark.createDataFrame(fraud_scores, [\"Customer_ID\", \"Fraud_Score\"])\ncredit_scores_df = spark.createDataFrame(credit_scores, [\"Customer_ID\", \"Credit_Score\"])\n\n# Join fraud and credit scores with derived data\nfinal_df = derived_df.join(fraud_scores_df, \"Customer_ID\", \"left\").join(credit_scores_df, \"Customer_ID\", \"left\")\n\n# Add AI-driven insights\nfinal_df = final_df.withColumn(\"Churn_Probability\", F.lit(0.25)) \\\n    .withColumn(\"Next_Best_Offer\", F.lit(\"Additional Life Coverage\")) \\\n    .withColumn(\"Claims_Fraud_Probability\", F.lit(0.10)) \\\n    .withColumn(\"Revenue_Potential\", F.lit(12000.00))\n\n# Write the final data to Unity Catalog target table\ntry:\n    spark.sql(\"DROP TABLE IF EXISTS catalog.customer_dw.customer_360\")\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.customer_dw.customer_360\")\n    logger.info(\"Data successfully written to Unity Catalog target table.\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog target table: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}