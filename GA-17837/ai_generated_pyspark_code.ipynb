{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom datetime import datetime\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to load data from Unity Catalog tables\ndef load_data_from_catalog(table_name: str) -> DataFrame:\n    try:\n        df = spark.table(table_name)\n        logger.info(f\"Loaded data from {table_name} with {df.count()} records.\")\n        return df\n    except Exception as e:\n        logger.error(f\"Error loading data from {table_name}: {str(e)}\")\n        raise\n\n# Function to perform transformations\ndef perform_transformations():\n    try:\n        # Load data from Unity Catalog tables\n        text_input_df = load_data_from_catalog(\"catalog.source_db.text_input_channel\")\n        manual_date_df = load_data_from_catalog(\"catalog.source_db.text_input_manual_date\")\n        oct_tc3_df = load_data_from_catalog(\"catalog.source_db.dbfileinput_oct_tc3\")\n        week1_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week1\")\n        week2_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week2\")\n        week3_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week3\")\n        week4_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week4\")\n        week5_df = load_data_from_catalog(\"catalog.source_db.dynamic_input_week5\")\n\n        # Perform transformations\n        # Example: Summarize operation\n        summarize_df = oct_tc3_df.groupBy(\"BILL_DTE\").agg(\n            F.sum(\"Invoices\").alias(\"Sum_Invoices\"),\n            F.sum(\"Invoice_Lines\").alias(\"Sum_Invoice_Lines\"),\n            F.sum(\"LANDED_COST\").alias(\"Sum_LANDED_COST\"),\n            F.sum(\"EXT_FINAL_PRICE\").alias(\"Sum_EXT_FINAL_PRICE\"),\n            F.sum(\"Trans_Charge_Amt\").alias(\"Sum_Trans_Charge_Amt\"),\n            F.sum(\"RESTOCK_Fee\").alias(\"Sum_RESTOCK_Fee\"),\n            F.sum(\"Special_Hndl_Amt\").alias(\"Sum_Special_Hndl_Amt\"),\n            F.sum(\"Vendor_Hndl_Amt\").alias(\"Sum_Vendor_Hndl_Amt\"),\n            F.sum(\"MOC_Amt\").alias(\"Sum_MOC_Amt\"),\n            F.sum(\"Fuel_Surcharge\").alias(\"Sum_Fuel_Surcharge\")\n        )\n        logger.info(f\"Summarized data with {summarize_df.count()} records.\")\n\n        # Example: DateTimeNow and DateTime Format\n        current_date = datetime.now().strftime('%Y-%m-%d')\n        date_df = spark.createDataFrame([(current_date,)], [\"CurrentDate\"])\n        date_df = date_df.withColumn(\"DateTime_Out\", F.date_format(F.col(\"CurrentDate\"), \"yyyy-MM-dd\"))\n        logger.info(f\"Generated current date: {current_date}\")\n\n        # Example: Join operation\n        joined_df = text_input_df.join(summarize_df, text_input_df.DIST_CHNL == summarize_df.DIST_CHNL_ID, \"inner\")\n        logger.info(f\"Joined data with {joined_df.count()} records.\")\n\n        # Example: Union operation\n        union_df = week1_df.union(week2_df).union(week3_df).union(week4_df).union(week5_df)\n        logger.info(f\"Unioned data with {union_df.count()} records.\")\n\n        # Example: Filter operation\n        filtered_df = union_df.filter(F.col(\"null_yn\").isNotNull())\n        logger.info(f\"Filtered data with {filtered_df.count()} records.\")\n\n        # Example: Rename operation\n        renamed_df = filtered_df.withColumnRenamed(\"Sum_Invoices\", \"Invoices\")\n        logger.info(f\"Renamed fields in data.\")\n\n        # Example: Calculate Invoice Sales\n        invoice_sales_df = renamed_df.withColumn(\"Invoice_Sales\", F.col(\"Sum_EXT_FINAL_PRICE\") * 1.1)  # Example calculation\n        logger.info(f\"Calculated Invoice Sales.\")\n\n        # Example: Write to Unity Catalog target table\n        target_catalog = \"catalog_name\"\n        target_schema = \"schema_name\"\n        target_table = \"table_name\"\n\n        # Create schema if it doesn't exist\n        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n        logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n        # Write to Unity Catalog target table (overwrite mode handles table replacement)\n        invoice_sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n        logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table}\")\n\n    except Exception as e:\n        logger.error(f\"Error during transformations: {str(e)}\")\n        raise\n\n# Execute the ETL workflow\nperform_transformations()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}