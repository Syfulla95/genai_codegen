{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# %md\n# # ETL Process for Customer 360 Profile\n# This notebook performs an ETL process to create a comprehensive customer profile using various datasets.\n\n# COMMAND ----------\n#\n",
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import count, avg, max, expr, datediff, current_date\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is already initialized as 'spark'\n\n# COMMAND ----------\n# %md\n# ## Step 1: Data Ingestion\n# Load data from CSV files into DataFrames.\n\n# COMMAND ----------\n#\n",
                "def load_data():\n    try:\n        logger.info(\"Loading data from CSV files into DataFrames\")\n        policy_df = spark.read.csv(\"dbfs:/path/to/policy.csv\", header=True, inferSchema=True)\n        claims_df = spark.read.csv(\"dbfs:/path/to/claims.csv\", header=True, inferSchema=True)\n        demographics_df = spark.read.csv(\"dbfs:/path/to/demographics.csv\", header=True, inferSchema=True)\n        scores_df = spark.read.csv(\"dbfs:/path/to/scores.csv\", header=True, inferSchema=True)\n        aiml_insights_df = spark.read.csv(\"dbfs:/path/to/aiml_insights.csv\", header=True, inferSchema=True)\n        return policy_df, claims_df, demographics_df, scores_df, aiml_insights_df\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\npolicy_df, claims_df, demographics_df, scores_df, aiml_insights_df = load_data()\n\n# COMMAND ----------\n# %md\n# ## Step 2: Data Selection and Filtering\n# Select relevant fields from each dataset.\n\n# COMMAND ----------\n#\n",
                "def select_fields(demographics_df, claims_df, policy_df, scores_df, aiml_insights_df):\n    try:\n        logger.info(\"Selecting relevant fields from each dataset\")\n        demographics_selected = demographics_df.select(\n            \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \n            \"City\", \"State\", \"Postal_Code\", \"Date_of_Birth\", \"Gender\", \n            \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n        )\n        claims_selected = claims_df.select(\n            \"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \n            \"Claim_Status\", \"Claim_Amount\", \"Claim_Payout\"\n        )\n        policy_selected = policy_df.select(\n            \"policy_id\", \"customer_id\", \"policy_type\", \"policy_status\", \n            \"policy_start_date\", \"policy_end_date\", \"policy_term\", \n            \"policy_premium\", \"total_premium_paid\", \"renewal_status\", \"policy_addons\"\n        )\n        scores_selected = scores_df.select(\n            \"Customer_ID\", \"Credit_Score\", \"Fraud_Score\", \"Customer_Risk_Score\"\n        )\n        aiml_insights_selected = aiml_insights_df.select(\n            \"Customer_ID\", \"Churn_Probability\", \"Next_Best_Offer\", \n            \"Claims_Fraud_Probability\", \"Revenue_Potential\"\n        )\n        return demographics_selected, claims_selected, policy_selected, scores_selected, aiml_insights_selected\n    except Exception as e:\n        logger.error(f\"Error selecting fields: {e}\")\n        raise\n\ndemographics_selected, claims_selected, policy_selected, scores_selected, aiml_insights_selected = select_fields(\n    demographics_df, claims_df, policy_df, scores_df, aiml_insights_df\n)\n\n# COMMAND ----------\n# %md\n# ## Step 3: Data Integration\n# Join datasets based on common identifiers.\n\n# COMMAND ----------\n#\n",
                "def integrate_data(demographics_selected, policy_selected, claims_selected):\n    try:\n        logger.info(\"Joining datasets based on common identifiers\")\n        demographics_policy_joined = demographics_selected.join(\n            policy_selected, F.col('Customer_ID') == F.col('customer_id'), \"inner\"\n        )\n        all_data_joined = demographics_policy_joined.join(\n            claims_selected, F.col('policy_id') == F.col('Policy_ID'), \"inner\"\n        )\n        return all_data_joined\n    except Exception as e:\n        logger.error(f\"Error joining datasets: {e}\")\n        raise\n\nall_data_joined = integrate_data(demographics_selected, policy_selected, claims_selected)\n\n# COMMAND ----------\n# %md\n# ## Step 4: Data Aggregation\n# Aggregate data to compute key metrics.\n\n# COMMAND ----------\n#\n",
                "def aggregate_data(all_data_joined):\n    try:\n        logger.info(\"Aggregating data to compute key metrics\")\n        aggregated_data = all_data_joined.groupBy(\"Customer_ID\").agg(\n            count(\"Claim_ID\").alias(\"Total_Claims\"),\n            count(\"policy_id\").alias(\"Policy_Count\"),\n            max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n            avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n        )\n        return aggregated_data\n    except Exception as e:\n        logger.error(f\"Error aggregating data: {e}\")\n        raise\n\naggregated_data = aggregate_data(all_data_joined)\n\n# COMMAND ----------\n# %md\n# ## Step 5: Custom Calculations\n# Implement custom calculations for additional metrics.\n\n# COMMAND ----------\n#\n",
                "def custom_calculations(aggregated_data):\n    try:\n        logger.info(\"Implementing custom calculations for additional metrics\")\n        age_expr = datediff(current_date(), F.col('Date_of_Birth')) / 365\n        claim_to_premium_ratio_expr = expr(\"Claim_Amount / total_premium_paid\")\n        claims_per_policy_expr = expr(\"Total_Claims / Policy_Count\")\n        retention_rate_expr = expr(\"0.85\")\n        cross_sell_opportunities_expr = expr(\"'Multi-Policy Discount, Home Coverage Add-on'\")\n        upsell_potential_expr = expr(\"'Premium Vehicle Coverage'\")\n\n        calculated_data = aggregated_data.withColumn(\"Age\", age_expr) \\\n            .withColumn(\"Claim_To_Premium_Ratio\", claim_to_premium_ratio_expr) \\\n            .withColumn(\"Claims_Per_Policy\", claims_per_policy_expr) \\\n            .withColumn(\"Retention_Rate\", retention_rate_expr) \\\n            .withColumn(\"Cross_Sell_Opportunities\", cross_sell_opportunities_expr) \\\n            .withColumn(\"Upsell_Potential\", upsell_potential_expr)\n        return calculated_data\n    except Exception as e:\n        logger.error(f\"Error in custom calculations: {e}\")\n        raise\n\ncalculated_data = custom_calculations(aggregated_data)\n\n# COMMAND ----------\n# %md\n# ## Step 6: Comprehensive Data Consolidation\n# Combine all processed data into a single customer profile.\n\n# COMMAND ----------\n#\n",
                "def consolidate_data(calculated_data, aiml_insights_selected, scores_selected):\n    try:\n        logger.info(\"Combining all processed data into a single customer profile\")\n        customer_360 = calculated_data.join(aiml_insights_selected, \"Customer_ID\", \"inner\") \\\n            .join(scores_selected, \"Customer_ID\", \"inner\")\n        return customer_360\n    except Exception as e:\n        logger.error(f\"Error consolidating data: {e}\")\n        raise\n\ncustomer_360 = consolidate_data(calculated_data, aiml_insights_selected, scores_selected)\n\n# COMMAND ----------\n# %md\n# ## Step 7: Output Data Source\n# Write the comprehensive customer profile to Unity Catalog table.\n\n# COMMAND ----------\n#\n",
                "def write_to_catalog(customer_360):\n    try:\n        logger.info(\"Writing the comprehensive customer profile to Unity Catalog table\")\n        spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.customer_360\")\n        customer_360.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.customer_360\")\n    except Exception as e:\n        logger.error(f\"Error writing data to Unity Catalog: {e}\")\n        raise\n\nwrite_to_catalog(customer_360)\n\nlogger.info(\"ETL process completed successfully\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}