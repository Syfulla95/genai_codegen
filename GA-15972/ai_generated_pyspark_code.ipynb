{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\nfrom datetime import datetime\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    policy_df = spark.table(\"genai_demo.jnj.policy\")\n    claims_df = spark.table(\"genai_demo.jnj.claims\")\n    demographics_df = spark.table(\"genai_demo.jnj.demographics\")\n    scores_df = spark.table(\"genai_demo.jnj.scores\")\n    aiml_insights_df = spark.table(\"genai_demo.jnj.aiml_insights\")\n\n    # Select relevant fields from each DataFrame\n    demographics_selected_df = demographics_df.select(\n        \"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \"City\", \"State\", \"Postal_Code\",\n        \"Date_of_Birth\", \"Gender\", \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\"\n    )\n\n    claims_selected_df = claims_df.select(\n        \"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\", \"Claim_Amount\", \"Claim_Payout\"\n    )\n\n    policy_selected_df = policy_df.select(\n        \"Policy_ID\", \"Customer_ID\", \"Policy_Type\", \"Policy_Status\", \"Policy_Start_Date\", \"Policy_End_Date\",\n        \"Policy_Term\", \"Policy_Premium\", \"Total_Premium_Paid\", \"Renewal_Status\", \"Policy_Addons\"\n    )\n\n    scores_selected_df = scores_df.select(\n        \"Customer_ID\", \"Credit_Score\", \"Fraud_Score\", \"Customer_Risk_Score\"\n    )\n\n    aiml_insights_selected_df = aiml_insights_df.select(\n        \"Customer_ID\", \"Churn_Probability\", \"Next_Best_Offer\", \"Claims_Fraud_Probability\", \"Revenue_Potential\"\n    )\n\n    # Join demographics and policy data on Customer_ID\n    demographics_policy_joined_df = demographics_selected_df.join(\n        policy_selected_df, \"Customer_ID\", \"inner\"\n    )\n\n    # Join claims and policy data on Policy_ID\n    claims_policy_joined_df = claims_selected_df.join(\n        policy_selected_df, \"Policy_ID\", \"inner\"\n    )\n\n    # Summarize data\n    summarized_df = claims_policy_joined_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.count(\"Policy_ID\").alias(\"Policy_Count\"),\n        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\"),\n        F.sum(\"Claim_Amount\").alias(\"Total_Claim_Amount\")\n    )\n\n    # Join summarized data with demographics and policy data\n    combined_df = demographics_policy_joined_df.join(\n        summarized_df, \"Customer_ID\", \"inner\"\n    )\n\n    # Custom Calculations\n    combined_df = combined_df.withColumn(\n        \"Age\", F.datediff(F.current_date(), F.to_date(\"Date_of_Birth\")) / 365\n    ).withColumn(\n        \"Claim_To_Premium_Ratio\", F.when(combined_df[\"Total_Premium_Paid\"] != 0, combined_df[\"Total_Claim_Amount\"] / combined_df[\"Total_Premium_Paid\"]).otherwise(0)\n    ).withColumn(\n        \"Claims_Per_Policy\", F.when(combined_df[\"Policy_Count\"] != 0, combined_df[\"Total_Claims\"] / combined_df[\"Policy_Count\"]).otherwise(0)\n    ).withColumn(\n        \"Retention_Rate\", F.lit(0.85)\n    ).withColumn(\n        \"Cross_Sell_Opportunities\", F.lit(\"Multi-Policy Discount, Home Coverage Add-on\")\n    ).withColumn(\n        \"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\")\n    )\n\n    # Join with AI/ML insights and scores data\n    final_df = combined_df.join(\n        aiml_insights_selected_df, \"Customer_ID\", \"inner\"\n    ).join(\n        scores_selected_df, \"Customer_ID\", \"inner\"\n    )\n\n    # Ensure no duplicate columns exist\n    final_df = final_df.dropDuplicates([\"Customer_ID\"])\n\n    # Rename columns to avoid duplicates\n    final_df = final_df.withColumnRenamed(\"Age\", \"Customer_Age\")\n\n    # Define target schema and table\n    target_catalog = \"genai_demo\"\n    target_schema = \"guardian\"\n    target_table = \"customer_360\"\n\n    # Ensure schema exists\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Enable schema auto-merging for Delta table\n    spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n\n    # Write final DataFrame to Unity Catalog table with schema merge option\n    final_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(f\"Data written to {target_catalog}.{target_schema}.{target_table}\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}