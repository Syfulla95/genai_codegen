{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Step 1: Load Demographics Data\n    demographics_df = spark.table(\"genai_demo.jnj.demographics\")\n    logger.info(f\"Demographics Data Loaded: {demographics_df.count()} records\")\n\n    # Step 2: Load Claims Data\n    claims_df = spark.table(\"genai_demo.jnj.claims\")\n    logger.info(f\"Claims Data Loaded: {claims_df.count()} records\")\n\n    # Step 3: Load Policy Data\n    policy_df = spark.table(\"genai_demo.jnj.policy\")\n    logger.info(f\"Policy Data Loaded: {policy_df.count()} records\")\n\n    # Step 4: Load Scores Data\n    scores_df = spark.table(\"genai_demo.jnj.scores\")\n    logger.info(f\"Scores Data Loaded: {scores_df.count()} records\")\n\n    # Step 5: Load AI/ML Insights Data\n    aiml_insights_df = spark.table(\"genai_demo.jnj.aiml_insights\")\n    logger.info(f\"AI/ML Insights Data Loaded: {aiml_insights_df.count()} records\")\n\n    # Step 6: Select Tool (Demographics)\n    demographics_selected_df = demographics_df.withColumn(\n        \"Age\", \n        F.floor(F.datediff(F.current_date(), F.col(\"Date_of_Birth\")) / 365.25).cast(IntegerType())\n    ).select(\"Customer_ID\", \"Customer_Name\", \"Age\", \"Gender\")\n    logger.info(\"Selected fields from Demographics with calculated Age\")\n\n    # Step 7: Select Tool (Claims)\n    claims_selected_df = claims_df.select(\"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Amount\")\n    logger.info(\"Selected fields from Claims\")\n\n    # Step 8: Select Tool (Policy)\n    policy_selected_df = policy_df.select(\"Policy_ID\", \"Customer_ID\", \"Policy_Premium\", \"Policy_Type\")\n    logger.info(\"Selected fields from Policy\")\n\n    # Step 9: Join Demographics and Policy Data\n    demographics_policy_joined_df = demographics_selected_df.join(\n        policy_selected_df, on=\"Customer_ID\", how=\"inner\"\n    ).drop(policy_selected_df.Customer_ID)\n    logger.info(f\"Demographics and Policy Data Joined: {demographics_policy_joined_df.count()} records\")\n\n    # Step 10: Join Claims and Policy Data\n    # Retain Policy_ID for aggregation\n    claims_policy_joined_df = claims_selected_df.join(\n        policy_selected_df, on=\"Policy_ID\", how=\"inner\"\n    )\n    logger.info(f\"Claims and Policy Data Joined: {claims_policy_joined_df.count()} records\")\n\n    # Step 11: Summarize Tool\n    summarized_df = claims_policy_joined_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.count(\"Policy_ID\").alias(\"Total_Policies\"),\n        F.max(\"Claim_Date\").alias(\"Latest_Claim_Date\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\n    logger.info(\"Data Summarized\")\n\n    # Step 12: Join Summarized Data and Combined Data\n    combined_df = demographics_policy_joined_df.join(\n        summarized_df, on=\"Customer_ID\", how=\"inner\"\n    ).drop(summarized_df.Customer_ID)\n    logger.info(f\"Summarized and Combined Data Joined: {combined_df.count()} records\")\n\n    # Step 13: Formula Tool\n    final_df = combined_df.withColumn(\"Claim_To_Premium_Ratio\", F.col(\"Average_Claim_Amount\") / F.col(\"Policy_Premium\")) \\\n                          .withColumn(\"Claims_Per_Policy\", F.col(\"Total_Claims\") / F.col(\"Total_Policies\")) \\\n                          .withColumn(\"Retention_Rate\", F.lit(0.85)) \\\n                          .withColumn(\"Cross_Sell_Opportunities\", F.lit(\"High\")) \\\n                          .withColumn(\"Upsell_Potential\", F.lit(\"Medium\"))\n    logger.info(\"Calculated new fields\")\n\n    # Step 14: Output Customer 360 Data\n    target_catalog = \"genai_demo\"\n    target_schema = \"jnj\"\n    target_table = \"customer_360\"\n\n    # Ensure schema exists before creating table\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(\"Customer 360 Data Written Successfully\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {str(e)}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}