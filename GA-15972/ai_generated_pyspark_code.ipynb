{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType, DoubleType\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Node 1: Load Demographics Data\n    demographics_df = spark.table(\"genai_demo.jnj.demographics\")\n    logger.info(f\"Loaded demographics data with {demographics_df.count()} records\")\n    logger.info(f\"Demographics schema: {demographics_df.schema}\")\n\n    # Node 2: Load Claims Data\n    claims_df = spark.table(\"genai_demo.jnj.claims\")\n    logger.info(f\"Loaded claims data with {claims_df.count()} records\")\n    logger.info(f\"Claims schema: {claims_df.schema}\")\n\n    # Node 3: Load Policy Data\n    policy_df = spark.table(\"genai_demo.jnj.policy\")\n    logger.info(f\"Loaded policy data with {policy_df.count()} records\")\n    logger.info(f\"Policy schema: {policy_df.schema}\")\n\n    # Node 4: Load Scores Data\n    scores_df = spark.table(\"genai_demo.jnj.scores\")\n    logger.info(f\"Loaded scores data with {scores_df.count()} records\")\n    logger.info(f\"Scores schema: {scores_df.schema}\")\n\n    # Node 5: Load AI/ML Insights Data\n    aiml_insights_df = spark.table(\"genai_demo.jnj.aiml_insights\")\n    logger.info(f\"Loaded AI/ML insights data with {aiml_insights_df.count()} records\")\n    logger.info(f\"AI/ML Insights schema: {aiml_insights_df.schema}\")\n\n    # Node 6: Select Fields from Demographics\n    demographics_selected_df = demographics_df.select(\"Customer_ID\", \"Email\", \"Date_of_Birth\", \"Gender\", \"City\")\n    logger.info(\"Selected fields from demographics data\")\n\n    # Node 7: Select Fields from Claims\n    claims_selected_df = claims_df.select(\"Claim_ID\", \"Policy_ID\", \"Claim_Amount\", \"Claim_Date\")\n    logger.info(\"Selected fields from claims data\")\n\n    # Node 8: Select Fields from Policy\n    policy_selected_df = policy_df.select(\"Policy_ID\", \"Customer_ID\", \"Policy_Premium\", \"Policy_Type\")\n    logger.info(\"Selected fields from policy data\")\n\n    # Node 9: Join Demographics and Policy Data\n    demographics_policy_df = demographics_selected_df.join(policy_selected_df, \"Customer_ID\", \"inner\")\n    logger.info(f\"Joined demographics and policy data with {demographics_policy_df.count()} records\")\n\n    # Node 10: Join Claims and Policy Data\n    claims_policy_df = claims_selected_df.join(policy_selected_df, \"Policy_ID\", \"inner\")\n    logger.info(f\"Joined claims and policy data with {claims_policy_df.count()} records\")\n\n    # Node 11: Summarize Data\n    summarized_df = claims_policy_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.countDistinct(\"Policy_ID\").alias(\"Policy_Count\"),\n        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\n    logger.info(\"Summarized claims data\")\n\n    # Node 12: Join Summarized Data with Combined Data\n    combined_df = demographics_policy_df.join(summarized_df, \"Customer_ID\", \"inner\")\n    logger.info(f\"Joined summarized data with combined data, resulting in {combined_df.count()} records\")\n\n    # Node 13: Apply Formula Calculations\n    calculated_df = combined_df.withColumn(\"Claim_To_Premium_Ratio\", F.col(\"Average_Claim_Amount\") / F.col(\"Policy_Premium\")) \\\n                               .withColumn(\"Claims_Per_Policy\", F.col(\"Total_Claims\") / F.col(\"Policy_Count\")) \\\n                               .withColumn(\"Retention_Rate\", F.lit(0.85)) \\\n                               .withColumn(\"Cross_Sell_Opportunities\", F.when(F.col(\"Policy_Type\") == \"Home\", 1).otherwise(0).cast(IntegerType())) \\\n                               .withColumn(\"Upsell_Potential\", F.when(F.col(\"Policy_Premium\") < 500, 1).otherwise(0).cast(IntegerType()))\n    logger.info(\"Applied formula calculations\")\n\n    # Node 14: Join with Scores Data\n    combined_scores_df = calculated_df.join(scores_df, \"Customer_ID\", \"inner\")\n    logger.info(f\"Joined with scores data, resulting in {combined_scores_df.count()} records\")\n\n    # Node 15: Join with AI/ML Insights Data\n    final_df = combined_scores_df.join(aiml_insights_df, \"Customer_ID\", \"inner\")\n    logger.info(f\"Joined with AI/ML insights data, resulting in {final_df.count()} records\")\n\n    # Node 16: Output Customer 360 Data\n    target_catalog = \"genai_demo\"\n    target_schema = \"jnj\"\n    target_table = \"customer_360\"\n\n    # Ensure schema exists before creating table\n    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {target_catalog}.{target_schema}\")\n    logger.info(f\"Schema {target_catalog}.{target_schema} ensured\")\n\n    # Write to Unity Catalog target table (overwrite mode handles table replacement)\n    # Ensure the column types match the existing Delta table schema\n    final_df = final_df.withColumn(\"Cross_Sell_Opportunities\", F.col(\"Cross_Sell_Opportunities\").cast(IntegerType()))\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{target_catalog}.{target_schema}.{target_table}\")\n    logger.info(\"Customer 360 data written to Unity Catalog\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {str(e)}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}