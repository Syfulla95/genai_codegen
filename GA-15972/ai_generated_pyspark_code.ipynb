{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql.functions import col, when, datediff, current_date, avg, max as spark_max, count\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    logger.info(\"Loading data from Unity Catalog tables...\")\n    claims_df = spark.table(\"genai_demo.jnj.claims\")\n    demographics_df = spark.table(\"genai_demo.jnj.demographics\")\n    policy_df = spark.table(\"genai_demo.jnj.policy\")\n    scores_df = spark.table(\"genai_demo.jnj.scores\")\n    aiml_insights_df = spark.table(\"genai_demo.jnj.aiml_insights\")\n\n    # Select relevant fields\n    logger.info(\"Selecting relevant fields...\")\n    demographics_df = demographics_df.select(\"Customer_ID\", \"Customer_Name\", \"Email\", \"Phone_Number\", \"Address\", \n                                             \"City\", \"State\", \"Postal_Code\", \"Date_of_Birth\", \"Gender\", \n                                             \"Marital_Status\", \"Occupation\", \"Income_Level\", \"Customer_Segment\")\n    \n    claims_df = claims_df.select(\"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Type\", \"Claim_Status\", \n                                 \"Claim_Amount\", \"Claim_Payout\")\n    \n    policy_df = policy_df.select(\"Policy_ID\", \"Customer_ID\", \"Policy_Type\", \"Policy_Status\", \"Policy_Start_Date\", \n                                 \"Policy_End_Date\", \"Policy_Term\", \"Policy_Premium\", \"Total_Premium_Paid\", \n                                 \"Renewal_Status\", \"Policy_Addons\")\n    \n    scores_df = scores_df.select(\"Customer_ID\", \"Credit_Score\", \"Fraud_Score\", \"Customer_Risk_Score\")\n    \n    aiml_insights_df = aiml_insights_df.select(\"Customer_ID\", \"Churn_Probability\", \"Next_Best_Offer\", \n                                               \"Claims_Fraud_Probability\", \"Revenue_Potential\")\n\n    # Join Demographics and Policy data\n    logger.info(\"Joining Demographics and Policy data...\")\n    demo_policy_df = demographics_df.join(policy_df, \"Customer_ID\", \"inner\")\n\n    # Join Claims and Policy data\n    logger.info(\"Joining Claims and Policy data...\")\n    claims_policy_df = claims_df.join(policy_df, \"Policy_ID\", \"inner\")\n\n    # Summarize data\n    logger.info(\"Summarizing data...\")\n    summary_df = claims_policy_df.groupBy(\"Customer_ID\").agg(\n        count(\"Claim_ID\").alias(\"Total_Claims\"),\n        count(\"Policy_ID\").alias(\"Policy_Count\"),\n        spark_max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n    )\n\n    # Join summarized data with combined data\n    logger.info(\"Joining summarized data with combined data...\")\n    combined_df = demo_policy_df.join(summary_df, \"Customer_ID\", \"inner\")\n\n    # Custom Calculations\n    logger.info(\"Performing custom calculations...\")\n    combined_df = combined_df.withColumn(\"Age\", datediff(current_date(), col(\"Date_of_Birth\")) / 365.25) \\\n                             .withColumn(\"Claim_To_Premium_Ratio\", when(col(\"Total_Premium_Paid\") != 0, \n                                                                        col(\"Claim_Amount\") / col(\"Total_Premium_Paid\")).otherwise(0)) \\\n                             .withColumn(\"Claims_Per_Policy\", when(col(\"Policy_Count\") != 0, \n                                                                   col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0)) \\\n                             .withColumn(\"Retention_Rate\", lit(0.85)) \\\n                             .withColumn(\"Cross_Sell_Opportunities\", lit(\"Multi-Policy Discount, Home Coverage Add-on\")) \\\n                             .withColumn(\"Upsell_Potential\", lit(\"Premium Vehicle Coverage\"))\n\n    # Integrate AI/ML insights\n    logger.info(\"Integrating AI/ML insights...\")\n    final_df = combined_df.join(aiml_insights_df, \"Customer_ID\", \"left\") \\\n                          .join(scores_df, \"Customer_ID\", \"left\")\n\n    # Write to Unity Catalog target table\n    logger.info(\"Writing to Unity Catalog target table...\")\n    spark.sql(\"DROP TABLE IF EXISTS genai_demo.jnj.customer_360\")\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.jnj.customer_360\")\n\n    logger.info(\"ETL process completed successfully.\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}