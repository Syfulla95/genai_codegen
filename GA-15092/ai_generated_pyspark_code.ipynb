{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import DoubleType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    # Load data from Unity Catalog tables\n    hospital_stats_df = spark.table(\"genai_demo.cardinal_health.hospital_stats_north_america\")\n    hospital_assignments_df = spark.table(\"genai_demo.cardinal_health.hospital_sales_assignments\")\n    employment_details_df = spark.table(\"genai_demo.cardinal_health.sales_associates_employment_details\")\n    compensation_guidelines_df = spark.table(\"genai_demo.cardinal_health.compensation_guidelines\")\n    logistics_channels_df = spark.table(\"genai_demo.cardinal_health.logistics_channels\")\n    growth_opportunities_df = spark.table(\"genai_demo.cardinal_health.growth_opportunities\")\n    company_goals_df = spark.table(\"genai_demo.cardinal_health.company_goals\")\n    historical_sales_df = spark.table(\"genai_demo.cardinal_health.historical_sales\")\n    third_party_trends_df = spark.table(\"genai_demo.cardinal_health.third_party_sales_trends\")\n\n    # Step 1: Join hospital statistics with sales assignments\n    hospital_sales_df = hospital_stats_df.join(\n        hospital_assignments_df,\n        on=[\"Hospital_ID\", \"Hospital_Name\"],\n        how=\"inner\"\n    )\n\n    # Step 2: Join employment details with compensation guidelines\n    employment_compensation_df = employment_details_df.join(\n        compensation_guidelines_df,\n        on=\"Associate_ID\",\n        how=\"inner\"\n    )\n\n    # Step 3: Calculate total compensation\n    employment_compensation_df = employment_compensation_df.withColumn(\n        \"Compensation\",\n        F.col(\"Base_Salary\") + (F.col(\"Commission_Percentage\") / 100 * F.col(\"Base_Salary\")) + F.col(\"Bonus\")\n    )\n\n    # Step 4: Select relevant fields\n    selected_employment_df = employment_compensation_df.select(\n        \"Associate_ID\", \"Associate_Name\", \"Compensation\", \"Department\", \"Employment_Type\", \"Region\", \"Years_of_Experience\"\n    )\n\n    # Step 5: Join logistics channels with growth opportunities\n    logistics_growth_df = logistics_channels_df.join(\n        growth_opportunities_df,\n        on=[\"Channel_ID\", \"Channel_Type\", \"Hospital_ID\"],\n        how=\"inner\"\n    )\n\n    # Step 6: Join compensation data with logistics and growth data\n    combined_df = selected_employment_df.join(\n        logistics_growth_df,\n        on=\"Hospital_ID\",\n        how=\"inner\"\n    )\n\n    # Step 7: Select fields for further processing\n    selected_combined_df = combined_df.select(\n        \"Hospital_ID\", \"Channel_Type\", \"Growth_Opportunities\", \"Projected_Growth_Rate\"\n    )\n\n    # Step 8: Join selected data with company goals\n    goals_combined_df = selected_combined_df.join(\n        company_goals_df,\n        on=[\"Hospital_ID\", \"Channel_Type\"],\n        how=\"inner\"\n    )\n\n    # Step 9: Ensure unique records\n    unique_goals_df = goals_combined_df.dropDuplicates([\"Hospital_ID\", \"Channel_Type\", \"Projected_Growth_Rate\", \"Investment_Planned\"])\n\n    # Step 10: Join historical sales with third-party sales trends\n    sales_trends_df = historical_sales_df.join(\n        third_party_trends_df,\n        on=\"Channel_Type\",\n        how=\"inner\"\n    )\n\n    # Step 11: Ensure unique sales records\n    unique_sales_df = sales_trends_df.dropDuplicates([\"Year\", \"Channel_Type\", \"Sales_Revenue\"])\n\n    # Step 12: Join unique sales data with growth and investment data\n    final_df = unique_sales_df.join(\n        unique_goals_df,\n        on=[\"Hospital_ID\", \"Channel_ID\", \"Channel_Type\"],\n        how=\"inner\"\n    )\n\n    # Step 13: Generate rows for target years\n    final_df = final_df.withColumn(\"Target Year\", F.expr(\"sequence(2023, 2026)\")).selectExpr(\"*\", \"explode(`Target Year`) as `Target Year`\")\n\n    # Step 14: Calculate projected sales growth rate\n    final_df = final_df.withColumn(\n        \"projected_sales_growth_rate\",\n        F.when(F.col(\"Target Year\") == 2024, F.col(\"Projected_Growth_Rate\") + (F.col(\"Projected_Growth_Rate\") / 100))\n         .when(F.col(\"Target Year\") == 2025, (F.col(\"Projected_Growth_Rate\") + (F.col(\"Projected_Growth_Rate\") / 100)) + (F.col(\"Projected_Growth_Rate\") / 100))\n         .when(F.col(\"Target Year\") == 2026, ((F.col(\"Projected_Growth_Rate\") + (F.col(\"Projected_Growth_Rate\") / 100)) + (F.col(\"Projected_Growth_Rate\") / 100)) + (F.col(\"Projected_Growth_Rate\") / 100))\n         .otherwise(F.col(\"Projected_Growth_Rate\"))\n    )\n\n    # Step 15: Calculate projected investments\n    final_df = final_df.withColumn(\n        \"projected_investments\",\n        F.when(F.col(\"Target Year\") == 2024, F.col(\"Investment_Planned\") * (F.col(\"projected_sales_growth_rate\") / 100))\n         .when(F.col(\"Target Year\") == 2025, F.col(\"Investment_Planned\") * (1 + F.col(\"projected_sales_growth_rate\") / 100))\n         .when(F.col(\"Target Year\") == 2026, F.col(\"Investment_Planned\") * (1 + F.col(\"projected_sales_growth_rate\") / 100))\n         .otherwise(F.col(\"Investment_Planned\"))\n    )\n\n    # Step 16: Calculate projected revenue\n    final_df = final_df.withColumn(\n        \"Projected Revenue\",\n        F.when(F.col(\"Target Year\") == 2024, F.col(\"Sales_Revenue\") * (F.col(\"projected_sales_growth_rate\") / 100))\n         .when(F.col(\"Target Year\") == 2025, F.col(\"Sales_Revenue\") * (1 + F.col(\"projected_sales_growth_rate\") / 100))\n         .when(F.col(\"Target Year\") == 2026, F.col(\"Sales_Revenue\") * (1 + F.col(\"projected_sales_growth_rate\") / 100))\n         .otherwise(F.col(\"Sales_Revenue\"))\n    )\n\n    # Step 17: Filter data for target years greater than 2023\n    filtered_df = final_df.filter(F.col(\"Target Year\") > 2023)\n\n    # Step 18: Select final fields for output\n    output_df = filtered_df.select(\n        \"Channel_Type\", \"Hospital_ID\", \"Market_Trend\", \"Political_Impact\", \"Economic_Impact\", \"Target Year\",\n        \"projected_sales_growth_rate\", \"projected_investments\", \"Projected Revenue\"\n    )\n\n    # Step 19: Sort data by target year in ascending order\n    sorted_output_df = output_df.orderBy(\"Target Year\")\n\n    # Step 20: Write the processed data to Unity Catalog tables using Delta format\n    spark.sql(\"DROP TABLE IF EXISTS genai_demo.cardinal_health.sales_prediction_output\")\n    sorted_output_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"genai_demo.cardinal_health.sales_prediction_output\")\n\n    logger.info(\"ETL process completed successfully.\")\n\nexcept Exception as e:\n    logger.error(f\"An error occurred during the ETL process: {e}\")\n    raise\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}