{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# MAGIC %md\n# MAGIC # Insurance Data Processing\n# MAGIC This notebook processes insurance data by loading, transforming, and summarizing it, and finally writing the results to a Delta table.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Import necessary libraries\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import SparkSession\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Assume the Spark session is already available as 'spark'\n\n# COMMAND ----------\n# MAGIC\n",
                "# Load data from CSV files stored in cloud storage\ndef load_data():\n    try:\n        logger.info(\"Loading data from CSV files...\")\n        policy_df = spark.read.csv(\"s3://bucket/path/policy.csv\", header=True, inferSchema=True)\n        claims_df = spark.read.csv(\"s3://bucket/path/claims.csv\", header=True, inferSchema=True)\n        demographics_df = spark.read.csv(\"s3://bucket/path/demographics.csv\", header=True, inferSchema=True)\n        scores_df = spark.read.csv(\"s3://bucket/path/scores.csv\", header=True, inferSchema=True)\n        aiml_insights_df = spark.read.csv(\"s3://bucket/path/aiml_insights.csv\", header=True, inferSchema=True)\n        logger.info(\"Data loaded successfully.\")\n        return policy_df, claims_df, demographics_df, scores_df, aiml_insights_df\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\npolicy_df, claims_df, demographics_df, scores_df, aiml_insights_df = load_data()\n\n# COMMAND ----------\n# MAGIC\n",
                "# Select relevant fields\ndef select_fields(demographics_df, claims_df, policy_df):\n    try:\n        logger.info(\"Selecting relevant fields...\")\n        demographics_selected = demographics_df.select(\"Customer_ID\", \"Customer_Name\", \"Email\", \"Date_of_Birth\")\n        claims_selected = claims_df.select(\"Claim_ID\", \"Policy_ID\", \"Claim_Date\", \"Claim_Amount\")\n        policy_selected = policy_df.select(\"policy_id\", \"customer_id\", \"policy_type\", \"total_premium_paid\")\n        logger.info(\"Fields selected successfully.\")\n        return demographics_selected, claims_selected, policy_selected\n    except Exception as e:\n        logger.error(f\"Error selecting fields: {e}\")\n        raise\n\ndemographics_selected, claims_selected, policy_selected = select_fields(demographics_df, claims_df, policy_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Join DataFrames\ndef join_dataframes(demographics_selected, policy_selected, claims_selected):\n    try:\n        logger.info(\"Joining DataFrames...\")\n        joined_df = demographics_selected.join(policy_selected, F.col('Customer_ID') == F.col('customer_id'), \"inner\")\n        joined_df = joined_df.join(claims_selected, F.col('policy_id') == F.col('Policy_ID'), \"inner\")\n        logger.info(\"DataFrames joined successfully.\")\n        return joined_df\n    except Exception as e:\n        logger.error(f\"Error joining DataFrames: {e}\")\n        raise\n\njoined_df = join_dataframes(demographics_selected, policy_selected, claims_selected)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Summarize Data\ndef summarize_data(joined_df):\n    try:\n        logger.info(\"Summarizing data...\")\n        summarized_df = joined_df.groupBy(\"Customer_ID\").agg(\n            F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n            F.count(\"policy_id\").alias(\"Policy_Count\"),\n            F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n            F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\")\n        )\n        logger.info(\"Data summarized successfully.\")\n        return summarized_df\n    except Exception as e:\n        logger.error(f\"Error summarizing data: {e}\")\n        raise\n\nsummarized_df = summarize_data(joined_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Calculate Additional Metrics\ndef calculate_metrics(summarized_df):\n    try:\n        logger.info(\"Calculating additional metrics...\")\n        claim_to_premium_ratio_expr = F.when(F.col(\"total_premium_paid\") > 0, F.col(\"Average_Claim_Amount\") / F.col(\"total_premium_paid\")).otherwise(0)\n        final_df = summarized_df.withColumn(\"Age\", F.datediff(F.current_date(), F.col(\"Date_of_Birth\")) / 365)\n        final_df = final_df.withColumn(\"Claim_To_Premium_Ratio\", claim_to_premium_ratio_expr)\n        final_df = final_df.withColumn(\"Claims_Per_Policy\", F.when(F.col(\"Policy_Count\") > 0, F.col(\"Total_Claims\") / F.col(\"Policy_Count\")).otherwise(0))\n        final_df = final_df.withColumn(\"Retention_Rate\", F.lit(0.85))\n        final_df = final_df.withColumn(\"Cross_Sell_Opportunities\", F.lit(\"MultiPolicy Discount, Home Coverage Addon\"))\n        final_df = final_df.withColumn(\"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\"))\n        logger.info(\"Additional metrics calculated successfully.\")\n        return final_df\n    except Exception as e:\n        logger.error(f\"Error calculating additional metrics: {e}\")\n        raise\n\nfinal_df = calculate_metrics(summarized_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Join with AIML Insights and Scores\ndef join_with_insights(final_df, aiml_insights_df, scores_df):\n    try:\n        logger.info(\"Joining with AIML insights and scores...\")\n        comprehensive_df = final_df.join(aiml_insights_df, \"Customer_ID\", \"inner\").join(scores_df, \"Customer_ID\", \"inner\")\n        logger.info(\"Joined with AIML insights and scores successfully.\")\n        return comprehensive_df\n    except Exception as e:\n        logger.error(f\"Error joining with AIML insights and scores: {e}\")\n        raise\n\ncomprehensive_df = join_with_insights(final_df, aiml_insights_df, scores_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Output Configuration\ndef write_output(comprehensive_df):\n    try:\n        logger.info(\"Writing output to Unity Catalog...\")\n        spark.sql(\"DROP TABLE IF EXISTS catalog.target_db.Customer_360\")\n        comprehensive_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.target_db.Customer_360\")\n        logger.info(\"Output written successfully.\")\n    except Exception as e:\n        logger.error(f\"Error writing output: {e}\")\n        raise\n\nwrite_output(comprehensive_df)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}