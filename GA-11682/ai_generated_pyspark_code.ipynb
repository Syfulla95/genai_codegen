{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\nimport requests\nimport psycopg2\nfrom pyspark.sql.functions import col, expr, datediff, current_date, when, count, avg, max\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to fetch data from REST API\ndef fetch_api_data(api_url, customer_id):\n    try:\n        response = requests.get(api_url.format(Customer_ID=customer_id))\n        response.raise_for_status()\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Error fetching data from API: {e}\")\n        return None\n\n# Load data from Unity Catalog tables\ntry:\n    policy_df = spark.table(\"postgresql_catalog.demo.policydb\")\n    claims_df = spark.table(\"mysql_catalog.vsco.claimsdb\")\n    demographics_df = spark.table(\"sqlserver_catalog.dbo.demographicsdb\")\nexcept Exception as e:\n    logger.error(f\"Error loading data from Unity Catalog: {e}\")\n    raise\n\n# Join policy data with customer demographics\ntry:\n    policy_demo_df = policy_df.join(demographics_df, \"Customer_ID\", \"inner\")\nexcept Exception as e:\n    logger.error(f\"Error joining policy and demographics data: {e}\")\n    raise\n\n# Join the result with claims data\ntry:\n    policy_claims_df = policy_demo_df.join(claims_df, \"Policy_ID\", \"inner\")\nexcept Exception as e:\n    logger.error(f\"Error joining policy-demo and claims data: {e}\")\n    raise\n\n# Aggregate data at the customer level\ntry:\n    agg_df = policy_claims_df.groupBy(\"Customer_ID\").agg(\n        count(\"Claim_ID\").alias(\"Total_Claims\"),\n        avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\"),\n        max(\"Claim_Date\").alias(\"Recent_Claim_Date\"),\n        count(\"Policy_ID\").alias(\"Policy_Count\")\n    )\nexcept Exception as e:\n    logger.error(f\"Error aggregating data: {e}\")\n    raise\n\n# Calculate derived fields\ntry:\n    derived_df = agg_df.withColumn(\"Age\", datediff(current_date(), col(\"Date_of_Birth\")) / 365) \\\n        .withColumn(\"Claim_To_Premium_Ratio\", when(col(\"Total_Premium_Paid\") != 0, col(\"Claim_Amount\") / col(\"Total_Premium_Paid\")).otherwise(0)) \\\n        .withColumn(\"Claims_Per_Policy\", when(col(\"Policy_Count\") != 0, col(\"Total_Claims\") / col(\"Policy_Count\")).otherwise(0)) \\\n        .withColumn(\"Retention_Rate\", expr(\"0.85\")) \\\n        .withColumn(\"Cross_Sell_Opportunities\", expr(\"'Multi-Policy Discount, Home Coverage Add-on'\")) \\\n        .withColumn(\"Upsell_Potential\", expr(\"'Premium Vehicle Coverage'\"))\nexcept Exception as e:\n    logger.error(f\"Error calculating derived fields: {e}\")\n    raise\n\n# Fetch fraud and credit scores from APIs\ntry:\n    fraud_scores = derived_df.rdd.map(lambda row: (row.Customer_ID, fetch_api_data(\"http://18.189.118.116:9010/fraudscore?Customer_ID={Customer_ID}\", row.Customer_ID)))\n    credit_scores = derived_df.rdd.map(lambda row: (row.Customer_ID, fetch_api_data(\"http://18.189.118.116:9010/creditscore?Customer_ID={Customer_ID}\", row.Customer_ID)))\nexcept Exception as e:\n    logger.error(f\"Error fetching fraud and credit scores: {e}\")\n    raise\n\n# Convert RDDs to DataFrames\nfraud_scores_df = fraud_scores.toDF([\"Customer_ID\", \"Fraud_Score\"])\ncredit_scores_df = credit_scores.toDF([\"Customer_ID\", \"Credit_Score\"])\n\n# Join derived data with fraud and credit scores\ntry:\n    final_df = derived_df.join(fraud_scores_df, \"Customer_ID\", \"left\") \\\n        .join(credit_scores_df, \"Customer_ID\", \"left\")\nexcept Exception as e:\n    logger.error(f\"Error joining derived data with scores: {e}\")\n    raise\n\n# Add AI-driven insights\ntry:\n    final_df = final_df.withColumn(\"Churn_Probability\", expr(\"0.25\")) \\\n        .withColumn(\"Next_Best_Offer\", expr(\"'Additional Life Coverage'\")) \\\n        .withColumn(\"Claims_Fraud_Probability\", expr(\"0.10\")) \\\n        .withColumn(\"Revenue_Potential\", expr(\"12000.00\"))\nexcept Exception as e:\n    logger.error(f\"Error adding AI-driven insights: {e}\")\n    raise\n\n# Write the final DataFrame to Unity Catalog target table\ntry:\n    spark.sql(\"DROP TABLE IF EXISTS catalog_name.schema_name.customer_360\")\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog_name.schema_name.customer_360\")\nexcept Exception as e:\n    logger.error(f\"Error writing data to Unity Catalog: {e}\")\n    raise\n\nlogger.info(\"ETL process completed successfully.\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}