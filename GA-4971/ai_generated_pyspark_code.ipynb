{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Databricks notebook source\n# COMMAND ----------\n# MAGIC %md\n# MAGIC # ETL Process for Insurance Data\n# MAGIC This notebook performs an ETL process on insurance data, integrating various datasets and performing transformations to create a comprehensive customer view.\n\n# COMMAND ----------\n# MAGIC\n",
                "# Import necessary libraries\nimport logging\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 1: Data Ingestion\ndef load_data():\n    logger.info(\"Loading data from Unity Catalog tables...\")\n    policy_df = spark.table(\"catalog.insurance_db.policy\")\n    claims_df = spark.table(\"catalog.insurance_db.claims\")\n    demographics_df = spark.table(\"catalog.insurance_db.demographics\")\n    scores_df = spark.table(\"catalog.insurance_db.scores\")\n    aiml_insights_df = spark.table(\"catalog.insurance_db.aiml_insights\")\n    return policy_df, claims_df, demographics_df, scores_df, aiml_insights_df\n\npolicy_df, claims_df, demographics_df, scores_df, aiml_insights_df = load_data()\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 2: Data Selection\ndef select_data(demographics_df, claims_df, policy_df):\n    logger.info(\"Selecting relevant fields from each dataset...\")\n    selected_demographics_df = demographics_df.select(\n        F.col(\"Customer_ID\"), F.col(\"Customer_Name\"), F.col(\"Email\"), F.col(\"Phone_Number\"), F.col(\"Date_of_Birth\")\n    )\n    selected_claims_df = claims_df.select(\n        F.col(\"Claim_ID\"), F.col(\"Policy_ID\"), F.col(\"Claim_Date\"), F.col(\"Claim_Amount\")\n    )\n    selected_policy_df = policy_df.select(\n        F.col(\"policy_id\"), F.col(\"customer_id\"), F.col(\"policy_type\"), F.col(\"policy_premium\"), F.col(\"total_premium_paid\")\n    )\n    return selected_demographics_df, selected_claims_df, selected_policy_df\n\nselected_demographics_df, selected_claims_df, selected_policy_df = select_data(demographics_df, claims_df, policy_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 3: Data Integration\ndef integrate_data(selected_demographics_df, selected_policy_df, selected_claims_df):\n    logger.info(\"Joining datasets on key identifiers...\")\n    joined_df = selected_demographics_df.join(\n        selected_policy_df, selected_demographics_df.Customer_ID == selected_policy_df.customer_id, \"inner\"\n    ).join(\n        selected_claims_df, selected_policy_df.policy_id == selected_claims_df.Policy_ID, \"inner\"\n    )\n    return joined_df\n\njoined_df = integrate_data(selected_demographics_df, selected_policy_df, selected_claims_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 4: Aggregation and Custom Calculations\ndef aggregate_and_calculate(joined_df):\n    logger.info(\"Performing aggregation and custom calculations...\")\n    aggregated_df = joined_df.groupBy(\"Customer_ID\").agg(\n        F.count(\"Claim_ID\").alias(\"Total_Claims\"),\n        F.avg(\"Claim_Amount\").alias(\"Average_Claim_Amount\"),\n        F.max(\"Claim_Date\").alias(\"Recent_Claim_Date\")\n    )\n\n    age_expr = F.expr(\"DATEDIFF(current_date(), to_date(Date_of_Birth, 'yyyy-MM-dd')) / 365\").cast(IntegerType())\n    claim_to_premium_ratio_expr = F.expr(\"Claim_Amount / total_premium_paid\")\n    claims_per_policy_expr = F.expr(\"Total_Claims / count(policy_id)\")\n\n    enriched_df = aggregated_df.withColumn(\"Age\", age_expr) \\\n        .withColumn(\"Claim_To_Premium_Ratio\", claim_to_premium_ratio_expr) \\\n        .withColumn(\"Claims_Per_Policy\", claims_per_policy_expr) \\\n        .withColumn(\"Retention_Rate\", F.lit(0.85)) \\\n        .withColumn(\"Cross_Sell_Opportunities\", F.lit(\"Multi-Policy Discount, Home Coverage Add-on\")) \\\n        .withColumn(\"Upsell_Potential\", F.lit(\"Premium Vehicle Coverage\"))\n    return enriched_df\n\nenriched_df = aggregate_and_calculate(joined_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 5: Advanced Data Enrichment\ndef enrich_data(enriched_df, scores_df, aiml_insights_df):\n    logger.info(\"Integrating AI/ML insights and scores data...\")\n    final_df = enriched_df.join(scores_df, \"Customer_ID\", \"inner\").join(aiml_insights_df, \"Customer_ID\", \"inner\")\n    return final_df\n\nfinal_df = enrich_data(enriched_df, scores_df, aiml_insights_df)\n\n# COMMAND ----------\n# MAGIC\n",
                "# Step 6: Output Generation\ndef write_output(final_df):\n    logger.info(\"Writing the final dataset to Unity Catalog table...\")\n    spark.sql(\"DROP TABLE IF EXISTS catalog.insurance_db.customer_360\")\n    final_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog.insurance_db.customer_360\")\n    logger.info(\"ETL process completed successfully.\")\n\nwrite_output(final_df)\n\n# COMMAND ----------\n# MAGIC %md\n# MAGIC ## Conclusion\n# MAGIC The ETL process has been successfully completed, and the final dataset is stored in the Unity Catalog table `customer_360`.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}